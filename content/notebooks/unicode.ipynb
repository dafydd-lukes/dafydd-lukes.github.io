{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, [the absolute minimum every ~~software developer~~ linguist absolutely, positively must know about Unicode and character sets (no excuses!)](http://www.joelonsoftware.com/articles/Unicode.html)\n",
    "\n",
    "> **Note**: This text uses the Python programming language to give some hands-on experience of the concepts discussed. If you're not familiar with programming at all, much less with Python, my advice is:\n",
    ">\n",
    "> - either: ignore the code, focus on the comments around it, they should be enough to follow the thread of the explanation;\n",
    "> - or, if you've got a little more time: Python is pretty intuitive, so you might want to have a look at the code examples anyway.\n",
    ">\n",
    "> In any case, the code might make more sense to you if you tinker with it in an interactive Python session: [![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/dlukes/dlukes.github.io/source?filepath=content%2Fnotebooks%2Funicode.ipynb)\n",
    ">\n",
    "> And now, without further ado...\n",
    "\n",
    "Much like any other piece of data inside a digital computer, text is represented as a series of binary digits (bits), i.e. 0's and 1's. A mapping between sequences of bits and characters is called an encoding. How many different characters your encoding can handle depends on how many bits you allow per character:\n",
    "\n",
    "- with 1 bit you can have 2^1 = 2 characters (one is mapped to 0, the other to 1)\n",
    "- with 2 bits you can have 2^2 = 2\\*2 = 4 characters (mapped to 00, 01, 10 and 11)\n",
    "- with 3 bits you can have 2^3 = 2\\*2\\*2 = 8 characters \n",
    "- etc.\n",
    "\n",
    "Now, a short digression on representing numbers, to make sure we're all on the same page: in the context of computers, you often see the same number represented in three different ways:\n",
    "\n",
    "1. as a **decimal** number, using 10 digits 0--9 (e.g. 12)\n",
    "2. as a **binary** number, using only 2 digits, 0 and 1 (e.g. 1100, which equals decimal 12)\n",
    "3. as a **hexadecimal** number, using 16 digits: 0--9 and a--f (e.g. c, which also equals decimal 12)\n",
    "\n",
    "As a consequence, the need often arises to convert between these different **numeral systems**. If you don't want to do so by hand, Python has your back! The `bin()` function gives you a *string representation* of the binary form of a number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0b1000001'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin(65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, in Python, binary numbers are given a `0b` prefix to distinguish them from regular (decimal) numbers. The number itself is what follows after (i.e. 1000001).\n",
    "\n",
    "Similarly, hexadecimal numbers are given an `0x` prefix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x41'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex(65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert in the opposite direction, i.e. *to* decimal, just evaluate the binary or hexadecimal representation of a number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0b1000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0x41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numbers using these different **bases** (base 2 = binary, base 10 = decimal, base 16 = hexadecimal) are mutually compatible, e.g. for comparison purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0b1000001 == 0x41 == 65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are hexadecimals useful? They're primarily a more **convenient, condensed** way of representing sequences of bits: each hexadecimal digit can represent 16 different values, and therefore it can stand in for a sequence of 4 bits (2^4 = 16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0xa == 0b1010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0xb == 0b1011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we paste together hexadecimal a and b, it's the\n",
    "# same as pasting together binary 1010 and 1011\n",
    "0xab == 0b10101011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, instead of binary 10101011, we can just write hexadecimal ab and save ourselves some space. Of course, this only works if shorter binary numbers are **padded to a 4-bit width**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0x2 == 0b10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0x3 == 0b11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we paste together hexadecimal 2 and 3, we have to\n",
    "# paste together binary 0010 and 0011...\n",
    "0x23 == 0b00100011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... not just 10 and 11\n",
    "0x23 == 0b1011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The padding has no effect on the value, much like decimal 42 and 00000042 are effectively the same numbers.\n",
    "\n",
    "Now back to text encodings. The oldest encoding still in widespread use is [`ASCII`](https://en.wikipedia.org/wiki/ASCII), which is a 7-bit encoding. What's the number of *different* sequences of seven 1's and 0's?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how Python spells 2^7, i.e. 2*2*2*2*2*2*2\n",
    "2**7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means ASCII can represent [128 different characters](http://www.ascii-code.com/), which comfortably fits the basic Latin alphabet (both lowercase and uppercase), Arabic numerals, punctuation and some \"control characters\" which were primarily useful on the old [teletype terminals](https://en.wikipedia.org/wiki/Teleprinter) for which `ASCII` was designed. For instance, the letter \"A\" corresponds to the number 65 (`1000001` in binary, see above).\n",
    "\n",
    "> \"ASCII\" stands for \"**American** Standard Code for Information Interchange\" -- which explains why there are no accented characters, for instance.\n",
    "\n",
    "Nowadays, `ASCII` is represented using 8 bits (= 1 byte), because that's the unit of computer memory which has become ubiquitous (in terms of both hardware and software assumptions), but still uses only 7 bits' worth of information. That extra bit means that there's **room for another 128 characters in addition to the 128 ASCII ones**, coming up to a total of 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**(7+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens in the range [128; 256) is not covered by the `ASCII` standard. In the 1990s, many encodings were standardized which used this range for their own purposes, usually representing additional accented characters used in a particular region. E.g. Czech (and Slovak, Polish...) alphabets can be represented using the ISO `latin-2` encoding, or Microsoft's `cp-1250`. Encodings which stick to the same character mappings as `ASCII` in the range [0; 128) **and represent them physically in the same way (as 1 byte)**, while potentially adding more character mappings beyond that, are called **`ASCII`-compatible**.\n",
    "\n",
    "`ASCII` compatibility is a good thing&trade;, because when you start reading a character stream in a computer, there's **no way to know in advance what encoding it is in** (unless it's a file you've encoded yourself). So in practice, a heuristic has been established to start reading the stream assuming it is `ASCII` by default, and switch to a different encoding if evidence becomes available that motivates it. For instance, HTML files should all start something like this:\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "  <meta charset=\"utf-8\"/>\n",
    "  ...\n",
    "```\n",
    "\n",
    "This way, whenever a program wants to read a file like this, it can start off with `ASCII`, waiting to see if it reaches the `charset` (i.e. encoding) attribute, and once it does, it can switch from `ASCII` to that encoding (`UTF-8` here) and restart reading the file, now fairly sure that it's using the correct encoding. This trick works only if we can assume that whatever encoding the rest of the file is in, the first few lines can be considered as `ASCII` for all practical intents and purposes.\n",
    "\n",
    "Without the `charset` attribute, the only way to know if the encoding is right would be for you to look at the rendered text and see if it makes sense; if it did not, you'd have to resort to trial and error, manually switching the encodings and looking for the one in which the numbers behind the characters stop coming out as gibberish and are actually translated into intelligible text.\n",
    "\n",
    "Let's take a look at printable characters in the `Latin-2` **character set**. The character set consists of mappings between positive **integers** (whole numbers) and characters; each one of these is called a **codepoint**. The `Latin-2` **encoding** then defines how to encode each of these integers as a series of bits (1's and 0's) in the computer's memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(32, ' '),\n",
       " (33, '!'),\n",
       " (34, '\"'),\n",
       " (35, '#'),\n",
       " (36, '$'),\n",
       " (37, '%'),\n",
       " (38, '&'),\n",
       " (39, \"'\"),\n",
       " (40, '('),\n",
       " (41, ')'),\n",
       " (42, '*'),\n",
       " (43, '+'),\n",
       " (44, ','),\n",
       " (45, '-'),\n",
       " (46, '.'),\n",
       " (47, '/'),\n",
       " (48, '0'),\n",
       " (49, '1'),\n",
       " (50, '2'),\n",
       " (51, '3'),\n",
       " (52, '4'),\n",
       " (53, '5'),\n",
       " (54, '6'),\n",
       " (55, '7'),\n",
       " (56, '8'),\n",
       " (57, '9'),\n",
       " (58, ':'),\n",
       " (59, ';'),\n",
       " (60, '<'),\n",
       " (61, '='),\n",
       " (62, '>'),\n",
       " (63, '?'),\n",
       " (64, '@'),\n",
       " (65, 'A'),\n",
       " (66, 'B'),\n",
       " (67, 'C'),\n",
       " (68, 'D'),\n",
       " (69, 'E'),\n",
       " (70, 'F'),\n",
       " (71, 'G'),\n",
       " (72, 'H'),\n",
       " (73, 'I'),\n",
       " (74, 'J'),\n",
       " (75, 'K'),\n",
       " (76, 'L'),\n",
       " (77, 'M'),\n",
       " (78, 'N'),\n",
       " (79, 'O'),\n",
       " (80, 'P'),\n",
       " (81, 'Q'),\n",
       " (82, 'R'),\n",
       " (83, 'S'),\n",
       " (84, 'T'),\n",
       " (85, 'U'),\n",
       " (86, 'V'),\n",
       " (87, 'W'),\n",
       " (88, 'X'),\n",
       " (89, 'Y'),\n",
       " (90, 'Z'),\n",
       " (91, '['),\n",
       " (92, '\\\\'),\n",
       " (93, ']'),\n",
       " (94, '^'),\n",
       " (95, '_'),\n",
       " (96, '`'),\n",
       " (97, 'a'),\n",
       " (98, 'b'),\n",
       " (99, 'c'),\n",
       " (100, 'd'),\n",
       " (101, 'e'),\n",
       " (102, 'f'),\n",
       " (103, 'g'),\n",
       " (104, 'h'),\n",
       " (105, 'i'),\n",
       " (106, 'j'),\n",
       " (107, 'k'),\n",
       " (108, 'l'),\n",
       " (109, 'm'),\n",
       " (110, 'n'),\n",
       " (111, 'o'),\n",
       " (112, 'p'),\n",
       " (113, 'q'),\n",
       " (114, 'r'),\n",
       " (115, 's'),\n",
       " (116, 't'),\n",
       " (117, 'u'),\n",
       " (118, 'v'),\n",
       " (119, 'w'),\n",
       " (120, 'x'),\n",
       " (121, 'y'),\n",
       " (122, 'z'),\n",
       " (123, '{'),\n",
       " (124, '|'),\n",
       " (125, '}'),\n",
       " (126, '~'),\n",
       " (161, 'Ą'),\n",
       " (162, '˘'),\n",
       " (163, 'Ł'),\n",
       " (164, '¤'),\n",
       " (165, 'Ľ'),\n",
       " (166, 'Ś'),\n",
       " (167, '§'),\n",
       " (168, '¨'),\n",
       " (169, 'Š'),\n",
       " (170, 'Ş'),\n",
       " (171, 'Ť'),\n",
       " (172, 'Ź'),\n",
       " (174, 'Ž'),\n",
       " (175, 'Ż'),\n",
       " (176, '°'),\n",
       " (177, 'ą'),\n",
       " (178, '˛'),\n",
       " (179, 'ł'),\n",
       " (180, '´'),\n",
       " (181, 'ľ'),\n",
       " (182, 'ś'),\n",
       " (183, 'ˇ'),\n",
       " (184, '¸'),\n",
       " (185, 'š'),\n",
       " (186, 'ş'),\n",
       " (187, 'ť'),\n",
       " (188, 'ź'),\n",
       " (189, '˝'),\n",
       " (190, 'ž'),\n",
       " (191, 'ż'),\n",
       " (192, 'Ŕ'),\n",
       " (193, 'Á'),\n",
       " (194, 'Â'),\n",
       " (195, 'Ă'),\n",
       " (196, 'Ä'),\n",
       " (197, 'Ĺ'),\n",
       " (198, 'Ć'),\n",
       " (199, 'Ç'),\n",
       " (200, 'Č'),\n",
       " (201, 'É'),\n",
       " (202, 'Ę'),\n",
       " (203, 'Ë'),\n",
       " (204, 'Ě'),\n",
       " (205, 'Í'),\n",
       " (206, 'Î'),\n",
       " (207, 'Ď'),\n",
       " (208, 'Đ'),\n",
       " (209, 'Ń'),\n",
       " (210, 'Ň'),\n",
       " (211, 'Ó'),\n",
       " (212, 'Ô'),\n",
       " (213, 'Ő'),\n",
       " (214, 'Ö'),\n",
       " (215, '×'),\n",
       " (216, 'Ř'),\n",
       " (217, 'Ů'),\n",
       " (218, 'Ú'),\n",
       " (219, 'Ű'),\n",
       " (220, 'Ü'),\n",
       " (221, 'Ý'),\n",
       " (222, 'Ţ'),\n",
       " (223, 'ß'),\n",
       " (224, 'ŕ'),\n",
       " (225, 'á'),\n",
       " (226, 'â'),\n",
       " (227, 'ă'),\n",
       " (228, 'ä'),\n",
       " (229, 'ĺ'),\n",
       " (230, 'ć'),\n",
       " (231, 'ç'),\n",
       " (232, 'č'),\n",
       " (233, 'é'),\n",
       " (234, 'ę'),\n",
       " (235, 'ë'),\n",
       " (236, 'ě'),\n",
       " (237, 'í'),\n",
       " (238, 'î'),\n",
       " (239, 'ď'),\n",
       " (240, 'đ'),\n",
       " (241, 'ń'),\n",
       " (242, 'ň'),\n",
       " (243, 'ó'),\n",
       " (244, 'ô'),\n",
       " (245, 'ő'),\n",
       " (246, 'ö'),\n",
       " (247, '÷'),\n",
       " (248, 'ř'),\n",
       " (249, 'ů'),\n",
       " (250, 'ú'),\n",
       " (251, 'ű'),\n",
       " (252, 'ü'),\n",
       " (253, 'ý'),\n",
       " (254, 'ţ'),\n",
       " (255, '˙')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latin2_printable_characters = []\n",
    "# the Latin-2 character set has 256 codepoints, corresponding to\n",
    "# integers from 0 to 255\n",
    "for codepoint in range(256):\n",
    "    # the Latin-2 encoding is simple: each codepoint is encoded\n",
    "    # as the byte corresponding to that integer in binary\n",
    "    byte = bytes([codepoint])\n",
    "    character = byte.decode(encoding=\"latin2\")\n",
    "    if character.isprintable():\n",
    "        latin2_printable_characters.append((codepoint, character))\n",
    "\n",
    "latin2_printable_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the 8th bit (and thus the codepoint range [128; 256)) solves the problem of handling languages with character sets different than that of American English, but introduces a lot of complexity -- whenever you come across a text file with an unknown encoding, it might be in one of literally dozens of encodings. Additional drawbacks include:\n",
    "\n",
    "- how to handle multilingual text with characters from many different alphabets, which are not part of the same 8-bit encoding?\n",
    "- how to handle writing systems which have way more than 256 \"characters\", e.g. Chinese, Japanese and Korean (CJK) ideograms?\n",
    "\n",
    "For these purposes, a standard character set known as [**Unicode**](https://en.wikipedia.org/wiki/Unicode) was developed which strives for universal coverage of (ultimately) all characters ever used in the history of writing, even adding new ones like [emojis](https://unicode.org/emoji/charts/full-emoji-list.html). Unicode is much bigger than the character sets we've seen so far -- its most frequently used subset, the [Basic Multilingual Plane](https://en.wikipedia.org/wiki/Plane_%28Unicode%29#Basic_Multilingual_Plane), has 2^16 codepoints, but overall the number of codepoints is past 1M and there's room to accommodate many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the most straightforward representation for 2^16 codepoints is what? Well, it's simply using 16 bits per character, i.e. 2 bytes. That encoding exists, it's called `UTF-16` (\"UTF\" stands for \"Unicode Transformation Format\"), but consider the drawbacks:\n",
    "\n",
    "- we've lost `ASCII` compatibility by the simple fact of using 2 bytes per character instead of 1 (encoding \"a\" as `01100001` or `00000000|01100001`, with the `|` indicating an imaginary boundary between bytes, is not the same thing)\n",
    "- encoding a string in a language which is mostly written down using basic letters of the Latin alphabet now takes up twice as much space (which is probably not a good idea, given the general dominance of English in electronic communication)\n",
    "\n",
    "Looks like we'll have to think outside the box. The box in question here is called **fixed-width encodings** -- all of the encoding schemes we've encountered so far were fixed-width, meaning that each character was represented by either 7, 8 or 16 bits. In other word, you could jump around the string in multiples of 7, 8 or 16 and always land at the beginning of a character. (Not exactly true for `UTF-16`, because it is something more than just a \"16-bit `ASCII`\": it has ways of handling characters beyond 2^16 using so-called [surrogate sequences](https://en.wikipedia.org/wiki/UTF-16#U.2B10000_to_U.2B10FFFF) -- but you get the gist.)\n",
    "\n",
    "The smart idea that some bright people have come up with was to use a **variable-width encoding**. The most ubiquitous one currently is **`UTF-8`**, which we've already met in the HTML example above. `UTF-8` *is* `ASCII`-compatible, i.e. the 1's and 0's used to encode text containing only `ASCII` characters are the same regardless of whether you use `ASCII` or `UTF-8`: it's a sequence of 8-bit bytes. But `UTF-8` can also handle many more additional characters, as defined by the Unicode standard, by using progressively longer and longer sequences of bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'A' encoded in UTF-8 is: ['01000001']\n",
      "'č' encoded in UTF-8 is: ['11000100', '10001101']\n",
      "'字' encoded in UTF-8 is: ['11100101', '10101101', '10010111']\n"
     ]
    }
   ],
   "source": [
    "def print_utf8_bytes(char):\n",
    "    \"\"\"Prints binary representation of character as encoded by UTF-8.\n",
    "    \n",
    "    \"\"\"\n",
    "    # encode the string as UTF-8 and iterate over the bytes;\n",
    "    # iterating over a sequence of bytes yields integers in the\n",
    "    # range [0; 256); the formatting directive \"{:08b}\" does two\n",
    "    # things:\n",
    "    #   - \"b\" prints the integer in its binary representation\n",
    "    #   - \"08\" left-pads the binary representation with 0's to a total\n",
    "    #     width of 8, which is the width of a byte\n",
    "    binary_bytes = [f\"{byte:08b}\" for byte in char.encode(\"utf8\")]\n",
    "    print(f\"{char!r} encoded in UTF-8 is: {binary_bytes}\")\n",
    "\n",
    "print_utf8_bytes(\"A\")   # the representations...\n",
    "print_utf8_bytes(\"č\")   # ... keep...\n",
    "print_utf8_bytes(\"字\")  # ... getting longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does that even work? The obvious problem here is that with a fixed-width encoding, you just chop up the string at regular intervals (7, 8, 16 bits) and you know that each interval represents one character. So **how do you know where to chop up a variable width-encoded string, if each character can take up a different number of bits?**\n",
    "\n",
    "Essentially, the trick is to **use some of the bits** in the representation of a codepoint **to store information** not about which character it is (whether it's an \"A\" or a \"字\"), but **how many bits it occupies**. In other words, if you want to skip ahead 10 characters in a string encoded with a variable width-encoding, you can't just skip 10 * 7 or 8 or 16 bits; you have to read all the intervening characters to figure out how much space they take up. Take the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'B' encoded in UTF-8 is: ['01000010']\n",
      "'á' encoded in UTF-8 is: ['11000011', '10100001']\n",
      "'s' encoded in UTF-8 is: ['01110011']\n",
      "'n' encoded in UTF-8 is: ['01101110']\n",
      "'í' encoded in UTF-8 is: ['11000011', '10101101']\n",
      "'k' encoded in UTF-8 is: ['01101011']\n",
      "' ' encoded in UTF-8 is: ['00100000']\n",
      "'李' encoded in UTF-8 is: ['11100110', '10011101', '10001110']\n",
      "'白' encoded in UTF-8 is: ['11100111', '10011001', '10111101']\n"
     ]
    }
   ],
   "source": [
    "for char in \"Básník 李白\":\n",
    "    print_utf8_bytes(char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the initial bits in each byte of a character follow a pattern depending on how many bytes in total that character has:\n",
    "\n",
    "- if it's a 1-byte character, that byte starts with 0\n",
    "- if it's a 2-byte character, the first byte starts with 11 and the following one with 10\n",
    "- if it's a 3-byte character, the first byte starts with 111 and the following ones with 10\n",
    "\n",
    "This makes it possible to find out which bytes belong to which characters, and also to spot invalid strings, as the **leading** byte in a **multi-byte sequence** always \"announces\" how many **continuation** bytes (= starting with 10) should follow.\n",
    "\n",
    "So much for a quick introduction to `UTF-8` (= the encoding), but there's much more to Unicode (= the character set). While `UTF-8` defines only how integer numbers corresponding to codepoints are to be represented as 1's and 0's in a computer's memory, Unicode specifies how those numbers are to be interpreted as characters, what their properties and mutual relationships are, what conversions (i.e. mappings between (sequences of) codepoints) they can undergo, etc.\n",
    "\n",
    "Consider for instance the various ways diacritics are handled: \"č\" can be represented either as a single codepoint ([`LATIN SMALL LETTER C WITH CARON`](http://www.fileformat.info/info/unicode/char/010D/index.htm) -- all Unicode codepoints have cute names like this) or a sequence of two codepoints, the character \"c\" and a combining diacritic mark (`COMBINING CARON`). You can search for the codepoints corresponding to Unicode characters e.g. [here](http://www.fileformat.info/info/unicode/char/search.htm) and play with them in Python using the `chr(0xXXXX)` built-in function or with the special string escape sequence `\\uXXXX` (where `XXXX` is the hexadecimal representation of the codepoint) -- both are ways to get the character corresponding to the given codepoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "č\n",
      "č\n"
     ]
    }
   ],
   "source": [
    "# \"č\" as LATIN SMALL LETTER C WITH CARON, codepoint 010d\n",
    "print(chr(0x010d))\n",
    "print(\"\\u010d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "č\n",
      "č\n"
     ]
    }
   ],
   "source": [
    "# \"č\" as a sequence of LATIN SMALL LETTER C, codepoint 0063, and\n",
    "# COMBINING CARON, codepoint 030c\n",
    "print(chr(0x0063) + chr(0x030c))\n",
    "print(\"\\u0063\\u030c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'č'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# of course, chr() also works with decimal numbers\n",
    "chr(269)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means you have to be careful when working with languages that use accents, because **to a computer, the two possible representations are of course different strings**, even though to you, they're conceptually the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "č č\n"
     ]
    }
   ],
   "source": [
    "s1 = \"\\u010d\"\n",
    "s2 = \"\\u0063\\u030c\"\n",
    "# s1 and s2 look the same to the naked eye...\n",
    "print(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... but they're not\n",
    "s1 == s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watch out, **they even have different lengths**! This might come to bite you if you're trying to compute the length of a word in letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1 is 1 character(s) long.\n",
      "s2 is 2 character(s) long.\n"
     ]
    }
   ],
   "source": [
    "print(\"s1 is\", len(s1), \"character(s) long.\")\n",
    "print(\"s2 is\", len(s2), \"character(s) long.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this reason, even though we've been informally calling these Unicode entities \"characters\", it is more accurate and less confusing to use the technical term \"codepoints\".\n",
    "\n",
    "Generally, most text out there will use the first, single-codepoint approach whenever possible, and pre-packaged linguistic corpora will try to be consistent about this (unless they come from the web, which always warrants being suspicious and defensive about your material). If you're worried about inconsistencies in your data, you can perform a [normalization](https://en.wikipedia.org/wiki/Unicode_equivalence#Normalization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "# NFC stands for Normal Form C; this normalization applies a canonical\n",
    "# decomposition (into a multi-codepoint representation) followed by a\n",
    "# canonical composition (into a single-codepoint representation)\n",
    "s1 = normalize(\"NFC\", s1)\n",
    "s2 = normalize(\"NFC\", s2)\n",
    "\n",
    "s1 == s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap things up by saying that Python itself uses Unicode internally, but the encoding it defaults to when opening an external file depends on the *locale* of the system (broadly speaking, the set of region, language and character-encoding related settings of the operating system). On most modern Linux and macOS systems, this will probably be a `UTF-8` locale and Python will therefore assume `UTF-8` as the encoding by default. Unfortunately, Windows is different. To be on the safe side, whenever opening files in Python, you can specify the encoding explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unicode.ipynb\", encoding=\"utf-8\") as file:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, it's **always a good idea to specify the encoding explicitly, using `UTF-8` as a default** if you don't know, for at least two reasons -- it makes your code more:\n",
    "\n",
    "1. **portable** -- it will work the same across different operating systems which assume different default encodings;\n",
    "2. and **resistant to data corruption** -- `UTF-8` is more restrictive than fixed-width encodings, in the sense that not all sequences of bytes are valid `UTF-8`. E.g. if one byte starts with 11, then the following one *must* start with 10 (see above). If it starts with anything else, it's an error. By contrast, in a fixed-width encoding, *any* sequence of bytes is valid. Decoding will always succeed, but if you use the wrong fixed-width encoding, the result will be garbage, which you might not notice. Therefore, it makes sense to default to `UTF-8`: if it works, then there's a good chance that the file actually *was* encoded in `UTF-8` and you've read the data in correctly; if it fails, you get an explicit error which prompts you to investigate further.\n",
    "\n",
    "Another good idea, when dealing with Unicode text from an unknown and unreliable source, is to look at the set of codepoints contained in it and eliminate or replace those that look suspicious. Here's a function to help with that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata as ud\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def inspect_codepoints(string):\n",
    "    \"\"\"Create a frequency distribution of the codepoints in a string.\n",
    "    \n",
    "    \"\"\"\n",
    "    char_frequencies = Counter(string)\n",
    "    df = pd.DataFrame.from_records(\n",
    "        (\n",
    "            freq,\n",
    "            char,\n",
    "            f\"U+{ord(char):04x}\",\n",
    "            ud.name(char),\n",
    "            ud.category(char)\n",
    "        )\n",
    "        for char, freq in char_frequencies.most_common()\n",
    "    )\n",
    "    df.columns = (\"freq\", \"char\", \"codepoint\", \"name\", \"category\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your font configuration, it may be very hard to spot the two intruders in the sentence below. The frequency table shows the string contains regular `LATIN SMALL LETTER T` and `LATIN SMALL LETTER G`, but also their specialized but visually similar variants `MATHEMATICAL SANS-SERIF SMALL T` and `LATIN SMALL LETTER SCRIPT G`. You might want to replace such codepoints before doing further text processing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "      <th>char</th>\n",
       "      <th>codepoint</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>e</td>\n",
       "      <td>U+0065</td>\n",
       "      <td>LATIN SMALL LETTER E</td>\n",
       "      <td>Ll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>U+0020</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>Zs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>r</td>\n",
       "      <td>U+0072</td>\n",
       "      <td>LATIN SMALL LETTER R</td>\n",
       "      <td>Ll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>d</td>\n",
       "      <td>U+0064</td>\n",
       "      <td>LATIN SMALL LETTER D</td>\n",
       "      <td>Ll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>h</td>\n",
       "      <td>U+0068</td>\n",
       "      <td>LATIN SMALL LETTER H</td>\n",
       "      <td>Ll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>I</td>\n",
       "      <td>U+0049</td>\n",
       "      <td>LATIN CAPITAL LETTER I</td>\n",
       "      <td>Lu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>n</td>\n",
       "      <td>U+006e</td>\n",
       "      <td>LATIN SMALL LETTER N</td>\n",
       "      <td>Ll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>o</td>\n",
       "      <td>U+006f</td>\n",
       "      <td>LATIN SMALL LETTER O</td>\n",
       "      <td>Ll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>c</td>\n",
       "      <td>U+0063</td>\n",
       "      <td>LATIN SMALL LETTER C</td>\n",
       "      <td>Ll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>t</td>\n",
       "      <td>U+0074</td>\n",
       "      <td>LATIN SMALL LETTER T</td>\n",
       "      <td>Ll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>u</td>\n",
       "      <td>U+0075</td>\n",
       "      <td>LATIN SMALL LETTER U</td>\n",
       "      <td>Ll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>s</td>\n",
       "      <td>U+0073</td>\n",
       "      <td>LATIN SMALL LETTER S</td>\n",
       "      <td>Ll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>,</td>\n",
       "      <td>U+002c</td>\n",
       "      <td>COMMA</td>\n",
       "      <td>Po</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>g</td>\n",
       "      <td>U+0067</td>\n",
       "      <td>LATIN SMALL LETTER G</td>\n",
       "      <td>Ll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>𝗍</td>\n",
       "      <td>U+1d5cd</td>\n",
       "      <td>MATHEMATICAL SANS-SERIF SMALL T</td>\n",
       "      <td>Ll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>i</td>\n",
       "      <td>U+0069</td>\n",
       "      <td>LATIN SMALL LETTER I</td>\n",
       "      <td>Ll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>ɡ</td>\n",
       "      <td>U+0261</td>\n",
       "      <td>LATIN SMALL LETTER SCRIPT G</td>\n",
       "      <td>Ll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>k</td>\n",
       "      <td>U+006b</td>\n",
       "      <td>LATIN SMALL LETTER K</td>\n",
       "      <td>Ll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>U+002e</td>\n",
       "      <td>FULL STOP</td>\n",
       "      <td>Po</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    freq char codepoint                             name category\n",
       "0      5    e    U+0065             LATIN SMALL LETTER E       Ll\n",
       "1      5         U+0020                            SPACE       Zs\n",
       "2      3    r    U+0072             LATIN SMALL LETTER R       Ll\n",
       "3      3    d    U+0064             LATIN SMALL LETTER D       Ll\n",
       "4      3    h    U+0068             LATIN SMALL LETTER H       Ll\n",
       "5      2    I    U+0049           LATIN CAPITAL LETTER I       Lu\n",
       "6      2    n    U+006e             LATIN SMALL LETTER N       Ll\n",
       "7      2    o    U+006f             LATIN SMALL LETTER O       Ll\n",
       "8      2    c    U+0063             LATIN SMALL LETTER C       Ll\n",
       "9      1    t    U+0074             LATIN SMALL LETTER T       Ll\n",
       "10     1    u    U+0075             LATIN SMALL LETTER U       Ll\n",
       "11     1    s    U+0073             LATIN SMALL LETTER S       Ll\n",
       "12     1    ,    U+002c                            COMMA       Po\n",
       "13     1    g    U+0067             LATIN SMALL LETTER G       Ll\n",
       "14     1    𝗍   U+1d5cd  MATHEMATICAL SANS-SERIF SMALL T       Ll\n",
       "15     1    i    U+0069             LATIN SMALL LETTER I       Ll\n",
       "16     1    ɡ    U+0261      LATIN SMALL LETTER SCRIPT G       Ll\n",
       "17     1    k    U+006b             LATIN SMALL LETTER K       Ll\n",
       "18     1    .    U+002e                        FULL STOP       Po"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect_codepoints(\"Intruders here, good 𝗍hinɡ I checked.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... because of course, for a computer, the word \"thing\" written with two different variants of \"g\" is really just two different words, which is probably not what you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"thing\" == \"thinɡ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to put things into perspective, here's a diagram what happens when processing text with Python (\"Unicode\" in the central box stands for Python's internal representation of Unicode, which is **not** `UTF-8` nor `UTF-16`):\n",
    "\n",
    "<img alt=\"Text IO in Python\" src=\"http://www.nltk.org/images/unicode.png\" style=\"max-width: 100%;\">\n",
    "\n",
    "(Image shamelessly hotlinked from / courtesy of the [NLTK Book](http://www.nltk.org/book/). Go check it out, it's an awesome intro to Python programming for linguists!)\n",
    "\n",
    "> A terminological postscript: we've been using some terms a bit informally, but now that we have a practical intuition for what they mean, it's good to get the definitions straight in one's head. So, a **character set** is a mapping between **codepoints** (integers) and **characters**. We may for instance say that in our character set, the integer 99 corresponds to the character \"c\".\n",
    ">\n",
    "> On the other hand, an **encoding** is a mapping between a **codepoint** (an integer) and a **physical sequence of 1's and 0's that represent it in memory**. With fixed-width encodings, this mapping is generally straightforward -- the 1's and 0's directly represent the given integer, only in binary and padded with zeros to fit the desired width. With variable-width encodings, which have to explicitly encode information about how many bits are spanned by each codepoint, this straightforward correspondence breaks down.\n",
    ">\n",
    "> A comparison might be helpful here: as encodings, `UTF-8` and `UTF-16` both use **the same character set** -- the same integers corresponding to the same characters. But since they're **different encodings**, when the time comes to turn these integers into sequences of bits to store in a computer's memory, each of them generates a different one.\n",
    "\n",
    "For more on Unicode, a great read already hinted at above is Joel Spolsky's [The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)](http://www.joelonsoftware.com/articles/Unicode.html). Another great piece of material is the [Characters, Symbols and the Unicode Miracle](https://youtu.be/MijmeoH9LT4) video by the [Computerphile](https://www.youtube.com/channel/UC9-y-6csu5WGm29I7JiwpnA) channel on YouTube. To make the discussion digestible for newcomers, I sometimes slightly distorted facts about how things are \"really really\" done. And some inaccuracies may be genuine mistakes. In any case, please let me know in the comments! I'm grateful for feedback and looking to improve this material; I'll fix the mistakes and consider ditching some of the simplifications if they prove untenable :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
