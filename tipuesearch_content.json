{"pages":[{"title":"about me","text":"Hi! My name is David Lukeš and I'm primarily a phonetician/linguist interested in programming and NLP applications, an avid fan of Linux and a beginning user of OS X. I like Python and Clojure and would like to learn me a Haskell for great good ; I find R useful (though convoluted on occasion) and avoid Perl as much as I can. I work at the Institute of the Czech National Corpus , so if you're interested in Czech and/or corpus linguistics , check us out! We have a nice and sleek web interface called KonText (with the manatee corpus query engine as a back-end) for all your Czech-related queries ;) I hail from Prague, Czech Republic, the most beautiful city in the world, and work and live there, though I've lived in France and Belgium for a while when I was a kid. I play badminton and football (of the soccer persuasion -- but it's still football) and enjoy American Jewish literature (to the point of writing my BA thesis on Philip Roth), as well as Bill Bailey, Jon Stewart, Ricky Gervais, Stephen Fry and several other smart people who can make me laugh (and who, along with Frank Zappa and The Beatles, are collectively responsible for my lifelong crush on the English language). I used to play the saxophone reasonably well, but nowadays mostly enjoy playing the guitar atrociously bad and singing along. As you can tell from the title of this blog (if you're a fellow aficionado), I spent most of my teen years listening to the great music of one Frank Zappa . If you're interested in my academic (and more broadly, professional) credentials, they're available for inspection here .","tags":"pages","url":"pages/about.html"},{"title":"work","text":"Current position I'm coordinating phonetic transcription for the ORTOFON corpus project at the Spoken Corpora Section of the Institute of the Czech National Corpus , Faculty of Arts , Charles University in Prague . I also help out with various data processing tasks and functionality prototyping (mainly using Python, Perl and R). Projects For more, check out my GitHub profile . KonText Layout Switcher • a Tampermonkey/Greasemonkey script to customize the interface of the KonText corpus manager (see link for a screenshot) MluvKonk • an experimental viewer for spoken corpus concordances as exported from KonText , which tries to give more visual cues as to the structure of the dialogue; similar functionality in KonText itself is hopefully coming up soon! CNC Maps • an interactive web interface for displaying and manipulating concordances from the ORAL series corpora on a map. also features a component for browsing dialect recordings along with descriptions. TransVer • a transcription verifier for our current spoken data collection project at the Czech National Corpus. written in Clojure, using the wonderful seesaw wrapper library around the horrible swing GUI toolkit. my MA thesis (in czech) -- Perceptual sensitivity to music and speech stimuli in the frequency and temporal domains • if you're interested in the affinities between the ways humans process language and music, check it out! EXMARaLDA/EXAKT tutorial • a short tutorial on using the EXAKT corpus concordancer tool. mainly intended for internal use, but you might find something useful in there. if you speak czech, that is :) suggestions for improvement welcome! PraatEdit • a code editor for scripting the Praat speech analysis software environment, with syntax highlighting, written as a Java-learning project -- so expect bugs :) CV pdf (in Czech; update: November 2016)","tags":"pages","url":"pages/work.html"},{"title":"Monkey-patching in R","text":"While building a Shiny application with R recently, I've come across the need to invert the filterRange() function in the DT package, which provides a convenient high-level way to add DataTables to your Shiny app. As indicated by its name, this function filters a numeric column in your datatable based on a range, so as it contains only values contained within that range . What I needed was the opposite: include values outside the specified range . The filtering is done server-side and unfortunately, no option is provided out-of-the-box to perform this inversion. One of the solutions is therefore to monkey-patch the filterRange() function in the DT R package, replacing it with a version that filters the outer range instead. Googling for \"monkey patching r\" (currently) yields this blog post , which provides a more complicated though arguably cleaner solution, which introduces a new environment in the search path. My position on this is that if you're worried about cleanliness, you shouldn't be monkey-patching in the first place. Conversely, if you decide monkey-patching is acceptable in your situation, the code required should be as quick and dirty as the thought. Of course, this is R, uncontested king of weird ways of doing anything but the most common data analysis tasks, and even some of those -- so it's never going to be as simple as Python, for instance: import sys sys . stdin = \"foo\" # Aaand done. But it doesn't have to be as complicated as the solution in the blog post above, either. The solution presented here is basically taken from this mailing list post , which has the disadvantage of not containing the key term \"monkey-patch\", which makes it hard to find on Google. It consists in the following steps: Get a handle on the relevant library's namespace with getNamespace() . Make the relevant binding modifiable with unlockBinding() . Define your custom version of the function. Store it in the namespace under the original name. Re-seal everything with lockBinding() . Here's the code for my specific use case with DT::filterRange() : # Monkey patch the filterRange() function in the DT package so that server-side filtering returns # values *outside* the range instead of inside. DT <- getNamespace ( \"DT\" ) unlockBinding ( \"filterRange\" , DT ) #################################################################################################### # This part of the code is deliberately kept as similar to the original as possible, in order to # make potential updates easier. See https://github.com/rstudio/DT/blob/v0.2/R/shiny.R#L474. # filter a numeric/date/time vector using the search string \"lower ... upper\" filterRange = function ( d , string ) { if ( ! grepl ( '[.]{3}' , string ) || length ( r <- strsplit ( string , '[.]{3}' )[[ 1 ]]) > 2 ) stop ( 'The range of a numeric / date / time column must be of length 2' ) if ( length ( r ) == 1 ) r = c ( r , '' ) # lower, r = gsub ( '&#94;\\\\s+|\\\\s+$' , '' , r ) r1 = r [ 1 ]; r2 = r [ 2 ] if ( is.numeric ( d )) { r1 = as.numeric ( r1 ); r2 = as.numeric ( r2 ) } else if ( inherits ( d , 'Date' )) { if ( r1 != '' ) r1 = as.Date ( r1 ) if ( r2 != '' ) r2 = as.Date ( r2 ) } else { if ( r1 != '' ) r1 = as.POSIXct ( r1 , tz = 'GMT' , '%Y-%m-%dT%H:%M:%S' ) if ( r2 != '' ) r2 = as.POSIXct ( r2 , tz = 'GMT' , '%Y-%m-%dT%H:%M:%S' ) } if ( r [ 1 ] == '' ) return ( d <= r2 ) if ( r [ 2 ] == '' ) return ( d >= r1 ) d <= r1 | d >= r2 } # End pastiche of original DT code. #################################################################################################### DT $ filterRange <- filterRange lockBinding ( \"filterRange\" , DT ) The last piece of the puzzle concerns UX: the user should understand that the filter applies to the outer range, not the inner one. Visually: This is easily achieved with a few lines of CSS: # datatable-id . noUi-background { background : #3FB8AF ; box-shadow : inset 0 0 3 px rgba ( 51 , 51 , 51 , .45 ); transition : background 450 ms ; } # datatable-id . noUi-connect { background : #FAFAFA ; box-shadow : inset 0 1 px 1 px #f0f0f0 ; } In conclusion, monkey-patching is rarely the most elegant, debuggable and maintainable solution to a problem you're having. More often, it's actually the least elegant (etc.) one. But every once in a while, it's the simplest one, the one with the best hassle/reward ratio (until it comes back to bite you once your codebase has grown or assumptions about the monkey-patched code have changed). At any rate, if you need to resort to it, it's nice to have a quick, googlable how-to, hence this post.","tags":"floss","url":"monkey-patching-in-r.html"},{"title":"Text munching in R?","text":"R has been gaining traction as a language for data analysis. My feelings about the whole ecosystem are mixed -- it has some incredibly well-designed libraries and a top-of-the-game IDE , but the core language makes me cringe (it feels like \"Perl and Lisp: The Worse Parts\"). Be that as it may, it has undeniably become the go-to programming language for many people for whom programming is not their main breadwinner, many linguists among them. If you're one of these people and wondering whether it's worth undergoing the cognitive burden of learning another language and having to context-switch between them, read on! The main problem with R and large data is of course that R is fast as long as you can load everything into memory at once and used vectorized operations. The whole point of this post is that with the current size of a typical corpus, you often can't do that. You'll have to process the corpus line by line, which means using a for-loop, and these are notoriously slow in R. I'm not pretending this is some new discovery (it's not), I'm just trying to quantify how problematic this slowness is for processing large quantities of text (prohibitive, in my opinion), so that you don't have to figure it out for yourself and can get started learning Python right away instead ;) (Another big problem is that R doesn't have an efficient and versatile hash table data structure.) tl;dr If you're planning to process corpora of tens of millions of tokens or more -- spoiler alert, you probably shouldn't do it in R. Task The following details an informal test comparing the speed of R, Python, Rust and Perl at processing a large corpus file (~120M tokens, 1.5GB gzipped) and creating a frequency distribution of headwords per part-of-speech. The idea is to see whether R is a viable alternative in this domain, or whether the slowing down caused by the inability to use vectorized computations (because we can't load the entire thing into memory at once) will just be too much. R: 1 day (!) After quite some exploration of various alternatives for the individual subtasks, this is the program that I came up with: library ( stringi ) library ( hash ) # read input from STDIN con <- file ( \"stdin\" , open = \"rt\" ) pos_sets <- hash () start <- Sys.time () while ( TRUE ) { line <- readLines ( con , n = 1 ) if ( length ( line ) == 0 ) { break } # lines with tokens (as opposed to metadata) contain tabs if ( stri_detect_fixed ( line , \"\\t\" )) { # individual token attributes are tab-separated attrs <- stri_split_fixed ( line , \"\\t\" ) # for each token, we're interested in the lemma (headword)... lemma <- attrs [[ 1 ]][ 2 ] # ... and the part-of-speech, which is the first character of the tag tag <- attrs [[ 1 ]][ 3 ] pos <- stri_split_boundaries ( tag , n = 2 , type = \"character\" )[[ 1 ]][ 1 ] # build a per-part-of-speech frequency distribution as a nested hash: # { # \"noun\": { # \"cat\": 5, # \"dog\": 3, # ... # }, # \"verb\": { # \"look\": 2, # ... # }, # ... # } if ( is.null ( pos_sets [[ pos ]])) { pos_sets [[ pos ]] <- hash () } if ( is.null ( pos_sets [[ pos ]][[ lemma ]])) { pos_sets [[ pos ]][[ lemma ]] <- 1 } else { pos_sets [[ pos ]][[ lemma ]] <- pos_sets [[ pos ]][[ lemma ]] + 1 } } } # report running time diff <- Sys.time () - start cat ( sprintf ( \"Done in %g %s.\\n\" , diff , units ( diff )), file = stderr ()) Given the input corpus mentioned above, this code takes 1.33 days to run. Compared to other languages one might conceivably use (see below), this is just ridiculous. Now, I'm certainly not an expert in R, so there may be better ways of doing some of this. But I doubt such improvements, if any, would be of any practical relevance, because even reducing the running time to a tenth of the original duration wouldn't be enough. And if there is a way to go even further, say to a hundredth, which would begin to make R competitive, then I would argue that a language which lets you shoot yourself so spectacularly in the foot performance-wise if you're not hip to some clever tricks should just be avoided for tasks where said performance matters. With these general considerations out of the way, let's look at some details of how to implement this task in R. In many cases, it's unclear how you should even approach the problem in R, due to missing or confusing built-in functionality . As a result, in addition to having a lousy running time on this task, R also puts a strain on the programmer's time. Reading the data This sounds so basic it should be obvious, right? Not so fast. First of all, the file(\"path/to/file\") function creates a file connection, which is however not open unless you also specify a mode in the open= argument, or alternatively, unless you call the open() function on the connection. Why you would want to create a connection that's not open is beyond me, but R adds insult to injury by allowing readLines() to work on a closed connection: it just opens the connection before doing the reading and closes it afterwards. This means that repeated calls to readLines(unopened_connection, n=1) will repeatedly read the first line of the file , which is most likely not what you want. This is API design level PHP. Second, the corpus is gzip compressed, so you'll need to uncompress it. There are basically two options: have an external program ( zcat ) do the decompression and pipe the data into R via STDIN handle the decompression within R itself As a general rule (for any language), it will always be faster to handle the decompression in a different process on a multi-core system, because the tasks can proceed in parallel. 1 On the other hand, it's more portable not to depend on external programs, and R does have a built-in function to open a connection to a gzipped file, namely gzfile() . Based on tests on shorter inputs, it's about 50% slower than external decompression, which is a somewhat worse performance deterioration than e.g. Python (40% based on the full input). In light of the already dire running time, it's something we can't really afford. Third, having to do the line-by-line reading in a while (TRUE) loop, using a function called readLines() (note the plural) with an argument of 1 , checking the length of the resulting character vector in order to determine the end of the input -- that's just gross. String manipulation R has built-in functions for string matching ( grepl() et al.), not so much for string splitting. This is the point where I got suspicious of the performance of everything and started testing alternatives. I finally ended up using the stringi package, which is fast and has a fairly consistent API. stringr is a set of higher-level wrappers around it, which have however proven somewhat slower than the built-ins in my highly informal testing. O hash map, where art thou? Building a per-part-of-speech frequency distribution of headwords requires an appropriate data structure. As indicated in the comments in the R source, we want to build a nested collection that looks something like this: { \"noun\": { \"cat\": 5, \"dog\": 3, ... }, \"verb\": { \"look\": 2, ... }, ... } The requirements on the data structure we need are the following: it's a collection strings can be used as keys it can be arbitrarily nested key lookup is fast, i.e. constant time In other words, we need a hash (or a dict, in Python terminology). R doesn't have a hash (I'll qualify this statement in a bit). The workhorse data structure in R that satisfies points 1--3 is a list. Unfortunately, it has linear access time . That's not going to work. R also has environments, which it uses to store and access variables. Under the hood, environments are implemented as hashes, but using them as such is a massive pain, because their API isn't meant for it. Fortunately, there's a wrapper package which makes it more convenient. Unfortunately, environments weren't optimized with this use case in mind. They were designed to hold key--value (variable name--variable value) pairs explicitly defined by people as part of their programs, not millions of items extracted from data. As a result, they slow down dramatically once the number of items grows large. (The article in the previous link provides a survey of the state of the art of fast key lookup in R. The state of the art is... dismal. Your only option is basically indexing a data table, which is fine for a finalized data set, but useless when building the data set -- you can't afford to reindex after each new data point.) There's also the hashmap library , which is a wrapper around C++ Boost hashes. However, it doesn't do nesting, so it's of no use to us, and of very limited usefulness in general. Conclusion: technically, we have to concede that R has hashes, but for all practical intents and purposes, it doesn't . There's one last twist, though. Funnily enough, in our use case, it turns out it doesn't really matter anyway. Indeed, it seems the performance of for-loops in R is so egregiously bad that it dwarfs even the inefficiencies accrued by the linear lookup time of lists: if you reimplement the script with lists, it takes just a little longer than the version with hashes, about 1.36 days. (Or maybe it's just that the performance of environment-based hashes becomes so bad when they grow large as to be comparable with that of lists? Who knows, and frankly, I don't care enough to want to find out. If it's the for-loops though, then adding efficient hashes to R won't really solve anything.) Summary If you like R and your reaction to this is, \"That's not fair! R was never meant to do any of this, that's why everything feels so backhanded.\" -- then good, that's basically the gist of this post: don't use R for something it wasn't meant to do . What are the alternatives, then? Python: 5 minutes Yes, that's right. It takes Python 5 minutes to do the same task that took R over a day . The code feels a lot simpler too: import sys import time def main (): pos_sets = {} start = time . time () for line in sys . stdin : if \" \\t \" in line : _ , lemma , tag , _ = line . split ( \" \\t \" , maxsplit = 3 ) pos = tag [ 0 ] # this is an intentionally naive implementation which mimicks # the R code and something an inexperienced coder might do; # a more concise and probably better performing solution could # be achieved using dict.setdefault() or collections.defaultdict # / collections.Counter if pos not in pos_sets : pos_sets [ pos ] = {} if lemma not in pos_sets [ pos ]: pos_sets [ pos ][ lemma ] = 1 else : pos_sets [ pos ][ lemma ] += 1 diff = time . time () - start print ( f \"Done in {diff:.0f} seconds.\" , file = sys . stderr ) if __name__ == \"__main__\" : main () Perl: 13 minutes Perl used to be a popular alternative for text processing. Like R, it has its fair share of nauseating language design and weird quirks, but since it was actually meant for use in this domain, it won't spectacularly let you down. (Unless your data is silently corrupted because you handled text encoding wrong. Perl's behavior in this respect is a relict of a pre-UTF-8-everywhere past, and it's the single biggest reason for why the language should be put out of its misery already.) Here's the code: use strict ; use utf8 ; use open qw(:std :encoding(utf8)) ; my $start = time (); my %pos_sets = (); while ( <> ) { if ( /\\t/ ) { my @attrs = split /\\t/ ; my $lemma = @attrs [ 1 ]; my $tag = @attrs [ 2 ]; my $pos = substr $tag , 0 , 1 ; # auto-vivification: ergonomic, but also made possible by the whole # \"implicit defaults that have a potential of screwing stuff up # without you even knowing about it\" culture of Perl $pos_sets { $pos }{ $lemma } += 1 ; } } my $diff = time () - $start ; print STDERR \"Done in $diff seconds.\\n\" ; Bottom line though, being more than twice as slow as Python (which came as a surprise to me, I must admit) and definitely the worse language, it has little to recommend itself if you're considering to learn a new language for this type of task. Except maybe if you want to continuously log what the program is doing to a terminal -- like output the number of lines processed after each line. Perl is clearly very efficient at writing to a terminal, the running time is basically the same with continuous logging incorporated. By contrast, Python takes about three times longer (~ 15 minutes). (I guess maybe Python flushes output after each print() call, whereas Perl does some smart buffering which results in it not being slowed down by the latency of the terminal...? Who knows, at any rate, it's hardly a \"killer\" feature.) Rust: 1.25 minutes As a compiled, systems-level language, Rust is in a different league compared to the previous contestants: of course it's going to be faster. I included it because it provides a frame of reference. The important takeaway is that we're in the same ballpark with Python (roughly units of minutes), so there's no pressing need to turn to a compiled language for this task. Here's the code, for completeness sake: use std :: io ; use std :: io :: prelude :: * ; use std :: collections :: HashMap ; use std :: time ; type LemmaCount = HashMap < String , i32 > ; type PosSet = HashMap < char , LemmaCount > ; fn main () { let start = time :: SystemTime :: now (); let mut pos_sets = PosSet :: new (); let stdin = io :: stdin (); for line in stdin . lock (). lines () { let line = line . unwrap (); if line . contains ( \" \\t \" ) { let mut attrs = line . split ( \" \\t \" ). skip ( 1 ). take ( 2 ); let lemma = attrs . next (). unwrap (); let tag = attrs . next (). unwrap (); let pos = tag . chars (). take ( 1 ). next (). unwrap (); let pos_set = pos_sets . entry ( pos ). or_insert ( LemmaCount :: new ()); let count_for_lemma = pos_set . entry ( String :: from ( lemma )). or_insert ( 0 ); * count_for_lemma += 1 ; } } let diff = start . elapsed (). unwrap (). as_secs (); println ! ( \"Done in {:.0} seconds.\" , diff ); } Note in passing how nicely the Rust code reads for a compiled language. Of course, since it's a much stricter (and safer) language than Python, it's more ceremonious to write and the APIs are more complicated, because they have to adhere to the various memory management guarantees Rust gives you (among other things). But once the code is written, it's very readable and clear. And all necessary functions and data structures are (a) available in the standard library, and (b) plenty efficient. Conclusion Just to be clear: the ultimate purpose of this post is not bashing R (not for being slow at text munching, at any rate); it's to give a convincing account of why it's just not the right tool for the job. And not in a small way, either -- in a way that requires to learn a different tool, there's no way around it. Let me reiterate that my recommendation would hands down be Python . Once the data is extracted, go back to R by all means. Although Python does have a fairly nice high-level data analysis library , it's not my intention to discourage anyone from using R for what it is good at, especially if this is a skill they are already proficient in. The internet is full of people asking advice on which programming language to learn, and the answers are invariably evasive -- it depends on your tastes, what fits your brain better, what your use case is. In the hopes that some people might find opinionated guidance useful for a change (I know I personally often do, when flirting with a new language): if you're looking to process large quantities of text data, the answer is a big, resounding NOT R ! You could also offload the decompression to a different thread in the same process, but that complicates the implementation. Piping gives you parallelization basically for free. ↩","tags":"ling","url":"text-munching-in-r.html"},{"title":"\"Responsive\" iframes, e.g. for DokuWiki and Shiny","text":"Sometimes, the best way to embed an interactive element into a website is to use an iframe. Obviously, not when your website is a webapp and that element represents the main functionality it's supposed to provide -- that would be gross. But when your website is mostly textual / graphical content, typically authored within a wiki or blogging platform, and you just want to include this one element to liven it up, iframes are actually a decent (and perhaps the only?) solution. Trouble is, you probably want this Frankenstein monster to actually look good, i.e. seamless if at all possible. But iframes don't have automatic vertical resizing according to their content, which means you'll need to take care of that manually. How? By using the JavaScript messaging API for communication between parent and child frames to send information about window resize events (from parent to child) and height updates (from child to parent). Let's imagine you have a DokuWiki article in which you want to embed a small Shiny app. If you just embed it in your dokuwiki code using an iframe, taking care to remove the border and stretch it horizontally... < html > < iframe id = \"embedded-app\" src = \"https://your.shiny.app/url\" frameborder = \"0\" width = \"100%\" ></ iframe > </ html > ... this will happen: Eww, scrollbar. Messaging to the rescue! First of all, you need to teach your embedded web page to send information about its height to the parent at appropriate times. This can be achieved by adding this piece of JavaScript to it: ( function () { //////////////////////////////////////////// // CONFIGURE THESE TO MATCH YOUR USE CASE // //////////////////////////////////////////// // set this to a selector for the element that contains the entire UI // you want to access via the iframe -- for a Shiny app, it might be // a div with Bootstrap's container-fluid class var containerSelector = \".container-fluid\" ; // this should be the root URL of the parent frame (DokuWiki) which you want // to allow to send messages to the child var allowedOrigin = \"https://dokuwiki.example.com\" /////////////////////// // END CONFIGURATION // /////////////////////// function sendHeightOf ( querySelector ) { var container = document . querySelector ( querySelector ); if ( container . scrollHeight !== undefined ) { var h = container . scrollHeight ; parent . postMessage ( h , \"*\" ); } else { console . log ( \"No element corresponding to querySelector \" + querySelector + \" found, or element did not have property scrollHeight.\" ); } } // cross-browser compatible infrastructure var eventMethod = window . addEventListener ? \"addEventListener\" : \"attachEvent\" ; var eventer = window [ eventMethod ]; var messageEvent = eventMethod == \"attachEvent\" ? \"onmessage\" : \"message\" ; // listen for resize message from parent window (see point ② below) eventer ( messageEvent , function ( e ) { if ( e . origin == allowedOrigin ) { sendHeightOf ( containerSelector ); } else { console . log ( \"Was expecting a message from \" + allowedOrigin + \", got \" + e . origin + \" instead.\" ); } }); window . onload = function () { // inform parent at least once after load (see point ① below) sendHeightOf ( containerSelector ); // monitor self-initiated changes in size (see point ③ below) var mo = new MutationObserver ( function () { sendHeightOf ( containerSelector ); }); mo . observe ( document , { subtree : true , childList : true , characterData : true }); }; })(); What are these \"appropriate times\" mentioned above? The code above implements the following ones, which should be generic enough to cover most situations: on initial page load on window resize (see below, the parent frame has to send a message to the child frame that it has been resized, to which the child responds with a size update message) on any kind of mutation of the DOM inside the child frame (not a full reload of the entire page, that would be handled by point ① above), which might affect the size of the rendered component On the parent (DokuWiki) side, you then need to handle the incoming size update messages from the child frame, and send resize messages when the window is resized. This can be achieved with the following DokuWiki markup: < html > < iframe id = \"embedded-app\" src = \"https://your.shiny.app/url\" frameborder = \"0\" width = \"100%\" ></ iframe > < script > ( function () { //////////////////////////////////////////// // CONFIGURE THESE TO MATCH YOUR USE CASE // //////////////////////////////////////////// // this should be the root URL of the child frame (Shiny app) which you want // to allow to send messages to the parent var allowedOrigin = \"https://your.shiny.app\" /////////////////////// // END CONFIGURATION // /////////////////////// var embeddedApp = document . getElementById ( \"embedded-app\" ); function resizeIframe ( pixels ) { embeddedApp . style . height = pixels + \"px\" ; } // cross-browser compatible infrastructure var eventMethod = window . addEventListener ? \"addEventListener\" : \"attachEvent\" ; var eventer = window [ eventMethod ]; var messageEvent = eventMethod == \"attachEvent\" ? \"onmessage\" : \"message\" ; // listen to message from iframe eventer ( messageEvent , function ( e ) { if ( e . origin === allowedOrigin ) { var key = e . message ? \"message\" : \"data\" ; var data = e [ key ]; resizeIframe ( data ); } else { console . log ( \"Was expecting a message from \" + allowedOrigin + \", got \" + e . origin + \" instead.\" ); } }, false ); // send message to iframe on window resize window . onresize = function () { embeddedApp . contentWindow . postMessage ( \"parentWindowResized\" , \"*\" ); }; })(); </ script > </ html > And the result? Yay! And of course, the iframe gets resized as needed when display conditions change: Cue bittersweet feeling after having figured out a workaround for such a specific use case that you're not quite sure it was worth putting all that effort into it in the first place...","tags":"floss","url":"responsive-iframe.html"},{"title":"The Cathedral and the Bazaar: What is a Useful Notion of \"Language\"?","text":"If you like the essay, then you'll definitely want to take a look at Luc Steels's The Talking Heads Experiment: Origins of Words and Meanings . It's published as an open-access book by Language Science Press, so go grab the free download ! Abstract The essay analyzes why Noam Chomsky's notion of language (both its essence — language as a set of grammatical sentences — and genesis) leads neither to interesting discoveries nor even to useful questions from the point of view of linguistics as a science. A much more fruitful approach to language is to view it as a complex, dynamic, distributed system with emergent properties stemming from its functions, as advocated e.g. by Luc Steels. The argument will be developed against the backdrop of the evolution of Ludwig Wittgenstein's thought, from the Tractatus to the concept of language games, i.e. from an approach to language based on thorough formal analysis but also misconceptions about its functions, to a much keener though less formal grasp of its praxis and purpose. Introduction At least since Thomas Kuhn's The Structure of Scientific Revolutions , it has been a fairly commonplace notion that working within the confines of a particular scientific paradigm conditions to a certain extent the questions one is likely to ask and therefore also the answers that ensue. This effectively limits the range of possible discoveries, because some are not answers to meaningful questions within a given framework while other observations still are taken as given axioms, which means they cannot be the target of further scientific investigation. In contemporary linguistics, one very prominent such paradigm is that of generative grammar , single-handedly established in 1957 by Noam Chomsky in his seminal work Syntactic Structures . While serious criticism has been leveled over time against this initial exposition as well as Chomsky's subsequent elaborations on it (see Pullum 2011; Sampson 2015; and Sampson 2005 for a book-length treatment), the book undeniably attracted significant numbers of brilliant young minds under the wings of its research program, which went from aspiring challenger in the domain of linguistics to established heavyweight in a comparatively short period of time (the transition had been achieved by the mid-1970s at the latest). In the process, it co-opted or spawned various other sub-fields of linguistics, and even rebranded itself, such that Cartesian linguistics , cognitive linguistics and most recently biolinguistics are all labels which suggest a strong generativist presence. One serious competitor to the Chomskyan account of language that has emerged over the years is the field of evolutionary linguistics . It might seem strange at first glance why biolinguistics and evolutionary linguistics should be at odds. As their names indicate, they both aspire to a close relationship with biology, which seems to indicate their research agendas and outlooks should largely overlap. Yet their fundamental assumptions about what constitutes language are so irreconcilable that they might as well be considered to deal with different objects of study. Of the two, it is evolutionary linguistics which leads to questions and investigations which can be conceived of as scientific (in the Popperian sense of involving falsifiable hypotheses instead of being merely speculative), consequently yielding the most useful insights – in the fairly pedestrian sense that these can be intersubjectively replicated without resorting to an argument from authority, which makes them a better foundation to build upon, because the superadded structures are less likely to crumble should said authority ever change their mind, as Chomsky has done several times already. Wittgenstein on language: From logical calculus to language games Let us now take a short détour through the development of Ludwig Wittgenstein's thoughts on language, so that we may couch our later discussion of the differences between generative grammar / biolinguistics and evolutionary linguistics in terms of a contrast that is perhaps more familiar. The imagery in the title of the present essay was borrowed from Eric S. Raymond's book The Cathedral & the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary (Raymond 1999). In it, Raymond describes two models of collaborative software development, one of them very rigid, restrictive and hostile to newcomers (the \"cathedral\"), the other overwhelmingly inclusive, open to outside contributions and organic change, an effervescent hive of activity (the \"bazaar\"), whose unexpected but empirically demonstrable virtues he has come to embrace. This architectural metaphor also happens to be very apt when characterizing Wittgenstein's view of language in the two major stages of his thought, as represented by his two books Tractatus Logico-Philosophicus and Philosophical Investigations . In the Tractatus , Wittgenstein has a \"preconceived idea of language as an exact calculus operated according to precise rules\" (McGinn 2006, 12) and formalizing this system of rules leads him to the following dogmatic conclusion: \"what can be said at all can be said clearly, and what we cannot talk about we must pass over in silence\" (Wittgenstein 2001, 3). The deontic force of the final injunction should be taken with a grain of salt; it could perhaps be rephrased in the following less epigraph-worthy manner: there is a sharp logical boundary to be drawn between meaningful and nonsensical propositions, and the purpose of language is to construct meaningful ones, therefore it is futile (rather than strictly forbidden) to engage in nonsensical ones. There have been attempts to read the Tractatus in an ironic mode, as a consciously doomed, self-defeating attempt to circumscribe the limits to the expression of thought, which prefigures the much more subtle attitude towards language that Wittgenstein later exhibits in the Philosophical Investigations (see McGinn 2006, 5–6 and elsewhere for an overview of this so-called \"resolute\" reading). In my opinion, such a stance exhibits a blatant, possibly wilful disregard of his almost penitent tone in the preface to Philosophical Investigations : \"I could not but recognize grave mistakes in what I set out in that first book\" (Wittgenstein 2009, 4e). Where does Wittgenstein think he went wrong then? Arguably, the most serious misconception was conferring a privileged ontological status to language, seeing it as \"the unique correlate, picture, of the world\" (Wittgenstein 2009, 49e), whereas in fact, these referential properties are highly dependent on communicative context. It signifies only insofar as it has an effect on the addressee (another human being, or even myself) which to all practical intents and purposes the speaker can identify as somehow related to what she was trying to achieve with her utterance in the first place. But there is little meaning, in any practical sense, outside these highly particular, localized (in both physical and cultural space and time), embodied, grounded interactions. Wittgenstein calls these interactions \"language-games\" to \"emphasize the fact that the speaking of language is part of an activity, or of a form of life\" (Wittgenstein 2009, 15e, original emphasis). Of course, the notion of game still involves some kind of rules, but by focusing on the activity rather than its regulations, it is much easier to account for \"the case where we play, and make up the rules as we go along […] and even where we alter them – as we go along\" (Wittgenstein 2009, 44e). This is not to say that we cannot construct abstractions, although Wittgenstein himself is clearly in favor of systematically examining particular cases: \"In order to see more clearly, here as in countless similar cases, we must look at what really happens in detail , as it were from close up\" (Wittgenstein 2009, 30e). However, once we do abstract away, it is crucial to approach the resulting theory from a pragmatic standpoint: \"We want to establish an order in our knowledge of language: an order for a particular purpose, one out of many possible orders, not the order\" (Wittgenstein 2009, 56e, original emphasis). Generative grammar In many ways, Chomsky conceives of language as the early Wittgenstein did, i.e. under the cathedral metaphor. This may come across as a surprise because unlike Wittgenstein, he is not concerned with issues of meaningfulness, philosophical or otherwise. Indeed, it is one of his fundamental precepts that grammar can and should be dissociated from meaning, as demonstrated by his famous example sentence \"Colorless green ideas sleep furiously\", which he claims is perfectly grammatical yet meaningless 1 (Chomsky 2002, Chap. 2). Nevertheless, the goal for both is to describe language per se , in the abstract, without regard to its context-grounded use in actual communication. Both strive to give a highly formal definition of the system they think they are uncovering: while Wittgenstein attempts to establish a logical calculus of how propositions can be said to carry meaning in terms of their referential relationship to external reality, Chomsky tries to hint at a calculus which would determine which candidate sentences belong to the language encoded by this calculus, i.e. separate those that are grammatical from those that are not. Another way to put this is that the descriptive part of linguistics can be equated with formal language theory: a language is viewed as a (potentially infinite) set of symbol strings (sentences), and the linguist's task is to find the simplest and most elegant set of rules that would constitute the basis for a procedure to generate (hence generative grammar) all of them and only those, whether observed or potential. Chomsky himself gives the following definition: \"by a generative grammar I mean simply a system of rules that in some explicit and well-defined way assigns structural descriptions to sentences\" (Chomsky 1965, 8). Furthermore, \"Linguistic theory is concerned primarily with an ideal speaker-listener, in a completely homogeneous speech-community, who knows its language perfectly\" (Chomsky 1965, 3). Specifically, it is concerned with his \" competence (the speaker-hearer's [intrinsic] knowledge of his language)\", not his \" performance (the actual use of language in concrete situations)\" (Chomsky 1965, 4). Additionally, no claim is made as to the cognitive or neurophysiological accuracy of the mechanisms described, although to hedge his bets both ways, Chomsky adds that \"No doubt, a reasonable model of language use will incorporate, as a basic component, the generative grammar that expresses the speaker-hearer's knowledge of the language\" (Chomsky 1965, 9). In short, Chomsky consciously sets up the playing field for a thoroughly mentalistic, speculative discipline. At first, it might seem like reasonable approximation and a small concession to make, especially in the face of the sheer daunting complexity of all the intricate mechanisms that conspire to yield the phenomenon we call language, but only until one fully realizes the consequences of such a move. Observe for instance the carefully crafted loophole claiming that linguistics is primarily concerned with an ideal speaker-hearer's competence and that actual usage data is just circumstantial evidence. This effectively allows linguists to dismiss inconvenient edge cases or counterexamples to their theories purely on grounds of their being noisy data or slips of the tongue, which is something they are allowed to determine based on introspection. Mind you, this is not just a theoretical loophole; Chomsky himself has repeatedly relied on it, especially with respect to so-called linguistic universals (see e.g. Sampson 2005, 139 or 160). The net result is rampant, unchecked theorizing. One such example is the postulation of a two-layer linguistic analysis, the observed language data corresponding to a surface structure which provides hints as to an underlying, more regular deep structure to be uncovered. The deep structure is purportedly closer to the universal properties of language; both layers are linked by a system of transformations: We can greatly simplify the description of English and gain new and important insight into its formal structure if we limit the direct description in terms of phrase structure to a kernel of basic sentences (simple, declarative, active, with no complex verb or noun phrases), deriving all other sentences from these (more properly, from the strings that underlie them) by transformation, possibly repeated. (Chomsky 2002, 106–7) Constructing a formal framework for grammar modeling with cognitively unmotivated levels of abstraction might have been a valid goal (though arguably not within linguistics) if the result was indeed, as Chomsky claims it to be, maximally elegant, as simple as can be but no simpler. I was not able to track down a formal definition of this criterion, but simplicity is clearly discursively construed as a desirable quality: \"simple and revealing\" (Chomsky 2002, 11) or \"effective and illuminating\" (Chomsky 2002, 13) are Chomsky's choice epithets for what to look for in a grammar. But that is not true either: transformations are an unnecessary addition, singling them out as a separate category of operations adds nothing to the generative power of his system (Pullum 2011, 290). They are therefore a wart under any reasonable definition of \"simplicity\" and Chomsky thus manages to fall short of even the self-defined, theory-internal standards that are the only ones he allows his enterprise to be held to. Ontogeny and phylogeny As we have seen, generative grammar concerns itself with an ideal speaker-hearer's competence in a perfectly homogeneous community. The trouble is that such an impoverished model eschews any possibility of dynamism. The very dichotomy between grammatical and ungrammatical is intuitively problematic if we consider that judgments are bound to diverge when made in reference to different dialects, sociolects and idiolects, 2 not to mention that binary classification might be too reductive in some cases (how would you categorize, on first encounter, a construction which you passively understand but would never produce actively?). Though Chomsky sometimes mentions in passing the possibility of levels of grammaticalness which would allow a finer-grained analysis (e.g. Chomsky 2002, 16; Chomsky 1965, 11), it seems to be just another instance of hedging his bets, he never makes it a fundamental component of his theory. This would seem to indicate that Chomsky's theory of language has a serious problem in that it is unable to account for any phenomena that involve fluctuations in linguistic ability, including language emergence / diachronic change (phylogeny) and acquisition (ontogeny). Chomsky's response to this is that our language faculty is largely innate: we are genetically endowed with a language-acquisition device in our brains (Chomsky 1965, 31–33) which can supposedly infer the correct grammatical rules even given incomplete, limited and noisy input, which is what Chomsky argues children get (the \"poverty of stimulus\" argument), thanks to strong universal constraints on what a human language can be like. Under this account, the capacity for language, initially \"a language of thought, later externalized and used in many ways\" (Chomsky 2007, 24), appeared as a random mutation in a single individual and progressively spread through the population because it offered a considerable competitive advantage: \"capacities for complex thought, planning, interpretation\" (Chomsky 2007, 22). In his later career, Chomsky increasingly focused on exploring this purported shared genetic basis for human language, hence the aforementioned label \"biolinguistics\". Make no mistake, this in no way entails a turn from mentalism towards empirical neurophysiological or genetic investigation. Quite to the contrary, liberated from the constraints of having to account for individual existing languages in detail, he soars to new heights of abstractness in postulating the formal language underpinnings of human language. The principles and parameters model of Universal Grammar (Chomsky 1986) expands upon the notion of linguistic universals by splitting them up into two sets: principles, which are hardwired and immutable, and parameters, which are hardwired too but can be flipped on or off based on linguistic behavior observed by the child in her particular language community. Since the choices are heavily constrained, the learner can infer correct parameter settings in spite of deficient input. This line of research culminates in the so-called minimalist program (Chomsky 1995), where Chomsky identifies the \"core principle of language, [the operation of] unbounded Merge\" (Chomsky 2007, 22). Under the \"strong\" minimalist hypothesis, this would be the only principle necessary to account for human-like languages (Chomsky 2007, 20), which would paradoxically essentially discard all work (or should I say speculation?) previously done on the parameters side of the Universal Grammar project. All other universal characteristics of language could then be explained by newly introduced \"interface\" conditions, 3 i.e. constraints on how language inter-operates with other systems, including thought and physical language production (Chomsky 2007, 14); 4 all empirically documented variation between the world's languages would be chalked up to lexical differences (Chomsky 2007, 25). The poverty of stimulus and language universals arguments for innateness have been thoroughly debunked, especially in Geoffrey Sampson's book-length diatribe The ‘ Language Instinct ' Debate . In short, it turns out that some of the grammatical constructions which were assumed to be absent from a language learner's input yet acquired nonetheless have since been empirically proven to occur fairly commonly (Sampson 2005, 72–79). Moreover, there is no qualitative difference between a statement like \"the stimulus is too poor to allow language learning without a genetic basis\" and \"the stimulus is just rich enough etc.\", both are unverifiable unless we have already independently proven that language learning occurs one way or the other, so to adduce either of the statements as proof for the hypothesis at stake is misguided (Sampson 2005, 47–48). Finally, the alleged language universals turn out to be either false when checked against additional languages (Sampson 2005, 138–9) or so general as to be meaningless (Sampson 2005, Chap. 5). Irrespective of this, let us suppose for a moment that genetic mutation and subsequent inheritance do play a role in the emergence of language, and work out an account of language emergence consistent with this hypothesis. If language started out through mutation in a single individual as a purely internal advanced conceptualization faculty, then once it started to spread, what was the motivation for the genetically-endowed humans to externalize their thoughts? How did they know to which of their peers they could speak (which had inherited the mutation) and which not? And most importantly, how did they know which parameters of Universal Grammar to flip on and which off, if there was no prior language based on which to decide? Universal Grammar would have had to be fairly detailed in order for intersubjective agreement on the norms for the first ever human language to be reached on the basis of it alone. Yet as we have seen, Chomsky has been moving away from this notion – at the limit, the minimalist program posits only one very general mechanism required for language. The poverty of stimulus argument is turned against its creator as the argument from poverty of the machinery supposed to make up for the poverty of said stimulus. Alternatively, if we fully subscribe to the minimalist program and the notion that all the surface variety exhibited by language comes from the lexicon, then how are individual words created, how do they propagate? One might be tempted to say \"people just invented them\", but consider for a while that in the current setup, there is absolutely no mechanism that would explain how a community of speakers reaches agreement on their lexicon – this theory offers no incentive whatsoever for consensus to be reached; from its point of view, a solution where each speaker ends up with their own private lexicon is equally valid because indistinguishable on the basis of the theory's conceptual apparatus. Chomsky's ideas on phylogenesis appear thoroughly ridiculous when fully carried out to their logical consequences, and this can all be blamed on his sterile, idealized and static view of language which dismisses actual communication as a secondary purpose and therefore a peripheral issue. On a side note, it is hard to say which aspect of Chomsky's theory of language came first – whether innateness accommodated the mentalism and the concomitant quest for formal purity (botched as it may be) of generative grammar, whether it was the other way round, or whether they perhaps co-evolved in his mind. The facts are that Chomsky's initial publications on generative grammar concentrate on the formal language theory part (Chomsky 1956; Chomsky 2002), but he added the innateness argument fairly early on, even tacking a seemingly respectable philosophical lineage onto it in Cartesian Linguistics (Chomsky 2009), which pretends to trace back both innateness and mentalism to Descartes and the Port-Royal grammarians, binding them as two sides of the same coin. It is worth noting that in both formal language theory and history of linguistics / philosophy, Chomsky is more of a dabbler than an expert: he has provably borrowed most of his ideas in the former field from others, sometimes mangling them or extending them in unfortunate ways (Pullum 2011; Sampson 2015), and has thoroughly underresearched (or wilfully twisted?) his understanding of the latter, which has resulted in serious misrepresentations of the history of ideas (Miel 1969; Aarsleff 1970). Evolutionary linguistics There are various sub-fields of linguistics which are in discord with generative grammar, especially over the notion that performance data should be used only as evidence for guiding the speculation and detailed usage and frequency patterns should be disregarded; the primacy of syntax (as advocated by Chomsky) is also disputed. One of these sub-fields is obviously corpus linguistics, which takes a decidedly empiricist stance and starts by assembling a large body of language data (a corpus) from which patterns of language use are inferred. Nevertheless, not all of these compete with generative grammar at the fundamental explanatory level of how language came about phylogenetically and how it is transmitted by ontogenetic acquisition. We have repeatedly encountered Chomsky's emphasis on how communication, actual interactions between speakers, are just an afterthought in the system of language: evolutionary biologist Salvador Luria was the most forceful advocate of the view that communicative needs would not have provided \"any great selective pressure to produce a system such as language,\" with its crucial relation to \"development of abstract or productive thinking.\" His fellow Nobel laureate François Jacob (1977) added later that \"the role of language as a communication system between individuals would have come about only secondarily, as many linguists believe,\" (Chomsky 2007, 23) Part of this vehemence dovetails with the single individual mutation hypothesis of the origins of language – it helps if the significance of communication is downplayed in an account where communication is initially impossible, simply because there is no other language-endowed being to communicate with. If communication were language's killer feature, then the selective pressure for the incriminated gene to propagate would not kick in. The other part can reasonably be attributed to Chomsky's intent to make a clean break from a prior popular theory on language acquisition, epitomized by B. F. Skinner's 1957 monograph Verbal Behavior , which offered a heavily empiricist, behaviorist account of language learning in terms of a stimulus-response cycle. Characteristically, Chomsky's strategy is to trivialize the function of the stimulus, casually implying both that it might not be needed at all, and if it is, then details of the role it plays are of little interest: it would not be at all surprising to find that normal language learning requires use of language in real-life situations, in some way. But this, if true [sic!], would not be sufficient to show that information regarding situational context (in particular, a pairing of signals with structural descriptions that is at least in part prior to assumptions about syntactic structure) plays any role in determining how language is acquired, once the mechanism is put to work and the task of language learning is undertaken by the child. (Chomsky 1965, 33) In retrospect, Skinner's account may be simplistic in many ways, but the basic notion that one has to pay attention to stimuli and responses in the course of particular linguistic interactions is sound. In particular, a theory of language built on this foundation successfully copes with all of the impasses we have explored above regarding Chomsky's approach. One such framework is that of evolutionary linguistics. Evolutionary linguistics views language as a complex adaptive system with emergent properties (Steels 2015, 8–9). A complex adaptive system is a system which is not centrally organized, coordinated or designed: its \"macroscopic\" characteristics are said to \"emerge\" as the result of localized interactions between individual entities (agents) with similar \"microscopic\" characteristics (be they physical, behavioral or motivational). The whole is more than the sum of its parts, and none of the agents can be properly said to have designed the system, nor can they deliberately change it in an arbitrary way; but all are continuously shaping it by taking part in the interactions that constitute its fabric. Examples of complex adaptive systems include the dynamics of insect societies (beehives, ant nests) or patterns of collective motion in large animal groups (flocks of birds or shoals of fish). These and more are discussed in much greater depth in the first chapter of Pierre-Yves Oudeyer's book Self-Organization in the Evolution of Speech . Adaptiveness is a property that these systems acquire by virtue of not being hardwired on the macro level: they are defined functionally instead of structurally. If the conditions in the environment change, the system will adapt to keep fulfilling its function, because the agents are forced to modify their behavior in order to achieve their individual goals. Of course, they may fail to do so, in which case the system breaks down and ceases to exist. If we revert to the metaphor from the title of the present essay, according to Chomsky, language is a cathedral erected by a single unwitting architect, the random genetic mutation that endowed us with the language faculty. Conversely, Luc Steels and fellow evolutionary linguists argue that the apparent macroscopic orderliness of language is the result of a myriad interactions of multiple individual agents, as suggested by the the bazaar image. One form that linguistic research can take under this paradigm is formulating and running computational models which simulate the behavior of agent populations and study the microscopic conditions, i.e. the cognitive and physical abilities, motivations etc. of each agent, necessary for a system like language to emerge within the population and stabilize. By direct inspiration from Wittgenstein's Philosophical Investigations , the interactions between agents are termed \"language games\" (Steels 2015, 167–8); depending on the topic being investigated, the agents can play different types of language games with different rules. It is openly acknowledged that such simulations represent only a limited approximation of a well-defined subspace of the actual uses of language. In the research to date, rules are generally definite and set for the entire experiment, but simulating language games with fuzzy rules remains a perfectly valid research topic within this framework, in the Wittgensteinian spirit of allowing rules to be made up and modified \"as we go along\" (Wittgenstein 2009, 44e). A relatively simple game that agents can play is the so-called Guessing Game (see Chap. 2 of Steels 2015 for more details). In this scenario, a population of agents, embodied in physical robots, tries to establish a shared lexicon and coupled ontology for a simple world consisting of geometrical shapes. Each game is an interaction of two agents picked at random, in the context of a scene consisting of said geometrical shapes. One agent (the speaker) takes the initiative, selects a topic from the scene and names it; the other (the hearer) tries to guess which object the first one had in mind and points to it; the speaker decodes the pointing gesture and the game succeeds if he interprets it as referencing his original topic. If so, he acknowledges the match; otherwise, he points at the intended topic as a repair strategy. At the outset, neither the lexicon nor the ontology are given, only a set of sensors and actuators (which allow the agents to interact with the environment by taking in streams of raw perceptual data or producing sound and pointing gestures) and very general cognitive principles. These include an associative memory and feedback mechanisms to propagate failures and successes in conceptualization and communication to all components of the system and act on them. 5 New distinctions along the perceptual dimensions are introduced in a random fashion, 6 but those that lead to a successful unambiguous selection of a topic and communicative success are strengthened over the course of many interactions, while useless ones are dampened by lateral inhibition and eventually pruned. At the same time, speakers create new words for concepts that are as of yet missing from their lexicon, and hearers may adopt them into theirs for their conceptualization of the topic the speaker points at in case of failure. A similar feedback mechanism then ensures that highly successful words are preferred and come to dominate within the speech community. It is important to realize that at no time do the individual agents share the same ontology or lexicon: newly introduced distinctions and words are random and unique for each agent, agents simply gradually learn which of these are useful in achieving communicative success, which means that they naturally settle on ontologies and lexicons that are close enough to those of others in the population. This barely scratches the surface of how all these notions must be orchestrated for a working computer implementation of this model, not to mention the even more elaborate agent-based language game modeling experiments that are already being conducted, investigating for instance the emergence of grammar (see Part III of Steels 2015 for an overview of recent scholarship). We see that even a seemingly simple task like establishing a shared conceptualization of reality and agreeing on names for these concepts is a complex endeavor which relies on a highly sophisticated (though also highly general) machinery. Another key observation is that while agent-based models can be fully virtual, grounding them in physical reality (cf. the use of robots with sensors and actuators) brings additional challenges that enable researchers to reach vital insights which would otherwise be impossible. In particular, grounding introduces fuzziness on the sensory input channels (by virtue of different points of view for the two robots and analog-to-digital conversion) which the agents must cope with, or else the mechanisms they were endowed with cannot be considered as constituting a plausible, sufficient model of the dynamics of human language. Unlike in generative grammar, anything that is transient, imperfect, is eminently included in the purview of linguistic inquiry. Failures are very much part of the dynamics that steer the evolution of language. How could it adapt to the speakers' changing requirements if it did not include appropriate repair strategies? Indeed, how could it be bootstrapped at all? The reward is a model that successfully simulates not only language emergence, but also transmission: if virgin agents are added into an existing population, they gradually acquire its language (see Fig. 1). Figure 1. Communicative success in a population of agents with a steady influx of virgin agents and outflux of old ones (overall population size remains the same). The game starts in phase 1 with 20 virgin agents; phases 2 and 4 show the behavior of the model at an agent renewal rate of 1/1000 games, whereas phase 3 corresponds to a heavier rate of 1/100. (Figure from Steels 2015, 121.) Crucially, the drive to communicate, to interact, is built into the agents, they just keep playing games as long as they can. But if it was not, the simulation would have to be more complex and somehow elicit this drive by introducing appropriate ecological constraints, e.g. by requiring co-operation as a survival strategy (Steels 2015, 106). Otherwise, the agents would have no motivation to strive for communicative success in their mutual encounters, they would fail to reach intersubjective alignment of their conceptual spaces and lexicons, and language would not emerge. In other words, far from being an afterthought, successful communication with a partner, grounded in an external context, turns out to be a fundamental requirement to establish the kind of dynamics which allow languages to appear. Paradoxically, since it concerns itself with simulations and computational models, this branch of evolutionary linguistics is, like generative grammar, also highly speculative. However, unlike generative grammar, it is a kind of speculation which considers guidance by empirical observations a necessity, not a nuisance. Furthermore, simulations are meant to be tested: if an agent-based model of language emergence fails to converge on the result stipulated for that particular experiment, the model is plain wrong and the dynamics it is trying to put into place (cognitive strategies, feedback propagation etc.) need to be revised. There is thus a clear-cut criterion for validity. Lastly, even if a simulation works, an accompanying debate as to whether the mechanisms involved are actually plausible approximations of reality is considered an integral part of hypothesis evaluation, with evidence from strongly empirically grounded disciplines like biology and neurophysiology a vital element in the process. Conclusion Taking a cue from Wittgenstein's Philosophical Investigations , this essay should not be construed as an attempt to replace one doctrine with another, but to advocate a \"change of attitude\" (cf. McGinn 2013, 33) which allows asking more meaningful questions about language. This being said, on the evidence presented above, it is hard not to conclude that Noam Chomsky is fundamentally mistaken about the corrective that is necessary for language learning to take place. According to Chomsky, the criterion for evaluating linguistic rules lies within a dedicated language organ we are genetically endowed with; the innate structures themselves embody the metric by which conjectures pertaining to linguistic rules will be judged. By contrast, in the evolutionary linguistics perspective, genetics provide innate structures which are capable of random growth, but the feedback (reinforcement and pruning) which results in steering this growth in a particular direction comes from interactions with the environment. This theory presupposes much less specificity in the hardware infrastructure which makes this possible and so should be preferred both on grounds of simplicity and flexibility of the model, not to mention that it is biologically plausible and has been empirically verified to work. In the context of science, Chomsky's rhetorical strategy in and of itself is dishonest: he preaches formal rigor while practicing sleight of hand, and casually retreats to increasingly abstract ground on reaching an impasse. He thus carves out a region in discursive space which has no corresponding equivalent in a logically consistent conceptual space, without which a piece of discourse can hardly constitute a scientific theory. In other words, much like his famous example sentence \"Colorless green ideas sleep furiously\", his discourse is grammatical but largely nonsensical under the requirements on a system of thought which aspires to mirror reality in a coherent fashion. Requirements on scientific discourse notwithstanding, we as linguists should keep in mind that language in general is much more than a system for encoding logical propositions. Even Wittgenstein had to resign himself to the fact – or perhaps knew all along – that the Tractatus , which he framed as the ultimate solution to all metaphysical controversies, could only fan the flames of philosophical debate. It was after all addressed to a diverse community of people bound perhaps exclusively by their penchant for elaborate language games. References Aarsleff, Hans. 1970. \"The History of Linguistics and Professor Chomsky.\" Language 46 (3): 570–85. Chomsky, Noam. 1956. \"Three Models for the Description of Language.\" ———. 1965. Aspects of the Theory of Syntax . Cambridge, MA: The M.I.T. Press. ———. 1986. Knowledge of Language: Its Nature, Origin, and Use . Convergence. New York, Westport, London: Praeger. ———. 1995. The Minimalist Program . Cambridge, MA: The MIT Press. ———. 2002. Syntactic Structures . 2nd ed. Berlin, New York: Mouton de Gruyter. ———. 2007. \"Of Minds and Language.\" Biolinguistics , no. 1: 9–27. ———. 2009. Cartesian Linguistics: A Chapter in the History of Rationalist Thought . 3rd ed. Cambridge: Cambridge University Press. McGinn, Marie. 2006. Elucidating the Tractatus: Wittgenstein's Early Philosophy of Logic and Language . Oxford: Oxford University Press. ———. 2013. The Routledge Guidebook to Wittgenstein's Philosophical Investigations . The Routledge Guides to Great Books. Routledge. Miel, Jan. 1969. \"Pascal, Port-Royal, and Cartesian Linguistics.\" Journal of the History of Ideas 30 (2): 261–71. Oudeyer, Pierre-Yves. 2006. Self-Organization in the Evolution of Speech . Translated by James R. Hurford. Oxford, New York: OUP. Pullum, Geoffrey K. 2011. \"On the Mathematical Foundations of Syntactic Structures .\" Journal of Logic, Language and Information 20: 277–96. Raymond, Eric S. 1999. The Cathedral & the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary . O'Reilly Media. Sampson, Geoffrey. 2005. The \"Language Instinct\" Debate . 3rd ed. London, New York: Continuum. ———. 2015. \"Rigid Strings and Flaky Snowflakes.\" Language and Cognition 10: 1–17. Skinner, B. F. 1957. Verbal Behavior . The Century Psychology Series. New York: Appleton – Century – Crofts. Steels, Luc. 2015. The Talking Heads Experiment: Origins of Words and Meanings . Computational Models of Language Evolution 1. Berlin: Language Science Press. Wittgenstein, Ludwig. 2001. Tractatus Logico-Philosophicus . Routledge Classics. London, New York: Routledge. ———. 2009. Philosophical Investigations . Chichester, United Kingdom: Blackwell Publishing. Let us pretend for a moment that language games like \"poetry\" or the surrealist pastime of cadavre exquis do not exist; in these, the quoted sentence could appear as perfectly valid and meaningful, though perhaps not in the sense that Chomsky intended. \"Meaningful\" in a late-Wittgensteinian perspective could be paraphrased as \"accepted by at least one involved party as a valid turn within the context of a particular language game\". ↩ Arguing that this does not matter because we should be concerned with the ideal speaker-hearer's competence just takes us further down the impasse, because now we have to determine how to delimit the purported \"ideal\". ↩ This is the beauty of building empirically unmotivated, purely speculative theories: at any moment, one can freely accommodate a new element into the existing framework, substituting novelty and amalgamation for critical evaluation. ↩ Cf. also Sampson's riposte : \"If complex properties of some aspect of human behaviour have to be as they are as a matter of conceptual necessity, then there is no reason to postulate complex genetically inherited cognitive machinery determining those behaviour patterns\" (Sampson 2015, 9). ↩ This interconnected architecture stands in stark contrast to Chomsky's deliberately isolationist approach: \"the relation between semantics and syntax […] can only be studied after the syntactic structure has been determined on independent grounds\" (Chomsky 2002, 17). ↩ Wittgenstein only hints at the problem of conceptualization, but he is prescient in realizing it is not a given: \"The primary elements [of the objects which constitute the world in this particular language game] are the coloured squares. ‘But are these simple?' – I wouldn't know what I could more naturally call a ‘simple' in this language-game. But under other circumstances, I'd call a monochrome square, consisting perhaps of two rectangles or of the elements colour and shape, ‘composite'\" (Wittgenstein 2009, 27e). ↩","tags":"ling","url":"cathedral-and-bazaar.html"},{"title":"Configuring Emacs Daemon on Mac OS X","text":"I know I promised this article a loooong time ago (June 2014, when I first got a Mac, to judge by the previous timestamp in the header of this file), but since the historically attested readership of this blog is 2 + a bunch of my facebook friends who I nagged to read my attempt at explaining character encodings to non-technical people , I don't suppose it's as if a legion of fans have been restlessly looking forward to this one ;) Nevertheless, the distinct advantage is that my OS X Emacs setup has had the opportunity to grow more mature and also much simpler in the meantime, which means that if a third reader accidentally stumbles over this note (exploding my ratings...), they might actually find something genuinely useful here. tl;dr This article presents a way to start Emacs Daemon (a persistent Emacs session) from the GUI and subsequently connect to it (creating frames on demand) using an Automator script . The benefit is that you incur startup time lag only once (when you start the daemon) while still being able to close all frames when you're not using Emacs, keeping a clean workspace . This is especially useful if your Emacs is heavily customized and loading it takes a while . Another benefit is that whenever you open a frame connected to an Emacs daemon, all your previously open buffers are still there as you left them (as opposed to opening a fresh instance of Emacs). Skim over the code blocks to get the important gist without the verbose sauce. Tested on OS X 10.11 El Capitan, with Homebrew Emacs and Spacemacs config. Why Emacs Daemon, why this post Installing Emacs on a Mac in and of itself is not that much of a problem -- there are several options, ranging from Homebrew and Macports to Emacs for Mac OS X , Emacs Mac Port and Aquamacs . The last two in this list have some OS X specific tweaks (smooth scrolling, tabs, adapted keyboard shortcuts), which makes them perhaps more appealing out of the box but also less extensible, as some of the information out there about generic Emacs might not apply to them as straightforwardly or indeed at all. With that in mind, if you want to tinker with your Emacs config, it's a good idea to stick with Homebrew's fairly conservative version of Emacs: $ brew update $ brew install emacs --with-cocoa # this step gets you a standard OS X launcher icon $ brew linkapps emacs But now that you've got Emacs, and especially if you're transferring some heavy customization over from say Linux, you might be unhappy that each time you start it from cold, it takes a while, typically a few seconds. That's what emacs --daemon and emacsclient are for: Emacs is run as a daemon in the backround and you connect to it with client frames that spawn almost instantly . This also means that you can close all existing frames to keep your workspace clean if you won't be using Emacs for a while (hard to imagine, right, since you can even read xkcd from inside Emacs ) and then whip up a frame at the speed of a thought when need arises. Now this is all easy to achieve when using the terminal , but since you probably bought that Mac in great part for its shiny pretty elegant ergonomic GUI, you might want Emacs to use GUI frames instead of terminal ones and connect to the Emacs daemon (or start it if it's not running) by just clicking on an app icon in the launcher or finding it from Spotlight. That's where Automator comes in. An Automator script Automator is a built-in OS X app for creating custom automated user workflows for just about any installed app you might have or even OS functionality. Among other things, this means that it allows you to wrap the daemon auto-start functionality available from the terminal (as described in the previous paragraph) into an app launchable from the GUI. Let's get down to business: Launch Automator and create a new document. Select Application as its type. Search the Actions palette on the left for the Run Shell Script action and add it to your Automator document. In the Run Shell Script building block, change the following: set Shell to the shell you're using and whose init files have thus the PATH correctly set to the emacs and emacsclient executables (if you're using Homebrew, it probably told you how to properly set up your PATH as a post-install step) set Pass input to \"as arguments\" (if you then set this Automator app as the default for opening a given type of file , you'll be able to use emacsclient to open files by double-clicking on them in Finder) Finally, paste in the following code snippet and save the app e.g. as EmacsClient.app , preferably in your Applications folder so that it is easily accessible from the launcher. emacsclient --no-wait -c -a emacs \" $@ \" >/dev/null 2 > & 1 & EDIT : An earlier version of this article had nohup prepended to the command above; as pointed out in the comments by MaTres (thanks!), this is unnecessary . At the end of the day, your Automator EmacsClient.app should look something like this: The core of the command that you might want to tweak based on your particular Emacs setup is emacsclient --no-wait -c -a emacs ; mine is optimized to work with mostly stock Spacemacs config (see below). If it doesn't work, you might also want to try a simple emacsclient -c -a \"\" and variations; a good debugging technique is to try these out in the terminal: as soon as you get the line working there, it'll start working in the Automator task as well. \"$@\" is just the list of files (if any) passed to Emacs to open (the aforementioned double-click in Finder use case). The rest is some black magic to ensure that the shell which spawns the Emacs process (because this Automator app is after all, at heart, only a shell script) totally and utterly disowns it, so that the shell script is allowed to return and the Automator task completes as soon as Emacs has started (or the client has spawned a new frame). Otherwise, you'd end up with an irritating spinning cog wheel in your notification area which would stay there until you completely quit Emacs. Which is probably not what you want, since you're undergoing all this hassle in the first place to get a zen, distraction-free Emacs experience. The details of the various incantations are discussed in this Apple forum thread , but let's have a whirlwind tour for the moderately interested (my knowledge of Unix processes is far from perfect, so feel free to correct me on these points!): >/dev/null redirects standard output to oblivion and 2>&1 redirects standard error to standard output (i.e. also to oblivion), which persuades Automator that you're really not expecting to hear from the process via these standard streams ever again, so there's no point in keeping the shell script running. These can be shortened to &>/dev/null . the final & runs the command in the background, which ensures control of the shell is returned to the user as soon as the process is spawned; since there are no additional commands in the shell script and all remaining ties have been severed, Automator finally agrees that the task has probably done all it was expected to do and exits it. Wrapping up Whew! That's it. It's really not that complicated, it's just that my prose is verbose, so it makes it look like there's lots and lots to do. Trust me, there isn't. My first go at solving this usability problem -- the one I originally wanted to post way back in 2014 -- was a lengthy, godawful Applescript prone to subtle breakage. This is much better. And the ability to just use a single GUI app for transparently launching and connecting to the Emacs daemon is pure bliss. While you're at it, for an even better Emacs experience, go fetch the excellent Spacemacs Emacs config distribution , which pulls this venerable piece of software screaming into the 21st century. The best editor is neither Vim nor Emacs, its Vim + Emacs! The addictive icing of Vim modal editing on the outside, a creamy Elisp core -- what more could you want from life? ;) Oh and if, like me, you love Spacemacs' snappy icon with the Evil spaceship over planet Emacs -- or if, like me, you have OCD -- you'll definitely want to switch your Emacs logo to the Spacemacs one !","tags":"macOS","url":"emacs-daemon-osx.html"},{"title":"How computers handle text: a gentle but thorough introduction to Unicode","text":"Or, the absolute minimum every software developer linguist absolutely, positively must know about Unicode and character sets (no excuses!) Note : This text was written as part of a larger programming tutorial in Python, and the code samples are taken from an interactive session using the Jupyter notebook . As a consequence, there are digressions here and there about playing with text data in Python. These might seem: useless if what you came for is just the part about text encoding; long-winded if you already know some Python; or confusing if, on the contrary, you're not familiar with programming at all, much less with Python. If any of these is your case, my advice is: ignore the code, focus on the comments around it, they're more than enough to follow the thread of the explanation. Though if you've got a little more time, why not try some of these out in an interactive Python session ? ;) And now, without further ado... Much like any other piece of data inside a digital computer, text is represented as a series of binary digits (bits), i.e. 0's and 1's. A mapping between sequences of bits and characters is called an encoding. How many different characters your encoding can handle depends on how many bits you allow per character: with 1 bit you can have 2&#94;1 = 2 characters (one is represented by 0, the other by 1) with 2 bits you can have 2&#94;2 = 4 characters(represented by 00, 01, 10 and 11) etc. The oldest encoding still in widespread use (it's what makes the Internet and the web tick) is ASCII , which is a 7-bit encoding: In [1]: 2 ** 7 Out[1]: 128 This means it can represent 128 different characters , which comfortably fits the basic Latin alphabet (both lowercase and uppercase), Arabic numerals, punctuation and some \"control characters\" which were primarily useful on the old teletype terminals for which ASCII was designed. For instance, the letter \"A\" corresponds to the number 65 ( 1000001 in binary, see below). \"ASCII\" stands for \" American Standard Code for Information Interchange\" -- which explains why there are no accented characters, for instance. Nowadays, ASCII is represented using 8 bits (== 1 byte), because that's the unit of computer memory which has become ubiquitous (in terms of both hardware and software assumptions), but still uses only 7 bits' worth of information. In [2]: 2 ** 8 Out[2]: 256 In [3]: # how to find out the binary representation of a decimal number? \" {:b} \" . format ( 65 ) Out[3]: '1000001' In [4]: # Digression/explanation: the format() method # # the format() string method inserts its arguments into the string # wherever there is a \"{}\" \" {} {} {} \" . format ( \"foo\" , \"bar\" , \"baz\" ) Out[4]: 'foo bar baz' In [5]: # you can also specify a different order by using (zero-based) # positional indices -- or even repeating them \" {1} {0} {1} \" . format ( \"foo\" , \"bar\" ) Out[5]: 'bar foo bar' In [6]: # for long strings with many insertions, where you might mess up the # order of arguments, keyword arguments are also available \" {foo_arg} {bar_arg} \" . format ( bar_arg = \"bar\" , foo_arg = \"foo\" ) Out[6]: 'foo bar' In [7]: # and you can also request various formatting adjustments or conversions # to be made by specifying them after a \":\" -- e.g. \"b\" prints a given # number in its binary representation \" {:b} \" . format ( 45 ) Out[7]: '101101' In [8]: # or simply bin ( 45 ) # but that has an ugly \"0b\" in front, and we would've missed out on # format() if we'd used that directly! Out[8]: '0b101101' What happens in the range [128; 256) is not covered by the ASCII standard. In the 1990s, many encodings were standardized which used this range for their own purposes, usually representing additional accented characters used in a particular region. E.g. Czech (and Slovak, Polish...) alphabets can be represented using the ISO latin-2 encoding, or Microsoft's cp-1250 . Encodings which stick with the same character mappings as ASCII in the range [0; 128) and represent them physically in the same way (as 1 byte) , while potentially adding more character mappings beyond that, are called ASCII -compatible . ASCII compatibility is a good thing™, because when you start reading a character stream in a computer, there's no way to know in advance what encoding it is in (unless it's a file you've encoded yourself). So in practice, a heuristic has been established to start reading the stream assuming it is ASCII by default, and switch to a different encoding if evidence becomes available that motivates it. For instance, HTML files should all start something like this: <!DOCTYPE html> < html > < head > < meta charset = \"utf-8\" /> ... This way, whenever a program wants to read a file like this, it can start off with ASCII , waiting to see if it reaches the charset (i.e. encoding) attribute, and once it does, it can switch from ASCII to that encoding ( UTF-8 here) and restart reading the file, now fairly sure that it's using the correct encoding. This trick works only if we can assume that whatever encoding the rest of the file is in, the first few lines can be considered as ASCII for all practical intents and purposes. Without the charset attribute, the only way to know if the encoding is right would be for you to look at the rendered text and see if it makes sense; if it did not, you'd have to resort to trial and error, manually switching the encodings and looking for the one in which the numbers behind the characters stop coming out as gibberish and are actually translated into intelligible text. In [9]: # Let's take a look at printable characters in the latin-2 character # set. Each mapping is called a \"codepoint\": it is a correspondence # between an integer and a character. import codecs latin2 = [] for codepoint in range ( 256 ): byte = bytes ([ codepoint ]) character = codecs . decode ( byte , encoding = \"latin2\" ) if character . isprintable (): latin2 . append (( codepoint , character )) latin2 Out[9]: [(32, ' '), (33, '!'), (34, '\"'), (35, '#'), (36, '$'), (37, '%'), (38, '&'), (39, \"'\"), (40, '('), (41, ')'), (42, '*'), (43, '+'), (44, ','), (45, '-'), (46, '.'), (47, '/'), (48, '0'), (49, '1'), (50, '2'), (51, '3'), (52, '4'), (53, '5'), (54, '6'), (55, '7'), (56, '8'), (57, '9'), (58, ':'), (59, ';'), (60, '<'), (61, '='), (62, '>'), (63, '?'), (64, '@'), (65, 'A'), (66, 'B'), (67, 'C'), (68, 'D'), (69, 'E'), (70, 'F'), (71, 'G'), (72, 'H'), (73, 'I'), (74, 'J'), (75, 'K'), (76, 'L'), (77, 'M'), (78, 'N'), (79, 'O'), (80, 'P'), (81, 'Q'), (82, 'R'), (83, 'S'), (84, 'T'), (85, 'U'), (86, 'V'), (87, 'W'), (88, 'X'), (89, 'Y'), (90, 'Z'), (91, '['), (92, '\\\\'), (93, ']'), (94, '&#94;'), (95, '_'), (96, '`'), (97, 'a'), (98, 'b'), (99, 'c'), (100, 'd'), (101, 'e'), (102, 'f'), (103, 'g'), (104, 'h'), (105, 'i'), (106, 'j'), (107, 'k'), (108, 'l'), (109, 'm'), (110, 'n'), (111, 'o'), (112, 'p'), (113, 'q'), (114, 'r'), (115, 's'), (116, 't'), (117, 'u'), (118, 'v'), (119, 'w'), (120, 'x'), (121, 'y'), (122, 'z'), (123, '{'), (124, '|'), (125, '}'), (126, '~'), (161, 'Ą'), (162, '˘'), (163, 'Ł'), (164, '¤'), (165, 'Ľ'), (166, 'Ś'), (167, '§'), (168, '¨'), (169, 'Š'), (170, 'Ş'), (171, 'Ť'), (172, 'Ź'), (174, 'Ž'), (175, 'Ż'), (176, '°'), (177, 'ą'), (178, '˛'), (179, 'ł'), (180, '´'), (181, 'ľ'), (182, 'ś'), (183, 'ˇ'), (184, '¸'), (185, 'š'), (186, 'ş'), (187, 'ť'), (188, 'ź'), (189, '˝'), (190, 'ž'), (191, 'ż'), (192, 'Ŕ'), (193, 'Á'), (194, 'Â'), (195, 'Ă'), (196, 'Ä'), (197, 'Ĺ'), (198, 'Ć'), (199, 'Ç'), (200, 'Č'), (201, 'É'), (202, 'Ę'), (203, 'Ë'), (204, 'Ě'), (205, 'Í'), (206, 'Î'), (207, 'Ď'), (208, 'Đ'), (209, 'Ń'), (210, 'Ň'), (211, 'Ó'), (212, 'Ô'), (213, 'Ő'), (214, 'Ö'), (215, '×'), (216, 'Ř'), (217, 'Ů'), (218, 'Ú'), (219, 'Ű'), (220, 'Ü'), (221, 'Ý'), (222, 'Ţ'), (223, 'ß'), (224, 'ŕ'), (225, 'á'), (226, 'â'), (227, 'ă'), (228, 'ä'), (229, 'ĺ'), (230, 'ć'), (231, 'ç'), (232, 'č'), (233, 'é'), (234, 'ę'), (235, 'ë'), (236, 'ě'), (237, 'í'), (238, 'î'), (239, 'ď'), (240, 'đ'), (241, 'ń'), (242, 'ň'), (243, 'ó'), (244, 'ô'), (245, 'ő'), (246, 'ö'), (247, '÷'), (248, 'ř'), (249, 'ů'), (250, 'ú'), (251, 'ű'), (252, 'ü'), (253, 'ý'), (254, 'ţ'), (255, '˙')] Using the 8th bit (and thus the codepoint range [128; 256)) solves the problem of handling languages with character sets different than that of American English, but introduces a lot of complexity -- whenever you come across a text file with an unknown encoding, it might be in one of literally dozens of encodings. Additional drawbacks include: how to handle multilingual text with characters from many different alphabets, which are not part of the same 8-bit encoding? how to handle writing systems which have way more than 256 \"characters\", e.g. Chinese, Japanese and Korean (CJK) ideograms? For these purposes, a standard encoding known as Unicode was developed which strives for universal coverage of all possible character sets. Unicode is much bigger than the encodings we've seen so far -- its most frequently used subset, the Basic Multilingual Plane , has 2&#94;16 codepoints, but overall the number of codepoints is past 1M and there's room to accommodate many more. In [10]: 2 ** 16 Out[10]: 65536 Now, the most straightforward representation for 2&#94;16 codepoints is what? Well, it's simply using 16 bits per character, i.e. 2 bytes. That encoding exists, it's called UTF-16 , but consider the drawbacks: we've lost ASCII compatibility by the simple fact of using 2 bytes per character instead of 1 (encoding \"a\" as 01100001 or 01100001|00000000 , with the | indicating an imaginary boundary between bytes, is not the same thing) encoding a string in a character set which uses a \"reasonable\" number of characters (like any European language) now takes twice as much space without any added benefit (which is probably not a good idea, given the general dominance of English -- one of those \"reasonable character set size\" languages -- in electronic communication) Looks like we'll have to think outside the box. The box in question here is called fixed-width encodings -- all of the encoding schemes we've encountered so far were fixed-width, meaning that each character was represented by either 7, 8 or 16 bits. In other word, you could jump around the string in multiples of 7, 8 or 16 and always land at the beginning of a character. (Not exactly true for UTF-16 , because it is something more than just a \"16-bit ASCII \": it has ways of handling characters beyond 2&#94;16 using so-called surrogate sequences -- but you get the gist.) \"UTF\" stands for \"Unicode Transformation Format\". The smart idea that some bright people have come up with was to use a variable-width encoding . The most ubiquitous one currently is UTF-8 , which we've already met in the HTML example above. UTF-8 is ASCII -compatible, i.e. the 1's and 0's used to encode text containing only ASCII characters are the same regardless of whether you use ASCII or UTF-8 : it's a sequence of 8-bit bytes. But UTF-8 can also handle many more additional characters, as defined by the Unicode standard, by using progressively longer and longer sequences of bits. In [11]: def print_as_binary_utf8 ( string ): \"\"\"Prints binary representation of string as encoded by UTF-8. \"\"\" binary_bytes = [] # encode the string as UTF-8 and iterate over the bytes for byte in string . encode ( \"utf-8\" ): # generate a string of general format \"0b101...\", which # is the binary representation of the byte binary = bin ( byte ) # remove the leading \"0b\" binary = binary [ 2 :] # pad the representation with leading zeros to the size of # a full byte (= a sequence of 8 1's and 0's) if necessary binary_byte = binary . rjust ( 8 , \"0\" ) binary_bytes . append ( binary_byte ) print ( \"' {} ' encoded in UTF-8 is: {} \" . format ( string , binary_bytes )) print_as_binary_utf8 ( \"A\" ) # the representations... print_as_binary_utf8 ( \"č\" ) # ... keep... print_as_binary_utf8 ( \"字\" ) # ... getting longer. 'A' encoded in UTF-8 is: ['01000001'] 'č' encoded in UTF-8 is: ['11000100', '10001101'] '字' encoded in UTF-8 is: ['11100101', '10101101', '10010111'] How does it achieve that? The obvious problem here is that with a fixed-width encoding, you just chop up the string at regular intervals (7, 8, 16 bits) and you know that each interval represents one character. So how do you know where to chop up a variable width-encoded string, if each character can take up a different number of bits? Essentially, the trick is to use some of the bits in the representation of a codepoint to store information not about which character it is (whether it's an \"A\" or a \"字\"), but how many bits it occupies . In other words, if you want to skip ahead 10 characters in a string encoded with a variable width-encoding, you can't just skip 10 * 7 or 8 or 16 bits; you have to read all the intervening characters to figure out how much space they take up. Take the following example: In [12]: for char in \"Básník 李白\" : print_as_binary_utf8 ( char ) 'B' encoded in UTF-8 is: ['01000010'] 'á' encoded in UTF-8 is: ['11000011', '10100001'] 's' encoded in UTF-8 is: ['01110011'] 'n' encoded in UTF-8 is: ['01101110'] 'í' encoded in UTF-8 is: ['11000011', '10101101'] 'k' encoded in UTF-8 is: ['01101011'] ' ' encoded in UTF-8 is: ['00100000'] '李' encoded in UTF-8 is: ['11100110', '10011101', '10001110'] '白' encoded in UTF-8 is: ['11100111', '10011001', '10111101'] Notice the initial bits in each byte of a character follow a pattern depending on how many bytes in total that character has: if it's a 1-byte character, that byte starts with 0 if it's a 2-byte character, the first byte starts with 11 and the following one with 10 if it's a 3-byte character, the first byte starts with 111 and the following ones with 10 This makes it possible to find out which bytes belong to which characters, and also to spot invalid strings, as the leading byte in a multi-byte sequence always \"announces\" how many continuation bytes (= starting with 10) should follow. So much for a quick introduction to UTF-8 (= the encoding), but there's much more to Unicode (= the character set). While UTF-8 defines only how integer numbers corresponding to codepoints are to be represented as 1's and 0's in a computer's memory, Unicode specifies how those numbers are to be interpreted as characters, what their properties and mutual relationships are, what conversions (i.e. mappings between (sequences of) codepoints) they can undergo, etc. Consider for instance the various ways diacritics are handled: \"č\" can be represented either as a single codepoint ( LATIN SMALL LETTER C WITH CARON -- all Unicode codepoints have cute names like this) or a sequence of two codepoints, the character \"c\" and a combining diacritic mark ( COMBINING CARON ). You can search for the codepoints corresponding to Unicode characters e.g. here and play with them in Python using the chr(0xXXXX) built-in function or with the special string escape sequence \\uXXXX (where XXXX is the hexadecimal representation of the codepoint) -- both are ways to get the character corresponding to the given codepoint: In [13]: # \"č\" as LATIN SMALL LETTER C WITH CARON, codepoint 010d print ( chr ( 0x010d )) print ( \" \\u010d \" ) č č In [14]: # \"č\" as a sequence of LATIN SMALL LETTER C, codepoint 0063, and # COMBINING CARON, codepoint 030c print ( chr ( 0x0063 ) + chr ( 0x030c )) print ( \" \\u0063\\u030c \" ) č č Hexadecimal is just a more convenient way of representing sequences of bits, where each of the X 's can be a number between 0 and 15 (10--15 are represented by the letters A--F). Each hexadecimal number can thus represent 16 different values, and therefore it can stand in for a sequence of 4 bits (2&#94;4 == 16). Without worrying too much about the details right now, our old friend ASCII uppercase \"A\" can be thought of equivalently either as decimal 65, binary 0b1000001 , or hexadecimal 0x41 (the \"0b\" / \"0x\" prefixes are there just to say \"this is a binary / hexadecimal number\"). Binary and hexadecimal numbers are often written padded with leading zeros to some number of bytes, but these have no effect on the value, much like decimal 42 and 00000042 are effectively the same numbers. In [15]: # use hex() to find out the hexadecimal representation of a decimal # integer... hex ( 99 ) Out[15]: '0x63' In [16]: # ... and int() to go back... int ( 0x63 ) Out[16]: 99 In [17]: # ... or just evaluate the hexadecimal number 0x63 Out[17]: 99 In [18]: # of course, chr() also works with decimal numbers chr ( 269 ) Out[18]: 'č' This means you have to be careful when working with languages that use accents, because for a computer, the two possible representations are of course different strings , even though for you, they're conceptually the same: In [19]: s1 = \" \\u010d \" s2 = \" \\u0063\\u030c \" # s1 and s2 look the same to the naked eye... print ( s1 , s2 ) č č In [20]: # ... but in the eternal realm of Plato's Ideas, they're not s1 == s2 Out[20]: False Watch out, they even have different lengths ! This might come to bite you if you're trying to compute the length of a word in letters. In [21]: print ( \"s1 is\" , len ( s1 ), \"character(s) long.\" ) print ( \"s2 is\" , len ( s2 ), \"character(s) long.\" ) s1 is 1 character(s) long. s2 is 2 character(s) long. For this reason, even though we've been informally calling these Unicode entities \"characters\", it is more accurate and less confusing to use the technical term \"codepoints\". Generally, most text out there will use the first, single-codepoint approach whenever possible, and pre-packaged linguistic corpora will try to be consistent about this (unless they come from the web, which always warrants being suspicious and defensive about your material). If you're worried about inconsistencies in your data, you can perform a normalization : In [22]: from unicodedata import normalize # NFC stands for Normal Form C; this normalization applies a canonical # decomposition (into a multi-codepoint representation) followed by a # canonical composition (into a single-codepoint representation) s1 = normalize ( \"NFC\" , s1 ) s2 = normalize ( \"NFC\" , s2 ) s1 == s2 Out[22]: True Let's wrap things up by saying that Python itself uses Unicode internally, but the encoding it defaults to when opening an external file depends on the locale of the system (broadly speaking, the set of region, language and character-encoding related settings of the operating system). On most modern Linux and macOS systems, this will probably be a UTF-8 locale and Python will therefore assume UTF-8 as the encoding by default. Unfortunately, Windows is different. To be on the safe side, whenever opening files in Python, you can specify the encoding explicitly: In [23]: with open ( \"unicode.ipynb\" , encoding = \"utf-8\" ) as file : pass In [24]: # a good idea when dealing with Unicode text from an unknown and # unreliable source is to look at the set of codepoints contained # in it and eliminate or replace those that shouldn't be there import unicodedata def inspect_codepoints ( text ): charset = set ( text ) for char in sorted ( charset ): info = r \" {} (\\u {:04x} ): {} (category: {} )\" . format ( char , ord ( char ), unicodedata . name ( char ), unicodedata . category ( char )) print ( info ) # depending on your font configuration, it may be very hard to spot # the two intruders in the sentence below that look like regular # letters but really are specialized variants; you might want # to replace them before doing further text processing... inspect_codepoints ( \"Intruders here, good 𝗍hinɡ I checked.\" ) (\\u0020): SPACE (category: Zs) , (\\u002c): COMMA (category: Po) . (\\u002e): FULL STOP (category: Po) I (\\u0049): LATIN CAPITAL LETTER I (category: Lu) c (\\u0063): LATIN SMALL LETTER C (category: Ll) d (\\u0064): LATIN SMALL LETTER D (category: Ll) e (\\u0065): LATIN SMALL LETTER E (category: Ll) g (\\u0067): LATIN SMALL LETTER G (category: Ll) h (\\u0068): LATIN SMALL LETTER H (category: Ll) i (\\u0069): LATIN SMALL LETTER I (category: Ll) k (\\u006b): LATIN SMALL LETTER K (category: Ll) n (\\u006e): LATIN SMALL LETTER N (category: Ll) o (\\u006f): LATIN SMALL LETTER O (category: Ll) r (\\u0072): LATIN SMALL LETTER R (category: Ll) s (\\u0073): LATIN SMALL LETTER S (category: Ll) t (\\u0074): LATIN SMALL LETTER T (category: Ll) u (\\u0075): LATIN SMALL LETTER U (category: Ll) ɡ (\\u0261): LATIN SMALL LETTER SCRIPT G (category: Ll) 𝗍 (\\u1d5cd): MATHEMATICAL SANS-SERIF SMALL T (category: Ll) In [25]: # ... because of course, for a computer, the word \"thing\" written with # two different variants of \"g\" is really just two different words, which # is probably not what you want \"thing\" == \"thinɡ\" Out[25]: False In any case, here's what happens when processing text with Python (\"Unicode\" in the central box stands for Python's internal representation of Unicode, which is not UTF-8 nor UTF-16 ): (Image shamelessly hotlinked from / courtesy of the NLTK Book . Go check it out, it's an awesome intro to Python programming for linguists!) A terminological postscript: we've been using some terms a bit informally and for the most part it's okay, but it's good to get the distinctions straight in one's head at least once. So, a character set is a mapping between codepoints (integers) and characters . We may for instance say that in our character set, the integer 99 corresponds to the character \"c\". On the other hand, an encoding is a mapping between a codepoint (an integer) and a physical sequence of 1's and 0's that represent it in memory . With fixed-width encodings, this mapping is generally straightforward -- the 1's and 0's directly represent the given integer, only in binary and padded with zeros to fit the desired width. With variable-width encodings, as the necessity creeps in to include the information about how many bits are spanned by the current character, this straightforward correspondence breaks down. A comparison might be helpful here: as encodings, UTF-8 and UTF-16 both use the same character set -- the same integers corresponding to the same characters. But since they're different encodings , when the time comes to turn these integers into sequences of bits to store in a computer's memory, each of them generates a different one. For more on Unicode, a great read already hinted at above is Joel Spolsky's The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) . Another great piece of material is the Characters, Symbols and the Unicode Miracle video by the Computerphile channel on YouTube. To make the discussion digestible for newcomers, I sometimes slightly distorted facts about how things are \"really really\" done. And some inaccuracies may be genuine mistakes. In any case, please let me know in the comments! I'm grateful for feedback and looking to improve this material; I'll fix the mistakes and consider ditching some of the simplifications if they prove untenable :)","tags":"ling","url":"unicode.html"},{"title":"Úprava rozhraní konkordanceru KonText -- vylepšená verze","text":"Před nějakou dobou jsem zde vyvěsil skript , jehož pomocí lze lehce \"přeskládat\" a upravit rozhraní korpusového konkordanceru KonText : menu je umístěné po straně místo nahoře a permanentně rozbalené nad vyhledanou konkordancí je umístěn rychlý hledací box, v němž lze předchozí dotaz pohodlně upravit Víc o motivaci těchto úprav se dočtete v původním článku . Stále platí, že ČNK nemá v plánu tyto změny začlenit přímo do oficiální verze KonTextu, zejména proto, že rychlý hledací box sice v jistých situacích může být užitečný, nicméně oproti standardnímu formuláři Nový dotaz výrazně omezuje možnosti pro zadání dotazu. Vylepšená verze, která je k dispozici níže, odstraňuje některé předchozí nedostatky skriptu: rychlý hledací box nad konkordancí je větší, ukazuje vždy CQL podobu posledního zadaného dotazu 1 , a především zůstává zobrazený i během listování konkordancí (tj. není k dispozici jen na její první stránce). Dotaz lze nyní navíc pro větší přehlednost rozdělit do více řádků, takže opětovné vyhledávání se nově spouští stiskem kombinace kláves Ctrl+Enter (místo jen Enteru). Výsledné upravené rozhraní KonText vypadá stále podobně: Postup instalace skriptu Nová verze skriptu je k dispozici zde: Kroky k jeho zprovoznění zůstávají stejné: Nainstalovat si do svého prohlížeče plugin Tampermonkey , pokud používáte Chrome, nebo Greasemonkey , pokud používáte Firefox. (Pokud používáte Internet Explorer, budete muset dočasně přesedlat na Chrome nebo Firefox.) Testovaný je skript zatím jen na Chromu. Založit v daném pluginu nový skript (pro Chrome je tutorial zde , pro Firefox zde ). Smazat kostru nového skriptu a nahradit ji skriptem, který si zkopírujete výše. Skript uložit. Používat KonText jako normálně -- skript už by podle adresy měl sám poznat, že se má spustit. Pokud se tak nestane, nejspíš to znamená, že je prohlížečový plugin (Tampermonkey nebo Greasemonkey) deaktivovaný a je potřeba jej znovu aktivovat. V předchozí verzi se po aplikaci libovolného filtru změnil obsah hledacího boxu na parametry filtrování. ↩","tags":"ling","url":"kontext-interface-tweak-update.html"},{"title":"Úprava rozhraní konkordanceru KonText","text":"!POZOR! K dispozici je nyní vylepšená verze níže popsaného skriptu . Hledání v korpusech ČNK Český národní korpus je sbírka jazykových korpusů částečně vytvářených Ústavem Českého národního korpusu a částečně jinými institucemi. Všechny jsou hostované na jednom serveru a dostupné skrz různá vyhledávací rozhraní (tzv. konkordancery ), např. NoSke , Bonito či nejnověji KonText . Koncem března 2015 ovšem bude podpora starších rozhraní ukončena a nadále půjde k datům v ČNK přistupovat primárně pouze přes KonText. (Pokud vám odstavec výše nedává příliš smysl, s jazykovými korpusy se setkáváte poprvé, ale chcete se dozvědět víc, raději si místo tohoto postu přečtěte, k čemu je takový korpus dobrý , a zkuste si v něm něco pro zajímavost vyhledat . Pokud se vám při vzpomínce na Bonito či NoSke naopak zaskvěla slza v oku, čtěte dál!) KonText vs. Bonito / NoSke KonText má oproti starším rozhraním řadu výhod -- bohatší funkcionalitu, mnohé pomůcky, které vám pomohou se zadáním složitějších dotazů (sestavení morfologického tagu či podmínky within ), a v neposlední řadě mnohem lépe vypadá, což kupříkladu mně při práci působí jako balzám na duši. Nicméně dlouholetí uživatelé ČNK byli jednoduše zvyklí na některé aspekty Bonita a NoSke, které jim teď v KonTextu chybí. Onehdy při rozhovoru s jedním z nich vyplavaly na povrch jako hodně důležité dvě stížnosti: Vrchní menu v KonTextu je zákeřné, schovává se, člověk nemá přehled nad dostupnými funkcemi. Oproti tomu NoSke má menu po straně a je permanentně rozvinuté, takže uživatel má všechny možnosti interakce s konkordancí soustavně jako na dlani. Po zadání dotazu člověk často na základě konkordance zjistí, že jej potřebuje ještě trochu upravit / zjemnit. KonText si sice předchozí dotazy pamatuje, je ale potřeba se k nim doklikat; šikovnější by bylo, kdyby tato možnost byla dostupná přímo ze stránky konkordance v podobě nějakého zjednodušeného hledacího boxu. (NoSke tohle vlastně taky neumí, v Bonitu je to jednodušší.) V obou případech jde o smysluplné požadavky, jenže KonText je poměrně velká a složitá aplikace, takže i pokud se ČNK rozhodne do ní tyto podněty v nějaké podobě zapracovat (např. jako možnost přepnutí zobrazení menu), bude nějakou chvíli trvat, než se implementace navrhne, vytvoří, řádně otestuje a konečně dostane k uživatelům. Nicméně aby bylo možné alespoň vyzkoušet, jak by zmíněné změny vypadaly v praxi, dal jsem dohromady krátký skript, který již v prohlížeči nahraný KonText trochu \"přestaví\" a upraví. Výsledek vypadá následovně: Rovnou předesílám: ten skript je nevzhledný bastl přilepený na KonText zvnějšku; proto taky bylo možné jej dát dohromady poměrně rychle, protože si neklade nárok na spolehlivost, která se vyžaduje od oficiální verze KonTextu. Je to spíš prototyp, jehož účelem je otestovat výše popsané změny v praxi a získat představu o tom, zda a do jaké míry jsou přínosné. (Vlastní zkušenost: po chvíli používání mi přijde přídatný hledací box nad konkordancí hodně šikovný a užitečný.) Teď k jádru pudla: pokud máte zájem, můžete si KonText takto k obrazu svému (resp. k obrázku o odstavec výš) upravit také a vyzkoušet, jak vám takové nastavení vyhovuje. Když se vám jedna z úprav bude líbit (nebo vás u toho napadne jiná, kterou by si KonText zasloužil), můžete pak zadat požadavek na nový feature . Návod, jak si KonText upravit, následuje níže. Postup instalace skriptu Skript samotný je k dispozici zde: K jeho zprovoznění jsou potřeba následující kroky: Nainstalovat si do svého prohlížeče plugin Tampermonkey , pokud používáte Chrome, nebo Greasemonkey , pokud používáte Firefox. (Pokud používáte Internet Explorer, budete muset dočasně přesedlat na Chrome nebo Firefox.) Testovaný je skript zatím jen na Chromu. Založit v daném pluginu nový skript (pro Chrome je tutorial zde , pro Firefox zde ). Smazat kostru nového skriptu a nahradit ji skriptem, který si zkopírujete výše. Skript uložit. Používat KonText jako normálně -- skript už by podle adresy měl sám poznat, že se má spustit. Pokud se tak nestane, nejspíš to znamená, že je prohlížečový plugin (Tampermonkey nebo Greasemonkey) deaktivovaný a je potřeba jej znovu aktivovat. Omezení Skript má pravděpodobně hromadu drobných much, na které se mi zatím nepodařilo přijít -- budu se je snažit průběžně opravovat, když na ně padnu, nebo když mi o nich dáte vědět . Krom toho má i některé mouchy, o nichž už vím, ale bohužel toho s nimi nejde moc dělat. Asi nejnápadnější je, že přidaný hledací box funguje jen na těch stránkách, kde je původní dotaz i součástí adresy URL (což nejsou všechny -- třeba když začnete listovat konkordancí na druhou stránku a dál, dotaz je z adresy vyjmut a pomocný hledací box tedy zmizí ). Ale vzhledem k tomu, že jeho hlavní účel má být možnost lehce upravit dotaz po prvním rychlém nahlédnutí do konkordance, snad to nebude takový problém. Pokud někdy bude podobný box řádně přidán přímo do KonTextu, takovými nedostatky samozřejmě trpět nebude. A ještě k používání přidaného hledacího boxu : Typ dotazu, který je do něj potřeba zadat, je stejný jako ten, který jste při prvotním vyhledání konkordance zadali na stránce Nový dotaz . Pokud tento prvotní dotaz byl Základní dotaz, můžete pomocí rychlého boxu zadat jiný Základní dotaz; pokud to byl CQL dotaz, můžete ho upravit zas jen na další CQL dotaz. Důvodem je, že smyslem tohoto pomocného boxu není nahradit plnohodnotný formulář pro zadání dotazu, jen poskytnout rychlou možnost, jak již zadaný dotaz upravit . Pomocný hledací box se objeví i poté, co na konkordanci provedete filtrování. V takové situaci se dá použít k tomu, abyste pozměnili zadání aktuálního filtru , tj. filtrování se provede znovu na původní konkordanci, ne na této již filtrované. Pokud chcete opakovaně filtrovat tu samou konkordanci a postupně podle daných kritérií vyřazovat / přidávat řádky, je potřeba místo hledacího boxu opakovaně použít menu Filtr . Komu si stěžovat, když to nebude fungovat Skript je volně šiřitelný pod licencí GNU GPL v3 , takže se na něj neváže žádná záruka. Když se vám ale nebude dařit jej zprovoznit, rád se pokusím pomoct! Stačí se ozvat na adresu uvedenou zde .","tags":"ling","url":"kontext-interface-tweak.html"},{"title":"Beyond semantic versioning? (cross-post)","text":"Background Ever since I first read about semantic versioning , I've thought of it as a neat idea. But only recently did it occur to me that what I liked about the idea was its goal, much less its execution (more on that below). What made it obvious was this lengthy discussion about breaking changes introduced in v1.7 of underscore.js without an accompanying major version bump. Even though I still think sticking to semver is the right thing to do if your community of users expects you to (even if you don't personally like the system), I am convinced there are fundamentally better ways of dealing with the problem of safely and consistently updating dependencies. It made me want to add my two cents to the discussion , as someone who's more of a dabbler in programming and not really part of the community, so feel free to ignore me :) I attach my commentary below for reference (it's virtually the same text as in the link above). tl;dr semver is trying to do the right thing, but doing it wrong -- instead of implicitly encoding severity of change information in version numbers , explicit keywords like :patch, :potentially-breaking or :major-api-change would make much more sense. More verbosely I've always found the goals of semver worthy, but this thread has made me realize that while its aims are commendable, its methods are kind of broken: semver tries to take an existing semiotic system (= version numbers), which has developed informally and is therefore a loose convention rather than an exact spec, and reinterpret it in terms of an exact spec (or impose that spec on it). trouble is, the prior informal meaning won't go away so easily (why should it?), especially for projects that have been around longer than semver. the problem then is, since the two systems (the informal one and semver) look the same in terms of their symbolic representation, it's hard to guess which one you're dealing with by just eyeballing the version number of a library (or project in general). it's like if someone decided that \"f*ck\" should mean \"orchid\" from now on, because it's nicer -- on hearing the word, you'd never know if it's being used as the original profanity, or in its new meaning. homonymy is a pain to deal with when it's accidental (cf. NLP), so why introduce it on purpose? the job that semver set out to do should be fulfilled by a new formal means which is instantly recognizable, not by hijacking an existing one and overlaying additional interpretation on it and thus making it ambiguous . even if version numbers hadn't existed before semver, they're terribly inadequate for the purpose of conveying information about the severity of changes introduced by an update (though I understand their appeal to mathematically-minded people). they're inadequate because they're implicit -- it's a bit like if someone decided they don't need hash maps because they can make do with arrays by remembering the order in which they're adding in the key-val pairs. if I remember the order, then I know which key the given index implicitly refers to, and the result is as good as a hash map, isn't it? except it isn't. keys are useful because they have explicit semantics , making it instantly clear what kind of value you're retrieving. in the same way, encoding the information about the severity of changes into version numbers makes it implicit (in addition to being ambiguous, as stated previously). why not use explicit keyword tags along with the version number (which can be romantic, semantic -- whichever floats the dev team's boat and best reflects the progress of the project) to give a heads up as to the nature of the update? e.g. :patch, :potentially-breaking, :major-api-change etc. granted, even language is a code which needs to be learned, like semver (gross oversimplification here, but let's not get into the details of language acquisition), but since it's widely established and conventionalized for conveying the kinds of meanings semver is trying to convey, why not just use it when it's available ? why use a system (version numbers) which is less well-suited to the purpose and ambiguous to boot? (on the other hand, numbers are eminently well-suited for keeping track of which version is newer than which and how much so -- the original purpose of version numbering -- because they are designed to have orderings defined on them. by contrast, words would do a terrible job at this. if you care to indicate the evolution of your codebase, you might introduce your own disciplined romantic or sentimental versioning scheme, which ironically is a more meaningful and ergo semantic way of doing versioning than semver, because it sticks to the conventional semantics of numbers (the closer the numbers, the more similar the versions). if you don't care about this, which is perfectly fine, you might as well use dates for version numbers.) keyword tags have the advantage that they're instantly human-readable by anyone who has a basic command of English. if there is sufficient will in the community, a useful subset can be frozen in a binding spec, so that they are machine-readable as well. I'm not sure whether these keywords should be an appendix to the version number (like v2.3.4-:potentially-breaking), or whether the information they provide should be more extensive and included in a formalized preamble to the changelog (finally forcing people to at least take a glance at it ;) ). using the latter approach, the information provided could be (optionally) even more targeted, e.g. detailing explicitly which parts of the API are affected in a non-backwards compatible manner by the update. anyways, just a few ideas :) I am not primarily a coder, so there may be obvious drawbacks to this scheme that I can't see or which have already been discussed by the community on multiple occasions which have escaped my attention. in which case, please bear with me and excuse my lack of sophistication.","tags":"floss","url":"beyond-semver.html"},{"title":"Filling (hardwrapping) paragraphs in Airmail with `par`","text":"tl;dr Jump directly to the proposed solution . Tested on OS X 10.9 (Mavericks). Back story Airmail is a great application -- being very happy with Gmail's in-browser UI, it's honestly the first e-mail desktop client that I ever felt even remotely tempted to use. It has: a sleek, functional design almost flawless integration with Gmail (except for categories -- but there's a not-too-hackish way to deal with those) a Markdown compose mode (yay!) -- and tons of other good stuff. Especially that last feature almost got me sold -- you see, I like my e-mail hardwrapped (what Emacs calls \"filling paragraphs\"), because most of the time, I view it on monitors that are too wide for soft line wrapping to achieve a comfortable text width. (By the way, Airmail's layout deals with this issue very elegantly, but I know I won't be using only Airmail. Plus there are the obvious netiquette issues -- lines \"should be\" wrapped at 72 characters etc.) In Gmail, I therefore use plain-text compose, which is fine for the purposes described above, but frustrating whenever you want to apply formatting (obviously, you can't -- it's plain text). I tried using the usual replacements for formatting like stars & co., and I don't know about your grandma, but mine certainly doesn't take *...* to mean emphasis. I thought the Markdown compose mode in Airmail would solve my problems -- I could apply formatting if and when I wanted (using the frankly more streamlined process of typing it in rather than fumbling around for the right button in the GUI) and fill my paragraphs, because I somehow automatically assumed there'd by a hard-wrap feature like in any decent editor (read: emacs or vi). Markdown is plain text after all, isn't it? Long story short, as of yet, there isn't . There isn't even one for the plain-text compose mode, as far as I'm aware. So I added my two cents to this feature request thread and went back to the Gmail in-browser UI. Solution But then I realized (it took me a while, I'm still very much an OS X newbie): in OS X, you can define custom actions with shortcuts 1 for any application using Automator Services these actions can be easily set to receive text selected in the application as input these actions can also involve shell scripts there already is a great (command line) program for filling paragraphs -- it's called par , and as much as I admire what Airmail's developers have achieved, it's unlikely that they'd come up with a more sophisticated hard-wrapping algorithm than par 's simply as a side project for Airmail (see the EXAMPLES section in man par ) With that in mind, you can have hard-wrapping in Markdown or plain-text Airmail compose at your fingertips in no time flat. If you don't have homebrew , start by installing that (or any other ports manager that will allow you to install par ; I'll assume homebrew below) by pasting ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" at a Terminal prompt. Then: install par with brew install par at a Terminal prompt open Automator (e.g. by typing \"Automator\" into Spotlight) and create a new Service select the applications for which you want the service to be active (for me, that's just Airmail) and tick the \"Output replaces selected text\" box drag the \"Run Shell Script\" action onto the workflow canvas, and as the shell script, paste in PARINIT = \"rTbgqR B=.,?_A_a Q=_s>|\" /usr/local/bin/par 79 the $PARINIT environment variable contains the default recommended settings for par (if you want to customize its behavior, you can -- good luck wrapping your head around par 's manpage, though) you should set the full path to the par executable, the shell spawned by the Service might not inherit your $PATH -- for par installed via homebrew , it's /usr/local/bin/par the parameter at the end is the max number of characters per line -- mailing list etiquette stipulates 72, I personally prefer the pythonesque 79, but it's your choice At this point, your service should look something like in the screenshot below: Save it, open Keyboard preferences (type \"Keyboard\" into Spotlight), navigate to Shortcuts → Services → Text and set a keyboard shortcut for your newly created Service, e.g. Cmd+Opt+P. Next time you compose an e-mail in Airmail, just select the entire text when you're done (Cmd+A), press Cmd+Opt+P, and voilà! Your lines have been hardwrapped, your paragraphs filled :) (Same thing, I know.) If the shortcut doesn't appear to work 1 , try fiddling around with it, resetting it (maybe the one you've chosen conflicts with a pre-existing one?), restarting Airmail, logging out and back in, rebooting... The custom shortcut part is unfortunately the least reliable aspect of this whole setup. Automator is a great idea, I was pleasantly surprised by it when I started using OS X a few days back, but it could seriously use some bug-squashing. If you fail miserably at getting the shortcut to work, you can still access your fill paragraph service via the menu (select the text you want to hard-wrap, then navigate to Airmail → Services → <name of your fill paragraph service>). Clicking around in a GUI is tedious (though hey -- it's the Apple way after all, isn't it?), but it shouldn't be too much of a bother since you need to do it only once per e-mail. Bottom line : I am now officially completely sold on Airmail (even bought the released version instead of using the free beta) and look forward to the joy of using it! EDIT: In order to have the least trouble possible getting the shell script up and running as a Service , two rules of thumb: Leave it completely up to OS X where it stores the Service (.workflow) file. This will probably be in ~/Library/Services , and I learnt the hard way not to tinker with it -- if Services is a symlink instead of a real directory, the OS won't discover new Service files (though old ones will still be accessible). If the Service doesn't show up in the keyboard shortcuts menu after creation, try refreshing the service list with /System/Library/CoreServices/pbs -update . Those shortcuts are in fact quite buggy, especially those that you want to be global (not specific to a concrete app) -- at least on Mavericks (OS X 10.9). They tend to get disabled on a whim, especially if you tinker with them, and are a pain to get working again (login, logout, reboot -- anything goes). If anyone knows why, please let me know! ↩ ↩","tags":"macOS","url":"fill-par-in-airmail.html"}]}