{"pages":[{"title":"about me","text":"Contact David Lukeš, Personal I'm a phonetician/linguist with side interests in statistics, programming and NLP. In fact, I'm terminally curious about computer-related stuff in general, which has led me down more rabbit holes than I'd care to count. I hail from Prague, Czech Republic, a precious little gem of a city, but I've also lived in France and Belgium for a while when I was a kid. I play badminton and football (of the soccer persuasion) and enjoy reading American Jewish literature, as well as watching (in no particular order) Monty Python, Bill Bailey, Jon Stewart, Ricky Gervais, Stephen Fry and other assorted smart and funny people who, along with Frank Zappa and The Beatles, are collectively responsible for my lifelong crush on the English language. I used to play the saxophone reasonably well, but nowadays mostly enjoy playing the guitar atrociously bad and singing along. Academic and professional credentials Current position I'm a junior researcher at the Institute of the Czech National Corpus , Faculty of Arts , Charles University (Prague) . My duties/interests include: coordinating phonetic transcription for the ORTOFON corpus project data processing and functionality prototyping (mainly using Python and R) research into the scope of language variability in Czech using multi-dimensional analysis For further details and older history, see my CV (updated spring 2017). Code When coding, I feel most comfortable in Python, and I love spending time with Rust (though I'm nowhere near as proficient, and occasionally lost and frustrated). I find R useful (though convoluted on occasion), write JavaScript when forced to and avoid Perl as much as I can. I know my way around the Unix command line and enjoy using it. And even after years of casual acquaintance, I'm still not really sure how I feel about Lisp, though I'll take Emacs Lisp over Vimscript any day. Feel free to check out my GitHub profile . MA thesis If you're interested in the affinities between people's abilities to process language and music, my MA thesis was about Perceptual sensitivity to music and speech stimuli in the frequency and temporal domains and I'm moderately proud of it. Unfortunately, it's in Czech (with an English abstract). Here's a quick summary: I was curious to know whether having a good musical ear correlates with having a good \"speech perception ear\" (spoiler alert: it does), so I put together a listening test (using Praat ) in which I made participants listen to short utterances and melodies to see how well they could detect manipulations in pitch and timing. There's also a companion repository on GitHub which contains the listening test (so that anyone can run it in Praat) and raw results as they were collected (for reproducibility). BA thesis Way back when, my academic interests lay in literary theory, so I wrote a BA thesis on Philip Roth . I still like Philip Roth very much, but I mostly tend to steer away from literary theory nowadays because I realized I'd rather be wrapping my head around logically coherent complex abstract notions.","tags":"pages","url":"pages/about.html"},{"title":"Make the most of Python Jupyter notebooks","text":"Python-based Jupyter notebooks mostly consist of Python code, obviously, and some Markdown text. But they also offer some very handy functions and shortcuts which are not available in Python itself, and which are really helpful for interactive work. This is my personal best of / reference. The shortcuts fall into two groups: magics : special functions with special syntax whose names start with % (in which case they apply to the rest of the line → \"line magics\") or %% (in which case they apply to the entire cell → \"cell magics\") command line programs : if you know how to use command line programs, you can do so directly from the notebook by prefixing the command line invocation with ! If you want to follow along, the easiest way is to just click this link and have Binder launch a Jupyter environment with the notebook loaded for you. Or you can download this post in its original notebook format here and load it into your own Jupyter instance yourself. NB: Most of the following also applies to the IPython REPL . Magics The syntax of magic functions is modeled after the syntax of command line programs: to call them, just write their name and evaluate the cell, without any parentheses (unlike regular Python functions, which are called like this: function() ) arguments are separated just by whitespace (in Python, there are commas: function(arg1, arg) ) some have optional arguments ( options ) which tweak their behavior: these are formed by a hyphen and a letter, e.g. -r Getting help You can read more about the magic function system by calling the %magic magic: In [1]: % magic %quickref brings up a useful cheat sheet of special functionality: In [2]: % quickref If you want more information about an object, %pinfo and %pinfo2 are your friends: In [3]: def foo (): \"This foo function returns bar.\" return \"bar\" In [4]: # shows the object's docstring % pinfo foo In [5]: # shows the full source code % pinfo2 foo These are so handy that they have their own special syntax: ? and ?? , placed either before or after the object's name: In [6]: ? foo In [7]: foo ? In [8]: ?? foo In [9]: foo ?? Of course, this also works with magic functions: In [10]: ? %pylab You can also open a documentation popup by pressing Shift+Tab with your cursor placed in or after a variable name. Repeating the command cycles through different levels of detail. Manipulating objects The appeal of an interactive environment like Jupyter is that you can inspect any object you're working with by just evaluating it: In [11]: foo = 1 In [12]: foo Out[12]: 1 %who and %whos will show you all the objects you've defined: In [13]: % who foo In [14]: % whos Variable Type Data/Info ---------------------------- foo int 1 Sometimes though, these objects are large and you don't want to litter your notebook with tons of output you'll delete right afterwards. (Also, if you forget to delete it, your notebook might get too large to save .) That's when you need to use the Jupyter pager, which lets you inspect an object in a separate window. In [15]: foo = \"This is a line of text. \\n \" * 1000 In [16]: % page foo By default, the pager pretty-prints objects using the pprint() function from the pprint module. This is handy for collections, because it nicely shows the nesting hierarchy, but not so much for strings, because special characters like newlines \\n are shown as escape sequences. If you want the string to look like it would if it were a text file, pass the -r option (\"raw\") to page through the result of calling str() on the object instead: In [17]: % page -r foo If you want to inspect the source code of a module, use %pfile on the object representing that module, or an object imported from that module: In [18]: import os from random import choice In [19]: % pfile os In [20]: % pfile choice Sometimes, you create an object which you know you will want to reuse in a different session or maybe in a completely different notebook. A lightweight way to achieve this is using the %store magic: In [21]: % store foo Stored 'foo' (str) You can list the values stored in your database by invoking %store without arguments: In [22]: % store Stored variables and their in-db values: foo -> 'This is a line of text.\\nThis is a line of text.\\ To restore a variable from the database into your current Python process, use the -r option: In [23]: # restores only `foo` % store -r foo In [24]: # restores all variables in the database % store -r And this is how you clear no longer needed variables from storage: In [25]: # removes `foo` % store -d foo In [26]: # removes all variables % store -z In [27]: % store Stored variables and their in-db values: Working with the file system %ls lists files in the directory where your notebook is stored: In [28]: % ls 3foos.py command_line_intro.ipynb pos_tagging.ipynb zipf.ipynb classification.ipynb foo.py regex.ipynb cmudict.ipynb jupyter_magic.ipynb unicode.ipynb collocations.ipynb libraries.ipynb xcorr_vs_conv.ipynb If you provide a path as argument, it lists that directory instead: In [29]: % ls /etc/nginx conf.d / koi-utf nginx.conf sites-available / uwsgi_params fastcgi.conf koi-win proxy_params sites-enabled / win-utf fastcgi_params mime.types scgi_params snippets / If you provide a glob pattern , then only files that match it are listed: In [30]: % ls /etc/nginx/*.conf /etc/nginx/fastcgi.conf /etc/nginx/nginx.conf %ll (\"long listing\") formats the listing as one entry per line with columns providing additional information: In [31]: % ll ~/edu/ total 12805 drwxrwsrwt+ 3 lukes 2000147 Oct 4 18:52 exchange / drwxrwsr-x+ 21 lukes 2041640 Oct 24 14:44 lukes / drwxrwsr-x+ 9 lukes 2060567 Dec 18 21:54 mda / drwxrwsr-x+ 14 lukes 3001102 Oct 31 17:39 python / drwxrwsr-x+ 4 lukes 2004559 Feb 23 2017 r / drwxr-sr-x+ 4 lukes 2002420 Mar 9 2017 textlink / One of those columns indicates file size, which is great, but they're in bytes, which is less great (hard to read at a glance). The -h option makes the file sizes print in human-readable format: In [32]: % ll -h ~/edu/python/syn* -rw-rw-r--+ 1 lukes 1.5G Nov 1 2016 /home/lukes/edu/python/syn2015.gz -rw-rw-r--+ 1 lukes 112M Nov 2 2016 /home/lukes/edu/python/syn2015_sample %%writefile writes the contents of a cell to a file: In [33]: %% writefile foo.py def foo(): \"This foo function returns bar.\" return \"bar\" Writing foo.py %cat prints the contents of a file into the notebook: In [34]: % cat foo.py def foo(): \"This foo function returns bar.\" return \"bar\" %cat is called %cat because it can also con cat enate multiple files (or the same file, multiple times): In [35]: % cat foo.py foo.py def foo(): \"This foo function returns bar.\" return \"bar\" def foo(): \"This foo function returns bar.\" return \"bar\" The output of %cat can be saved into a file with > (if the file exists, it's overwritten): In [36]: % cat foo.py foo.py >3foos.py In [37]: % cat 3foos.py def foo(): \"This foo function returns bar.\" return \"bar\" def foo(): \"This foo function returns bar.\" return \"bar\" Hey! Our 3foos.py is one foo short. Let's add it by appending to the file with >> : In [38]: % cat foo.py >>3foos.py In [39]: % cat 3foos.py def foo(): \"This foo function returns bar.\" return \"bar\" def foo(): \"This foo function returns bar.\" return \"bar\" def foo(): \"This foo function returns bar.\" return \"bar\" There, much better. %less opens a file in the pager (with nice syntax highlighting if it's a Python source file): In [40]: % less foo.py %less is named after the program less , which is used to page through text files at the command line. Why is the original less called \"less\"? Because an earlier pager program was called more (as in \"show me more of this text file\"), and as the saying goes, \"less is more\". (Programmers are fond of dad jokes. I like how this one works on multiple levels -- the literal meaning that less -the-program is intended to replace more -the-program interacts with the figurative meaning that having less is better than having more, and both coalesce into \"use less because it's better than more \".) %cat and %ls are also named after corresponding command line programs. Finding out more about your code When developing, code often behaves differently from what you intended when you wrote it. The following tools might help you find out why. Timing the execution of a piece of code will help you determine if it's slowing you down. The %timeit magic has your back, it runs your code repeatedly and thus provides more reliable estimates. It comes in both line and cell variants. In [41]: % timeit sorted(range(1_000_000)) 61.4 ms ± 7.63 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) In [42]: %% timeit lst = list(range(1_000_000)) sorted(lst) 68.8 ms ± 7.57 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) The cell variant can include initialization code on the first line, which is run only once: In [43]: %% timeit lst = list(range(1_000_000)) sorted(lst) 28.8 ms ± 4.84 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) If you have the memory_profiler library installed, you can load its magic extension and use %memit in the same way as %timeit to get a notion of how much memory your code is consuming. In [44]: % load_ext memory_profiler In [45]: % memit list(range(1_000_000)) peak memory: 82.43 MiB, increment: 35.71 MiB Peak memory is the highest total amount of memory the Python process used when your code ran. Increment is peak memory minus the amount of memory Python used before your code ran. In [46]: %% memit lst = list(range(1_000_000)) even = [i for i in lst if i % 2 == 0] peak memory: 127.82 MiB, increment: 73.29 MiB In [47]: %% memit lst = list(range(1_000_000)) even = [i for i in lst if i % 2 == 0] peak memory: 98.02 MiB, increment: -30.69 MiB If you have a more involved piece of code where multiple functions are called, you may need more granular information about running times than that provided by %timeit . In that case, you can resort to profiling using the %prun magic. Profiling tells you how fast different parts of your code run relative to each other, in other words, where your bottlenecks are. In [48]: import time def really_slow (): time . sleep ( 1 ) def fast (): pass def only_slow_because_it_calls_another_slow_function (): fast () really_slow () In [49]: % prun only_slow_because_it_calls_another_slow_function() The results show up in the pager, here's a copy: 7 function calls in 1.001 seconds Ordered by: internal time ncalls tottime percall cumtime percall filename:lineno(function) 1 1.001 1.001 1.001 1.001 {built-in method time.sleep} 1 0.000 0.000 1.001 1.001 {built-in method builtins.exec} 1 0.000 0.000 1.001 1.001 <ipython-input-81-8d3b1f67a0d9>:3(really_slow) 1 0.000 0.000 1.001 1.001 <ipython-input-81-8d3b1f67a0d9>:9(only_slow_because_it_calls_another_slow_function) 1 0.000 0.000 1.001 1.001 <string>:1(<module>) 1 0.000 0.000 0.000 0.000 <ipython-input-81-8d3b1f67a0d9>:6(fast) 1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects} %prun also has a cell variant: In [50]: %% prun really_slow() fast() 6 function calls in 1.001 seconds Ordered by: internal time ncalls tottime percall cumtime percall filename:lineno(function) 1 1.001 1.001 1.001 1.001 {built-in method time.sleep} 1 0.000 0.000 1.001 1.001 {built-in method builtins.exec} 1 0.000 0.000 1.001 1.001 <string>:2(<module>) 1 0.000 0.000 1.001 1.001 <ipython-input-81-8d3b1f67a0d9>:3(really_slow) 1 0.000 0.000 0.000 0.000 <ipython-input-81-8d3b1f67a0d9>:6(fast) 1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects} Perhaps the most useful magic for development is %debug , which allows you to pause the execution of a piece of code, examine the variables which are defined at that moment in time, resume execution fully or step-by-step etc. You can either pass a statement that you want to debug as argument: In [51]: def foo (): for i in range ( 10 ): print ( \"printing\" , i ) In [52]: % debug foo() NOTE: Enter 'c' at the ipdb> prompt to continue execution. > <string> (1) <module> () ipdb> help Documented commands (type help <topic>): ======================================== EOF cl disable interact next psource rv unt a clear display j p q s until alias commands down jump pdef quit source up args condition enable l pdoc r step w b cont exit list pfile restart tbreak whatis break continue h ll pinfo return u where bt d help longlist pinfo2 retval unalias c debug ignore n pp run undisplay Miscellaneous help topics: ========================== exec pdb ipdb> step --Call-- > <ipython-input-51-e20c0aea6cfb> (1) foo () ----> 1 def foo ( ) : 2 for i in range ( 10 ) : 3 print ( \"printing\" , i ) ipdb> next > <ipython-input-51-e20c0aea6cfb> (2) foo () 1 def foo ( ) : ----> 2 for i in range ( 10 ) : 3 print ( \"printing\" , i ) ipdb> next > <ipython-input-51-e20c0aea6cfb> (3) foo () 1 def foo ( ) : 2 for i in range ( 10 ) : ----> 3 print ( \"printing\" , i ) ipdb> i 0 ipdb> next printing 0 > <ipython-input-51-e20c0aea6cfb> (2) foo () 1 def foo ( ) : ----> 2 for i in range ( 10 ) : 3 print ( \"printing\" , i ) ipdb> next > <ipython-input-51-e20c0aea6cfb> (3) foo () 1 def foo ( ) : 2 for i in range ( 10 ) : ----> 3 print ( \"printing\" , i ) ipdb> i 1 ipdb> quit Or you can invoke plain %debug after an exception has been raised to jump directly to the place where the error occurred, so that you can figure out why things went wrong: In [53]: def foo (): dct = dict ( foo = 1 ) return dct [ \"bar\" ] In [54]: foo () --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-54-c19b6d9633cf> in <module> () ----> 1 foo ( ) <ipython-input-53-29ed6ce0c4f1> in foo () 1 def foo ( ) : 2 dct = dict ( foo = 1 ) ----> 3 return dct [ \"bar\" ] KeyError : 'bar' In [55]: % debug > <ipython-input-53-29ed6ce0c4f1> (3) foo () 1 def foo ( ) : 2 dct = dict ( foo = 1 ) ----> 3 return dct [ \"bar\" ] ipdb> \"bar\" in dct False ipdb> dct.keys() dict_keys(['foo']) ipdb> quit If you want to pause one of your functions and explore its state at a particular point, set a breakpoint using the set_trace() function from the IPython.core.debugger module. The debugger will be automatically invoked when the call to set_trace() is reached during execution: In [56]: from IPython.core.debugger import set_trace def foo (): for i in range ( 2 ): set_trace () print ( \"printing\" , i ) In [57]: foo () > <ipython-input-56-805417d84ad8> (6) foo () 2 3 def foo ( ) : 4 for i in range ( 2 ) : 5 set_trace ( ) ----> 6 print ( \"printing\" , i ) ipdb> i 0 ipdb> continue printing 0 > <ipython-input-56-805417d84ad8> (5) foo () 2 3 def foo ( ) : 4 for i in range ( 2 ) : ----> 5 set_trace ( ) 6 print ( \"printing\" , i ) ipdb> i 1 ipdb> continue printing 1 The Python debugger is called pdb and it has some special commands of its own which allow you to step through the execution. They can be listed by typing help at the debugger prompt (see above), or you can have a look at the documentation . The examples above also illustrate what a typical debugging session looks like (stepping through the program, inspecting variables). When you want to stop debugging, don't forget to quit the debugger with quit (or just q ) at the debugger prompt, or else your Python process will become unresponsive. Plotting Jupyter is tightly integrated with the matplotlib plotting library. Plotting is enabled by running the %matplotlib magic with an argument specifying how the notebook should handle graphical output. %matplotlib notebook will generate an interactive plot which you can resize, pan, zoom and more. A word of caution though: when using this variant, once you're done with the plot , don't forget to \"freeze\" it using the ⏻ symbol in the upper right corner, or else subsequent plotting commands from different cells will all draw into this same plot. In [58]: % matplotlib notebook In [59]: import matplotlib.pyplot as plt In [60]: plt . plot ( range ( 10 )) var element = $('#cdbac897-7165-4ac9-900f-7ffcd2a57b16'); /* Put everything inside the global mpl namespace */ window.mpl = {}; mpl.get_websocket_type = function() { if (typeof(WebSocket) !== 'undefined') { return WebSocket; } else if (typeof(MozWebSocket) !== 'undefined') { return MozWebSocket; } else { alert('Your browser does not have WebSocket support.' + 'Please try Chrome, Safari or Firefox ≥ 6. ' + 'Firefox 4 and 5 are also supported but you ' + 'have to enable WebSockets in about:config.'); }; } mpl.figure = function(figure_id, websocket, ondownload, parent_element) { this.id = figure_id; this.ws = websocket; this.supports_binary = (this.ws.binaryType != undefined); if (!this.supports_binary) { var warnings = document.getElementById(\"mpl-warnings\"); if (warnings) { warnings.style.display = 'block'; warnings.textContent = ( \"This browser does not support binary websocket messages. \" + \"Performance may be slow.\"); } } this.imageObj = new Image(); this.context = undefined; this.message = undefined; this.canvas = undefined; this.rubberband_canvas = undefined; this.rubberband_context = undefined; this.format_dropdown = undefined; this.image_mode = 'full'; this.root = $('<div/>'); this._root_extra_style(this.root) this.root.attr('style', 'display: inline-block'); $(parent_element).append(this.root); this._init_header(this); this._init_canvas(this); this._init_toolbar(this); var fig = this; this.waiting = false; this.ws.onopen = function () { fig.send_message(\"supports_binary\", {value: fig.supports_binary}); fig.send_message(\"send_image_mode\", {}); if (mpl.ratio != 1) { fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio}); } fig.send_message(\"refresh\", {}); } this.imageObj.onload = function() { if (fig.image_mode == 'full') { // Full images could contain transparency (where diff images // almost always do), so we need to clear the canvas so that // there is no ghosting. fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height); } fig.context.drawImage(fig.imageObj, 0, 0); }; this.imageObj.onunload = function() { this.ws.close(); } this.ws.onmessage = this._make_on_message_function(this); this.ondownload = ondownload; } mpl.figure.prototype._init_header = function() { var titlebar = $( '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' + 'ui-helper-clearfix\"/>'); var titletext = $( '<div class=\"ui-dialog-title\" style=\"width: 100%; ' + 'text-align: center; padding: 3px;\"/>'); titlebar.append(titletext) this.root.append(titlebar); this.header = titletext[0]; } mpl.figure.prototype._canvas_extra_style = function(canvas_div) { } mpl.figure.prototype._root_extra_style = function(canvas_div) { } mpl.figure.prototype._init_canvas = function() { var fig = this; var canvas_div = $('<div/>'); canvas_div.attr('style', 'position: relative; clear: both; outline: 0'); function canvas_keyboard_event(event) { return fig.key_event(event, event['data']); } canvas_div.keydown('key_press', canvas_keyboard_event); canvas_div.keyup('key_release', canvas_keyboard_event); this.canvas_div = canvas_div this._canvas_extra_style(canvas_div) this.root.append(canvas_div); var canvas = $('<canvas/>'); canvas.addClass('mpl-canvas'); canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\") this.canvas = canvas[0]; this.context = canvas[0].getContext(\"2d\"); var backingStore = this.context.backingStorePixelRatio || this.context.webkitBackingStorePixelRatio || this.context.mozBackingStorePixelRatio || this.context.msBackingStorePixelRatio || this.context.oBackingStorePixelRatio || this.context.backingStorePixelRatio || 1; mpl.ratio = (window.devicePixelRatio || 1) / backingStore; var rubberband = $('<canvas/>'); rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\") var pass_mouse_events = true; canvas_div.resizable({ start: function(event, ui) { pass_mouse_events = false; }, resize: function(event, ui) { fig.request_resize(ui.size.width, ui.size.height); }, stop: function(event, ui) { pass_mouse_events = true; fig.request_resize(ui.size.width, ui.size.height); }, }); function mouse_event_fn(event) { if (pass_mouse_events) return fig.mouse_event(event, event['data']); } rubberband.mousedown('button_press', mouse_event_fn); rubberband.mouseup('button_release', mouse_event_fn); // Throttle sequential mouse events to 1 every 20ms. rubberband.mousemove('motion_notify', mouse_event_fn); rubberband.mouseenter('figure_enter', mouse_event_fn); rubberband.mouseleave('figure_leave', mouse_event_fn); canvas_div.on(\"wheel\", function (event) { event = event.originalEvent; event['data'] = 'scroll' if (event.deltaY < 0) { event.step = 1; } else { event.step = -1; } mouse_event_fn(event); }); canvas_div.append(canvas); canvas_div.append(rubberband); this.rubberband = rubberband; this.rubberband_canvas = rubberband[0]; this.rubberband_context = rubberband[0].getContext(\"2d\"); this.rubberband_context.strokeStyle = \"#000000\"; this._resize_canvas = function(width, height) { // Keep the size of the canvas, canvas container, and rubber band // canvas in synch. canvas_div.css('width', width) canvas_div.css('height', height) canvas.attr('width', width * mpl.ratio); canvas.attr('height', height * mpl.ratio); canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;'); rubberband.attr('width', width); rubberband.attr('height', height); } // Set the figure to an initial 600x600px, this will subsequently be updated // upon first draw. this._resize_canvas(600, 600); // Disable right mouse context menu. $(this.rubberband_canvas).bind(\"contextmenu\",function(e){ return false; }); function set_focus () { canvas.focus(); canvas_div.focus(); } window.setTimeout(set_focus, 100); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items) { var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { // put a spacer in here. continue; } var button = $('<button/>'); button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' + 'ui-button-icon-only'); button.attr('role', 'button'); button.attr('aria-disabled', 'false'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); var icon_img = $('<span/>'); icon_img.addClass('ui-button-icon-primary ui-icon'); icon_img.addClass(image); icon_img.addClass('ui-corner-all'); var tooltip_span = $('<span/>'); tooltip_span.addClass('ui-button-text'); tooltip_span.html(tooltip); button.append(icon_img); button.append(tooltip_span); nav_element.append(button); } var fmt_picker_span = $('<span/>'); var fmt_picker = $('<select/>'); fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content'); fmt_picker_span.append(fmt_picker); nav_element.append(fmt_picker_span); this.format_dropdown = fmt_picker[0]; for (var ind in mpl.extensions) { var fmt = mpl.extensions[ind]; var option = $( '<option/>', {selected: fmt === mpl.default_extension}).html(fmt); fmt_picker.append(option) } // Add hover states to the ui-buttons $( \".ui-button\" ).hover( function() { $(this).addClass(\"ui-state-hover\");}, function() { $(this).removeClass(\"ui-state-hover\");} ); var status_bar = $('<span class=\"mpl-message\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; } mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) { // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client, // which will in turn request a refresh of the image. this.send_message('resize', {'width': x_pixels, 'height': y_pixels}); } mpl.figure.prototype.send_message = function(type, properties) { properties['type'] = type; properties['figure_id'] = this.id; this.ws.send(JSON.stringify(properties)); } mpl.figure.prototype.send_draw_message = function() { if (!this.waiting) { this.waiting = true; this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id})); } } mpl.figure.prototype.handle_save = function(fig, msg) { var format_dropdown = fig.format_dropdown; var format = format_dropdown.options[format_dropdown.selectedIndex].value; fig.ondownload(fig, format); } mpl.figure.prototype.handle_resize = function(fig, msg) { var size = msg['size']; if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) { fig._resize_canvas(size[0], size[1]); fig.send_message(\"refresh\", {}); }; } mpl.figure.prototype.handle_rubberband = function(fig, msg) { var x0 = msg['x0'] / mpl.ratio; var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio; var x1 = msg['x1'] / mpl.ratio; var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio; x0 = Math.floor(x0) + 0.5; y0 = Math.floor(y0) + 0.5; x1 = Math.floor(x1) + 0.5; y1 = Math.floor(y1) + 0.5; var min_x = Math.min(x0, x1); var min_y = Math.min(y0, y1); var width = Math.abs(x1 - x0); var height = Math.abs(y1 - y0); fig.rubberband_context.clearRect( 0, 0, fig.canvas.width, fig.canvas.height); fig.rubberband_context.strokeRect(min_x, min_y, width, height); } mpl.figure.prototype.handle_figure_label = function(fig, msg) { // Updates the figure title. fig.header.textContent = msg['label']; } mpl.figure.prototype.handle_cursor = function(fig, msg) { var cursor = msg['cursor']; switch(cursor) { case 0: cursor = 'pointer'; break; case 1: cursor = 'default'; break; case 2: cursor = 'crosshair'; break; case 3: cursor = 'move'; break; } fig.rubberband_canvas.style.cursor = cursor; } mpl.figure.prototype.handle_message = function(fig, msg) { fig.message.textContent = msg['message']; } mpl.figure.prototype.handle_draw = function(fig, msg) { // Request the server to send over a new figure. fig.send_draw_message(); } mpl.figure.prototype.handle_image_mode = function(fig, msg) { fig.image_mode = msg['mode']; } mpl.figure.prototype.updated_canvas_event = function() { // Called whenever the canvas gets updated. this.send_message(\"ack\", {}); } // A function to construct a web socket function for onmessage handling. // Called in the figure constructor. mpl.figure.prototype._make_on_message_function = function(fig) { return function socket_on_message(evt) { if (evt.data instanceof Blob) { /* FIXME: We get \"Resource interpreted as Image but * transferred with MIME type text/plain:\" errors on * Chrome. But how to set the MIME type? It doesn't seem * to be part of the websocket stream */ evt.data.type = \"image/png\"; /* Free the memory for the previous frames */ if (fig.imageObj.src) { (window.URL || window.webkitURL).revokeObjectURL( fig.imageObj.src); } fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL( evt.data); fig.updated_canvas_event(); fig.waiting = false; return; } else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") { fig.imageObj.src = evt.data; fig.updated_canvas_event(); fig.waiting = false; return; } var msg = JSON.parse(evt.data); var msg_type = msg['type']; // Call the \"handle_{type}\" callback, which takes // the figure and JSON message as its only arguments. try { var callback = fig[\"handle_\" + msg_type]; } catch (e) { console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg); return; } if (callback) { try { // console.log(\"Handling '\" + msg_type + \"' message: \", msg); callback(fig, msg); } catch (e) { console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg); } } }; } // from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas mpl.findpos = function(e) { //this section is from http://www.quirksmode.org/js/events_properties.html var targ; if (!e) e = window.event; if (e.target) targ = e.target; else if (e.srcElement) targ = e.srcElement; if (targ.nodeType == 3) // defeat Safari bug targ = targ.parentNode; // jQuery normalizes the pageX and pageY // pageX,Y are the mouse positions relative to the document // offset() returns the position of the element relative to the document var x = e.pageX - $(targ).offset().left; var y = e.pageY - $(targ).offset().top; return {\"x\": x, \"y\": y}; }; /* * return a copy of an object with only non-object keys * we need this to avoid circular references * http://stackoverflow.com/a/24161582/3208463 */ function simpleKeys (original) { return Object.keys(original).reduce(function (obj, key) { if (typeof original[key] !== 'object') obj[key] = original[key] return obj; }, {}); } mpl.figure.prototype.mouse_event = function(event, name) { var canvas_pos = mpl.findpos(event) if (name === 'button_press') { this.canvas.focus(); this.canvas_div.focus(); } var x = canvas_pos.x * mpl.ratio; var y = canvas_pos.y * mpl.ratio; this.send_message(name, {x: x, y: y, button: event.button, step: event.step, guiEvent: simpleKeys(event)}); /* This prevents the web browser from automatically changing to * the text insertion cursor when the button is pressed. We want * to control all of the cursor setting manually through the * 'cursor' event from matplotlib */ event.preventDefault(); return false; } mpl.figure.prototype._key_event_extra = function(event, name) { // Handle any extra behaviour associated with a key event } mpl.figure.prototype.key_event = function(event, name) { // Prevent repeat events if (name == 'key_press') { if (event.which === this._key) return; else this._key = event.which; } if (name == 'key_release') this._key = null; var value = ''; if (event.ctrlKey && event.which != 17) value += \"ctrl+\"; if (event.altKey && event.which != 18) value += \"alt+\"; if (event.shiftKey && event.which != 16) value += \"shift+\"; value += 'k'; value += event.which.toString(); this._key_event_extra(event, name); this.send_message(name, {key: value, guiEvent: simpleKeys(event)}); return false; } mpl.figure.prototype.toolbar_button_onclick = function(name) { if (name == 'download') { this.handle_save(this, null); } else { this.send_message(\"toolbar_button\", {name: name}); } }; mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) { this.message.textContent = tooltip; }; mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]]; mpl.extensions = [\"eps\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\"]; mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) { // Create a \"websocket\"-like object which calls the given IPython comm // object with the appropriate methods. Currently this is a non binary // socket, so there is still some room for performance tuning. var ws = {}; ws.close = function() { comm.close() }; ws.send = function(m) { //console.log('sending', m); comm.send(m); }; // Register the callback with on_msg. comm.on_msg(function(msg) { //console.log('receiving', msg['content']['data'], msg); // Pass the mpl event to the overriden (by mpl) onmessage function. ws.onmessage(msg['content']['data']) }); return ws; } mpl.mpl_figure_comm = function(comm, msg) { // This is the function which gets called when the mpl process // starts-up an IPython Comm through the \"matplotlib\" channel. var id = msg.content.data.id; // Get hold of the div created by the display call when the Comm // socket was opened in Python. var element = $(\"#\" + id); var ws_proxy = comm_websocket_adapter(comm) function ondownload(figure, format) { window.open(figure.imageObj.src); } var fig = new mpl.figure(id, ws_proxy, ondownload, element.get(0)); // Call onopen now - mpl needs it, as it is assuming we've passed it a real // web socket which is closed, not our websocket->open comm proxy. ws_proxy.onopen(); fig.parent_element = element.get(0); fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\"); if (!fig.cell_info) { console.error(\"Failed to find cell for figure\", id, fig); return; } var output_index = fig.cell_info[2] var cell = fig.cell_info[0]; }; mpl.figure.prototype.handle_close = function(fig, msg) { var width = fig.canvas.width/mpl.ratio fig.root.unbind('remove') // Update the output cell to use the data from the current canvas. fig.push_to_output(); var dataURL = fig.canvas.toDataURL(); // Re-enable the keyboard manager in IPython - without this line, in FF, // the notebook keyboard shortcuts fail. IPython.keyboard_manager.enable() $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">'); fig.close_ws(fig, msg); } mpl.figure.prototype.close_ws = function(fig, msg){ fig.send_message('closing', msg); // fig.ws.close() } mpl.figure.prototype.push_to_output = function(remove_interactive) { // Turn the data on the canvas into data in the output cell. var width = this.canvas.width/mpl.ratio var dataURL = this.canvas.toDataURL(); this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">'; } mpl.figure.prototype.updated_canvas_event = function() { // Tell IPython that the notebook contents must change. IPython.notebook.set_dirty(true); this.send_message(\"ack\", {}); var fig = this; // Wait a second, then push the new image to the DOM so // that it is saved nicely (might be nice to debounce this). setTimeout(function () { fig.push_to_output() }, 1000); } mpl.figure.prototype._init_toolbar = function() { var fig = this; var nav_element = $('<div/>') nav_element.attr('style', 'width: 100%'); this.root.append(nav_element); // Define a callback function for later on. function toolbar_event(event) { return fig.toolbar_button_onclick(event['data']); } function toolbar_mouse_event(event) { return fig.toolbar_button_onmouseover(event['data']); } for(var toolbar_ind in mpl.toolbar_items){ var name = mpl.toolbar_items[toolbar_ind][0]; var tooltip = mpl.toolbar_items[toolbar_ind][1]; var image = mpl.toolbar_items[toolbar_ind][2]; var method_name = mpl.toolbar_items[toolbar_ind][3]; if (!name) { continue; }; var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>'); button.click(method_name, toolbar_event); button.mouseover(tooltip, toolbar_mouse_event); nav_element.append(button); } // Add the status bar. var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>'); nav_element.append(status_bar); this.message = status_bar[0]; // Add the close button to the window. var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>'); var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>'); button.click(function (evt) { fig.handle_close(fig, {}); } ); button.mouseover('Stop Interaction', toolbar_mouse_event); buttongrp.append(button); var titlebar = this.root.find($('.ui-dialog-titlebar')); titlebar.prepend(buttongrp); } mpl.figure.prototype._root_extra_style = function(el){ var fig = this el.on(\"remove\", function(){ fig.close_ws(fig, {}); }); } mpl.figure.prototype._canvas_extra_style = function(el){ // this is important to make the div 'focusable el.attr('tabindex', 0) // reach out to IPython and tell the keyboard manager to turn it's self // off when our div gets focus // location in version 3 if (IPython.notebook.keyboard_manager) { IPython.notebook.keyboard_manager.register_events(el); } else { // location in version 2 IPython.keyboard_manager.register_events(el); } } mpl.figure.prototype._key_event_extra = function(event, name) { var manager = IPython.notebook.keyboard_manager; if (!manager) manager = IPython.keyboard_manager; // Check for shift+enter if (event.shiftKey && event.which == 13) { this.canvas_div.blur(); // select the cell after this one var index = IPython.notebook.find_cell_index(this.cell_info[0]); IPython.notebook.select(index + 1); } } mpl.figure.prototype.handle_save = function(fig, msg) { fig.ondownload(fig, null); } mpl.find_output_cell = function(html_output) { // Return the cell and output element which can be found *uniquely* in the notebook. // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\" // IPython event is triggered only after the cells have been serialised, which for // our purposes (turning an active figure into a static one), is too late. var cells = IPython.notebook.get_cells(); var ncells = cells.length; for (var i=0; i<ncells; i++) { var cell = cells[i]; if (cell.cell_type === 'code'){ for (var j=0; j<cell.output_area.outputs.length; j++) { var data = cell.output_area.outputs[j]; if (data.data) { // IPython >= 3 moved mimebundle to data attribute of output data = data.data; } if (data['text/html'] == html_output) { return [cell, data, j]; } } } } } // Register the function which deals with the matplotlib target/channel. // The kernel may be null if the page has been refreshed. if (IPython.notebook.kernel != null) { IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm); } Out[60]: [<matplotlib.lines.Line2D at 0x7ff8e9b94518>] By contrast, %matplotlib inline will just show a basic plot with a default size: In [61]: % matplotlib inline In [62]: plt . plot ( range ( 10 )) Out[62]: [<matplotlib.lines.Line2D at 0x7ff8e9afa358>] For more information on plotting with matplotlib, see their usage guide . Command line programs The operations listed in the section on magics for working with the file system can of course also be achieved using the corresponding command line programs, so if you know those, no need to memorize the magics. In fact, the magics are often just thin wrappers around the command line programs, which is why they are named the same. In [63]: ! ls --color -hArtl /etc/nginx total 56K -rw-r--r-- 1 root root 3.0K Mar 30 2016 win-utf -rw-r--r-- 1 root root 664 Mar 30 2016 uwsgi_params -rw-r--r-- 1 root root 636 Mar 30 2016 scgi_params -rw-r--r-- 1 root root 180 Mar 30 2016 proxy_params -rw-r--r-- 1 root root 3.9K Mar 30 2016 mime.types -rw-r--r-- 1 root root 2.2K Mar 30 2016 koi-win -rw-r--r-- 1 root root 2.8K Mar 30 2016 koi-utf -rw-r--r-- 1 root root 1007 Mar 30 2016 fastcgi_params -rw-r--r-- 1 root root 1.1K Mar 30 2016 fastcgi.conf drwxr-xr-x 2 root root 4.0K Apr 26 2016 conf.d -rw-r--r-- 1 root root 1.5K Apr 27 2016 nginx.conf drwxr-xr-x 2 root root 4.0K Apr 27 2016 sites-enabled drwxr-xr-x 2 root root 4.0K Aug 15 09:21 sites-available drwxr-xr-x 2 root root 4.0K Aug 15 09:21 snippets The only functionality that I miss among the magics is the ability to take a quick look at part of a possibly very large text file. This can be done with the head command line program, which prints the beginning of a file: In [64]: ! head jupyter_magic.ipynb { \"cells\": [ { \"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [ \"# Make the most of Python Jupyter notebooks\\n\", \"\\n\", \"[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/dlukes/dlukes.github.io/source?filepath=content%2Fnotebooks%2Fjupyter_magic.ipynb)\\n\", \"\\n\", The -n option controls how many lines from the beginning of the file should be printed: In [65]: ! head -n3 jupyter_magic.ipynb regex.ipynb ==> jupyter_magic.ipynb <== { \"cells\": [ { ==> regex.ipynb <== { \"cells\": [ { Similarly, the tail program prints endings of files: In [66]: ! tail -n5 jupyter_magic.ipynb } }, \"nbformat\": 4, \"nbformat_minor\": 2 } Another useful feature of command line execution is that instead of printing the result, you can have it returned as a list of strings corresponding to lines of output. Either by prepending two exclamation marks instead of one: In [67]: !! tail -n5 jupyter_magic.ipynb Out[67]: [' }', ' },', ' \"nbformat\": 4,', ' \"nbformat_minor\": 2', '}'] Or by assigning the expression to a variable: In [68]: out = ! tail -n5 jupyter_magic.ipynb In [69]: out Out[69]: [' }', ' },', ' \"nbformat\": 4,', ' \"nbformat_minor\": 2', '}'] In summary These are just my favorite shortcuts, the ones I find most helpful. Obviously, there are many more, see %magic or %quickref . If you think I've missed a really neat one, let me know!","tags":"floss","url":"jupyter-magic.html"},{"title":"Text munching in R?","text":"R has been gaining traction as a language for data analysis. My feelings about the whole ecosystem are mixed -- it has some incredibly well-designed libraries and a top-of-the-game IDE , but the core language makes me cringe (it feels like \"Perl and Lisp: The Worse Parts\"). Be that as it may, it has undeniably become the go-to programming language for many people for whom programming is not their main breadwinner, many linguists among them. If you're one of these people and wondering whether it's worth undergoing the cognitive burden of learning another language and having to context-switch between them, read on! The main problem with R and large data is of course that R is fast as long as you can load everything into memory at once and use vectorized operations. The whole point of this post is that with the current size of a typical corpus, you often can't do that. You'll have to process the corpus line by line, which means using a for-loop, and these are notoriously slow in R. I'm not pretending this is some new discovery (it's not), I'm just trying to quantify how problematic this slowness is for processing large quantities of text (prohibitive, in my opinion), so that you don't have to figure it out for yourself and can get started learning Python 3 right away instead ;) (Another big problem is that R doesn't have an efficient and versatile hash table data structure.) tl;dr If you're planning to process corpora of hundreds of millions of tokens or more -- spoiler alert, you probably shouldn't do it in R. Task OK, I've specifically complained about R being bad at for-loops and hashes. Let's devise a task that'll show exactly how bad. At the same time, I don't mean to come up with anything particularly convoluted or far-fetched. So here goes: we'll be trying to build a per-part-of-speech frequency distribution of lemmas (dictionary headword forms) from a 120M-token corpus. If you've ever done any linguistic data analysis, I hope you'll agree it's a pretty basic and common task. The corpus data consists of lines with tab-separated word form, lemma and tag fields, plus some additional lines with metadata which do not contain tabs. We'll be skipping those. Here's a glimpse of the corpus format: <!-- WORD LEMMA TAG --> Hladina hladina NNFS1-----A----- jezera jezero NNNS2-----A----- , , Z:-------------- mrtvá mrtvý AAFS1----1A----- a a J&#94;-------------- černá černý AAFS1----1A----- , , Z:-------------- The first character of the tag indicates the part of speech. For each part of speech, we want to create a separate frequency distribution, i.e. we want to be able to say, for instance, the most frequent noun is X, followed by Y, whereas the most frequent adjective is Z etc. This should nicely exercise all of R's weak spots. Let's get to it! R: 1 day (!) / 15 hours (see EDIT below, but still !) After quite some exploration of various alternatives for the individual subtasks, this is the program that I came up with: library ( stringi ) library ( hash ) # read input from STDIN con <- file ( \"stdin\" , open = \"rt\" ) pos_sets <- hash () start <- Sys.time () while ( TRUE ) { line <- readLines ( con , n = 1 ) if ( length ( line ) == 0 ) { break } # lines with tokens (as opposed to metadata) contain tabs if ( stri_detect_fixed ( line , \"\\t\" )) { # individual token attributes are tab-separated attrs <- stri_split_fixed ( line , \"\\t\" ) # for each token, we're interested in the lemma (headword)... lemma <- attrs [[ 1 ]][ 2 ] # ... and the part-of-speech, which is the first character of the tag tag <- attrs [[ 1 ]][ 3 ] pos <- stri_split_boundaries ( tag , n = 2 , type = \"character\" )[[ 1 ]][ 1 ] # build a per-part-of-speech frequency distribution as a nested hash: # { # \"noun\": { # \"cat\": 5, # \"dog\": 3, # ... # }, # \"verb\": { # \"look\": 2, # ... # }, # ... # } if ( is.null ( pos_sets [[ pos ]])) { pos_sets [[ pos ]] <- hash () } if ( is.null ( pos_sets [[ pos ]][[ lemma ]])) { pos_sets [[ pos ]][[ lemma ]] <- 1 } else { pos_sets [[ pos ]][[ lemma ]] <- pos_sets [[ pos ]][[ lemma ]] + 1 } } } # report running time diff <- Sys.time () - start cat ( sprintf ( \"Done in %g %s.\\n\" , diff , units ( diff )), file = stderr ()) Given the input corpus mentioned above, this code takes 1.33 days to run. Compared to other languages one might conceivably use (see below), this is just ridiculous. Now, I'm certainly not an expert in R, so there may be better ways of doing some of this. But I doubt such improvements, if any, would be of any practical relevance, because even reducing the running time to a tenth of the original duration wouldn't be enough. And if there is a way to go even further, say to a hundredth, which would begin to make R competitive, then I would argue that a language which lets you shoot yourself so spectacularly in the foot performance-wise if you're not hip to some clever tricks should just be avoided for tasks where said performance matters. EDIT: Replacing stri_split_boundaries(tag, n=2, type=\"character\")[[1]][1] above with stri_sub(tag, from=1, to=1) , you can cut the running time down to 15 hours. That's still way too much in comparison with the competitors, and just reinforces one of the points made below: there's often no default and efficient way of doing some basic operations (like string manipulation) in R. This is in great part due to R's emphasis on vectorization, which leads to a proliferation of subtly different functions designed for doing subtly different kinds of vectorized passes over data. Good luck trying to remember them all. And if you pick the wrong one (cf. stri_split_boundaries() vs. stri_sub() ) -- because there are just too many similar ways of achieving the same result and too much documentation to read before you even begin to see what you should use -- you get penalized heavily. This is very programmer-unfriendly design. Contrast this with the Zen of Python : \"There should be one -- and preferably only one -- obvious way to do it.\" With these general considerations out of the way, let's look at some details of how to implement this task in R. In many cases, it's unclear how you should even approach the problem in R, due to missing or confusing built-in functionality . As a result, in addition to having a lousy running time on this task, R also puts a strain on the programmer's time. Reading the data This sounds so basic it should be obvious, right? Not so fast. First of all, the file(\"path/to/file\") function creates a file connection, which is however not open unless you also specify a mode in the open= argument, or alternatively, unless you call the open() function on the connection. Why you would want to create a connection that's not open is beyond me, but R adds insult to injury by allowing readLines() to work on a closed connection: it just opens the connection before doing the reading and closes it afterwards. This means that repeated calls to readLines(unopened_connection, n=1) will repeatedly read the first line of the file , which is most likely not what you want. This is API design level PHP. Second, the corpus is gzip compressed, so you'll need to uncompress it. There are basically two options: have an external program ( zcat ) do the decompression and pipe the data into R via STDIN handle the decompression within R itself As a general rule (for any language), it will always be faster to handle the decompression in a different process on a multi-core system, because the tasks can proceed in parallel. 1 On the other hand, it's more portable not to depend on external programs, and R does have a built-in function to open a connection to a gzipped file, namely gzfile() . Based on tests on shorter inputs, it's about 50% slower than external decompression, which is a somewhat worse performance deterioration than e.g. Python 3 (40% based on the full input). In light of the already dire running time, it's something we can't really afford. Third, having to do the line-by-line reading in a while (TRUE) loop, using a function called readLines() (note the plural) with an argument of 1 , checking the length of the resulting character vector in order to determine the end of the input -- that's just gross. String manipulation R has built-in functions for string matching ( grepl() et al.), not so much for string splitting. This is the point where I got suspicious of the performance of everything and started testing alternatives. I finally ended up using the stringi package, which is fast and has a fairly consistent API. stringr is a set of higher-level wrappers around it, which have however proven somewhat slower than the built-ins in my highly informal testing. O hash map, where art thou? Building a per-part-of-speech frequency distribution of headwords requires an appropriate data structure. As indicated in the comments in the R source, we want to build a nested collection that looks something like this: { \"noun\": { \"cat\": 5, \"dog\": 3, ... }, \"verb\": { \"look\": 2, ... }, ... } The requirements on the data structure we need are the following: it's a collection strings can be used as keys it can be arbitrarily nested key lookup is fast, i.e. constant time In other words, we need a hash (or a dict, in Python terminology). R doesn't have a hash (I'll qualify this statement in a bit). The workhorse data structure in R that satisfies points 1--3 is a list. Unfortunately, it has linear access time . That's not going to work. R also has environments, which it uses to store and access variables. Under the hood, environments are implemented as hashes, but using them as such is a massive pain, because their API isn't meant for it. Fortunately, there's a wrapper package which makes it more convenient. Unfortunately, environments weren't optimized with this use case in mind. They were designed to hold key--value (variable name--variable value) pairs explicitly defined by people as part of their programs, not millions of items extracted from data. As a result, they slow down dramatically once the number of items grows large. (The article in the previous link provides a survey of the state of the art of fast key lookup in R. The state of the art is... dismal. Your only option is basically indexing a data table, which is fine for a finalized data set, but useless when building the data set -- you can't afford to reindex after each new data point.) There's also the hashmap library , which is a wrapper around C++ Boost hashes. However, it doesn't do nesting, so it's of no use to us, and of very limited usefulness in general. Conclusion: technically, we have to concede that R has hashes, but for all practical intents and purposes, it doesn't . There's one last twist, though. Funnily enough, in our use case, it turns out it doesn't really matter anyway. Indeed, it seems the performance of for-loops in R is so egregiously bad that it dwarfs even the inefficiencies accrued by the linear lookup time of lists: if you reimplement the script with lists, it takes just a little longer than the version with hashes, about 1.36 days. (Or maybe it's just that the performance of environment-based hashes becomes so bad when they grow large as to be comparable with that of lists? Who knows, and frankly, I don't care enough to want to find out. If it's the for-loops though, then adding efficient hashes to R won't really solve anything.) EDIT: With stri_sub() substituted for stri_split_boundaries() as detailed above, the code using lists runs in about 1.28 days, which is a much smaller improvement than in the case of the code using hashes (1.33 days → 15 hours). Summary If you like R and your reaction to this is, \"That's not fair! R was never meant to do any of this, that's why everything feels so backhanded.\" -- then good, that's basically the gist of this post: don't use R for something it wasn't meant to do . What are the alternatives, then? Task The following details an informal test comparing the speed of R, Python 3, Rust and Perl at processing a large corpus file (~120M tokens, 1.5GB gzipped) and creating a frequency distribution of headwords per part-of-speech. The idea is to see whether R is a viable alternative in this domain, or whether the slowing down caused by the inability to use vectorized computations (because we can't load the entire thing into memory at once) will just be too much. Python 3: 5 minutes Yes, that's right. It takes Python 3 5 minutes to do the same task that took R over a day . The code feels a lot simpler too: import sys import time def main (): pos_sets = {} start = time . time () for line in sys . stdin : if \" \\t \" in line : _ , lemma , tag , _ = line . split ( \" \\t \" , maxsplit = 3 ) pos = tag [ 0 ] # this is an intentionally naive implementation which mimicks # the R code and something an inexperienced coder might do; # a more concise and probably better performing solution could # be achieved using dict.setdefault() or collections.defaultdict # / collections.Counter if pos not in pos_sets : pos_sets [ pos ] = {} if lemma not in pos_sets [ pos ]: pos_sets [ pos ][ lemma ] = 1 else : pos_sets [ pos ][ lemma ] += 1 diff = time . time () - start print ( f \"Done in {diff:.0f} seconds.\" , file = sys . stderr ) if __name__ == \"__main__\" : main () FYI, this was run using Python 3.6. As a rule, use always the most recent version of Python 3 you can (at least 3.5, 3.4 in a pinch; with earlier releases, you may encounter performance issues). In any case, do not use Python 2 for new projects and let it end-of-life in peace. Perl: 13 minutes Perl used to be a popular alternative for text processing. Like R, it has its fair share of nauseating language design and weird quirks, but since it was actually meant for use in this domain, it won't spectacularly let you down. (Unless your data is silently corrupted because you handled text encoding wrong. Perl's behavior in this respect is a relict of a pre-UTF-8-everywhere past, and it's the single biggest reason for why the language should be put out of its misery already.) Here's the code: use strict ; use utf8 ; use open qw(:std :encoding(utf8)) ; my $start = time (); my %pos_sets = (); while ( <> ) { if ( /\\t/ ) { my @attrs = split /\\t/ ; my $lemma = @attrs [ 1 ]; my $tag = @attrs [ 2 ]; my $pos = substr $tag , 0 , 1 ; # auto-vivification: ergonomic, but also made possible by the whole # \"implicit defaults that have a potential of screwing stuff up # without you even knowing about it\" culture of Perl $pos_sets { $pos }{ $lemma } += 1 ; } } my $diff = time () - $start ; print STDERR \"Done in $diff seconds.\\n\" ; Bottom line though, being more than twice as slow as Python 3 (which came as a surprise to me, I must admit) and definitely the worse language, it has little to recommend itself if you're considering to learn a new language for this type of task. Except maybe if you want to continuously log what the program is doing to a terminal -- like output the number of lines processed after each line. Perl is clearly very efficient at writing to a terminal, the running time is basically the same with continuous logging incorporated. By contrast, Python 3 takes about three times longer (~ 15 minutes). (I guess maybe Python flushes output after each print() call, whereas Perl does some smart buffering which results in it not being slowed down by the latency of the terminal...? Who knows, at any rate, it's hardly a \"killer\" feature.) Rust: 1.25 minutes As a compiled, systems-level language, Rust is in a different league compared to the previous contestants: of course it's going to be faster. I included it because it provides a frame of reference. The important takeaway is that we're in the same ballpark with Python 3 (roughly units of minutes), so there's no pressing need to turn to a compiled language for this task. Here's the code, for completeness sake: use std :: io ; use std :: io :: prelude :: * ; use std :: collections :: HashMap ; use std :: time ; type LemmaCount = HashMap < String , i32 > ; type PosSet = HashMap < char , LemmaCount > ; fn main () { let start = time :: SystemTime :: now (); let mut pos_sets = PosSet :: new (); let stdin = io :: stdin (); for line in stdin . lock (). lines () { let line = line . unwrap (); if line . contains ( \" \\t \" ) { let mut attrs = line . split ( \" \\t \" ). skip ( 1 ). take ( 2 ); let lemma = attrs . next (). unwrap (); let tag = attrs . next (). unwrap (); let pos = tag . chars (). take ( 1 ). next (). unwrap (); let pos_set = pos_sets . entry ( pos ). or_insert ( LemmaCount :: new ()); let count_for_lemma = pos_set . entry ( String :: from ( lemma )). or_insert ( 0 ); * count_for_lemma += 1 ; } } let diff = start . elapsed (). unwrap (). as_secs (); println ! ( \"Done in {:.0} seconds.\" , diff ); } Note in passing how nicely the Rust code reads for a compiled language. Of course, since it's a much stricter (and safer) language than Python, it's more ceremonious to write and the APIs are more complicated, because they have to adhere to the various memory management guarantees Rust gives you (among other things). But once the code is written, it's very readable and clear. And all necessary functions and data structures are (a) available in the standard library, and (b) plenty efficient. Conclusion Just to be clear: the ultimate purpose of this post is not bashing R (not for being slow at text munching, at any rate); it's to give a convincing account of why it's just not the right tool for the job. And not in a small way, either -- in a way that requires to learn a different tool, there's no way around it. Let me reiterate that my recommendation would hands down be Python 3 . Once the data is extracted, go back to R by all means. Although Python does have a fairly nice high-level data analysis library , it's not my intention to discourage anyone from using R for what it is good at, especially if this is a skill they are already proficient in. The internet is full of people asking advice on which programming language to learn, and the answers are invariably evasive -- it depends on your tastes, what fits your brain better, what your use case is. In the hopes that some people might find opinionated guidance useful for a change (I know I personally often do, when flirting with a new language): if you're looking to process large quantities of text data, the answer is a big, resounding NOT R ! A vectorized postscript Since I ran these on a server with 64 GB of RAM, I figured I might as well try loading everything into memory in R and doing it the proper, vectorized way, while I'm at it. Here's the code, using dplyr : library ( dplyr ) library ( stringi ) start <- Sys.time () con <- file ( \"stdin\" , open = \"rt\" ) corpus <- readLines ( con ) diff <- Sys.time () - start cat ( sprintf ( \"Corpus read in after %g %s.\\n\" , diff , units ( diff ))) corpus <- stri_subset_fixed ( corpus , \"\\t\" ) corpus <- stri_split_fixed ( corpus , \"\\t\" , simplify = TRUE ) freq_dist <- tibble ( POS = stri_sub ( corpus [, 3 ], from = 1 , to = 1 ), LEMMA = corpus [, 2 ] ) %>% group_by ( POS , LEMMA ) %>% summarize ( FREQ = n ()) diff <- Sys.time () - start cat ( sprintf ( \"Finished processing corpus after %g %s.\\n\" , diff , units ( diff ))) Let me say at the outset that this code looks much nicer -- it's clean, modern R, made possible in great part by Hadley Wickham's efforts to redesign the data manipulation vocabulary from the ground up. Note also that we've made a concession on our requirements: the resulting data structure is a tibble, not a hash, i.e. key lookup time is not constant but depends on the size of the data. Well, just loading the corpus into memory took ~18 minutes. The script then ran for several days , in the course of which I checked every now and then to see how much memory it was using: ~35 GB. I don't suppose anyone has that much RAM on their laptop. Then someone rebooted the server before the program could complete. I think you'll agree the experiment is conclusive even so. You could also offload the decompression to a different thread in the same process, but that complicates the implementation. Piping gives you parallelization basically for free. ↩","tags":"ling","url":"text-munching-in-r.html"},{"title":"Monkey-patching in R","text":"While building a Shiny application with R recently, I've come across the need to invert the filterRange() function in the DT package, which provides a convenient high-level way to add DataTables to your Shiny app. As indicated by its name, this function filters a numeric column in your datatable based on a range, so as it contains only values contained within that range . What I needed was the opposite: include values outside the specified range . The filtering is done server-side and unfortunately, no option is provided out-of-the-box to perform this inversion. One of the solutions is therefore to monkey-patch the filterRange() function in the DT R package, replacing it with a version that filters the outer range instead. Googling for \"monkey patching r\" (currently) yields this blog post , which provides a more complicated though arguably cleaner solution, which introduces a new environment in the search path. My position on this is that if you're worried about cleanliness, you shouldn't be monkey-patching in the first place. Conversely, if you decide monkey-patching is acceptable in your situation, the code required should be as quick and dirty as the thought. Of course, this is R, uncontested king of weird ways of doing anything but the most common data analysis tasks, and even some of those -- so it's never going to be as simple as Python, for instance: import sys sys . stdin = \"foo\" # Aaand done. But it doesn't have to be as complicated as the solution in the blog post above, either. The solution presented here is basically taken from this mailing list post , which has the disadvantage of not containing the key term \"monkey-patch\", which makes it hard to find on Google. It consists in the following steps: Get a handle on the relevant library's namespace with getNamespace() . Make the relevant binding modifiable with unlockBinding() . Define your custom version of the function. Store it in the namespace under the original name. Re-seal everything with lockBinding() . Here's the code for my specific use case with DT::filterRange() : # Monkey patch the filterRange() function in the DT package so that server-side filtering returns # values *outside* the range instead of inside. DT <- getNamespace ( \"DT\" ) unlockBinding ( \"filterRange\" , DT ) #################################################################################################### # This part of the code is deliberately kept as similar to the original as possible, in order to # make potential updates easier. See https://github.com/rstudio/DT/blob/v0.2/R/shiny.R#L474. # filter a numeric/date/time vector using the search string \"lower ... upper\" filterRange = function ( d , string ) { if ( ! grepl ( '[.]{3}' , string ) || length ( r <- strsplit ( string , '[.]{3}' )[[ 1 ]]) > 2 ) stop ( 'The range of a numeric / date / time column must be of length 2' ) if ( length ( r ) == 1 ) r = c ( r , '' ) # lower, r = gsub ( '&#94;\\\\s+|\\\\s+$' , '' , r ) r1 = r [ 1 ]; r2 = r [ 2 ] if ( is.numeric ( d )) { r1 = as.numeric ( r1 ); r2 = as.numeric ( r2 ) } else if ( inherits ( d , 'Date' )) { if ( r1 != '' ) r1 = as.Date ( r1 ) if ( r2 != '' ) r2 = as.Date ( r2 ) } else { if ( r1 != '' ) r1 = as.POSIXct ( r1 , tz = 'GMT' , '%Y-%m-%dT%H:%M:%S' ) if ( r2 != '' ) r2 = as.POSIXct ( r2 , tz = 'GMT' , '%Y-%m-%dT%H:%M:%S' ) } if ( r [ 1 ] == '' ) return ( d <= r2 ) if ( r [ 2 ] == '' ) return ( d >= r1 ) d <= r1 | d >= r2 } # End pastiche of original DT code. #################################################################################################### DT $ filterRange <- filterRange lockBinding ( \"filterRange\" , DT ) The last piece of the puzzle concerns UX: the user should understand that the filter applies to the outer range, not the inner one. Visually: This is easily achieved with a few lines of CSS: # datatable-id . noUi-background { background : #3FB8AF ; box-shadow : inset 0 0 3 px rgba ( 51 , 51 , 51 , .45 ); transition : background 450 ms ; } # datatable-id . noUi-connect { background : #FAFAFA ; box-shadow : inset 0 1 px 1 px #f0f0f0 ; } In conclusion, monkey-patching is rarely the most elegant, debuggable and maintainable solution to a problem you're having. More often, it's actually the least elegant (etc.) one. But every once in a while, it's the simplest one, the one with the best hassle/reward ratio (until it comes back to bite you once your codebase has grown or assumptions about the monkey-patched code have changed). At any rate, if you need to resort to it, it's nice to have a quick, googlable how-to, hence this post.","tags":"floss","url":"monkey-patching-in-r.html"},{"title":"\"Responsive\" iframes, e.g. for DokuWiki and Shiny","text":"Sometimes, the best way to embed an interactive element into a website is to use an iframe. Obviously, not when your website is a webapp and that element represents the main functionality it's supposed to provide -- that would be gross. But when your website is mostly textual / graphical content, typically authored within a wiki or blogging platform, and you just want to include this one element to liven it up, iframes are actually a decent (and perhaps the only?) solution. Trouble is, you probably want this Frankenstein monster to actually look good, i.e. seamless if at all possible. But iframes don't have automatic vertical resizing according to their content, which means you'll need to take care of that manually. How? By using the JavaScript messaging API for communication between parent and child frames to send information about window resize events (from parent to child) and height updates (from child to parent). Let's imagine you have a DokuWiki article in which you want to embed a small Shiny app. If you just embed it in your dokuwiki code using an iframe, taking care to remove the border and stretch it horizontally... < html > < iframe id = \"embedded-app\" src = \"https://your.shiny.app/url\" frameborder = \"0\" width = \"100%\" ></ iframe > </ html > ... this will happen: Eww, scrollbar. Messaging to the rescue! First of all, you need to teach your embedded web page to send information about its height to the parent at appropriate times. This can be achieved by adding this piece of JavaScript to it: ( function () { //////////////////////////////////////////// // CONFIGURE THESE TO MATCH YOUR USE CASE // //////////////////////////////////////////// // set this to a selector for the element that contains the entire UI // you want to access via the iframe -- for a Shiny app, it might be // a div with Bootstrap's container-fluid class var containerSelector = \".container-fluid\" ; // this should be the root URL of the parent frame (DokuWiki) which you want // to allow to send messages to the child var allowedOrigin = \"https://dokuwiki.example.com\" /////////////////////// // END CONFIGURATION // /////////////////////// function sendHeightOf ( querySelector ) { var container = document . querySelector ( querySelector ); if ( container . scrollHeight !== undefined ) { var h = container . scrollHeight ; parent . postMessage ( h , \"*\" ); } else { console . log ( \"No element corresponding to querySelector \" + querySelector + \" found, or element did not have property scrollHeight.\" ); } } // cross-browser compatible infrastructure var eventMethod = window . addEventListener ? \"addEventListener\" : \"attachEvent\" ; var eventer = window [ eventMethod ]; var messageEvent = eventMethod == \"attachEvent\" ? \"onmessage\" : \"message\" ; // listen for resize message from parent window (see point ② below) eventer ( messageEvent , function ( e ) { if ( e . origin == allowedOrigin ) { sendHeightOf ( containerSelector ); } else { console . log ( \"Was expecting a message from \" + allowedOrigin + \", got \" + e . origin + \" instead.\" ); } }); window . onload = function () { // inform parent at least once after load (see point ① below) sendHeightOf ( containerSelector ); // monitor self-initiated changes in size (see point ③ below) var mo = new MutationObserver ( function () { sendHeightOf ( containerSelector ); }); mo . observe ( document , { subtree : true , childList : true , characterData : true }); }; })(); What are these \"appropriate times\" mentioned above? The code above implements the following ones, which should be generic enough to cover most situations: on initial page load on window resize (see below, the parent frame has to send a message to the child frame that it has been resized, to which the child responds with a size update message) on any kind of mutation of the DOM inside the child frame (not a full reload of the entire page, that would be handled by point ① above), which might affect the size of the rendered component On the parent (DokuWiki) side, you then need to handle the incoming size update messages from the child frame, and send resize messages when the window is resized. This can be achieved with the following DokuWiki markup: < html > < iframe id = \"embedded-app\" src = \"https://your.shiny.app/url\" frameborder = \"0\" width = \"100%\" ></ iframe > < script > ( function () { //////////////////////////////////////////// // CONFIGURE THESE TO MATCH YOUR USE CASE // //////////////////////////////////////////// // this should be the root URL of the child frame (Shiny app) which you want // to allow to send messages to the parent var allowedOrigin = \"https://your.shiny.app\" /////////////////////// // END CONFIGURATION // /////////////////////// var embeddedApp = document . getElementById ( \"embedded-app\" ); function resizeIframe ( pixels ) { embeddedApp . style . height = pixels + \"px\" ; } // cross-browser compatible infrastructure var eventMethod = window . addEventListener ? \"addEventListener\" : \"attachEvent\" ; var eventer = window [ eventMethod ]; var messageEvent = eventMethod == \"attachEvent\" ? \"onmessage\" : \"message\" ; // listen to message from iframe eventer ( messageEvent , function ( e ) { if ( e . origin === allowedOrigin ) { var key = e . message ? \"message\" : \"data\" ; var data = e [ key ]; resizeIframe ( data ); } else { console . log ( \"Was expecting a message from \" + allowedOrigin + \", got \" + e . origin + \" instead.\" ); } }, false ); // send message to iframe on window resize window . onresize = function () { embeddedApp . contentWindow . postMessage ( \"parentWindowResized\" , \"*\" ); }; })(); </ script > </ html > And the result? Yay! And of course, the iframe gets resized as needed when display conditions change: Cue bittersweet feeling after having figured out a workaround for such a specific use case that you're not quite sure it was worth putting all that effort into it in the first place...","tags":"floss","url":"responsive-iframe.html"},{"title":"The Cathedral and the Bazaar: What is a Useful Notion of \"Language\"?","text":"If you like the essay, then you'll definitely want to take a look at Luc Steels's The Talking Heads Experiment: Origins of Words and Meanings . It's published as an open-access book by Language Science Press, so go grab the free download ! Abstract The essay analyzes why Noam Chomsky's notion of language (both its essence — language as a set of grammatical sentences — and genesis) leads neither to interesting discoveries nor even to useful questions from the point of view of linguistics as a science. A much more fruitful approach to language is to view it as a complex, dynamic, distributed system with emergent properties stemming from its functions, as advocated e.g. by Luc Steels. The argument will be developed against the backdrop of the evolution of Ludwig Wittgenstein's thought, from the Tractatus to the concept of language games, i.e. from an approach to language based on thorough formal analysis but also misconceptions about its functions, to a much keener though less formal grasp of its praxis and purpose. Introduction At least since Thomas Kuhn's The Structure of Scientific Revolutions , it has been a fairly commonplace notion that working within the confines of a particular scientific paradigm conditions to a certain extent the questions one is likely to ask and therefore also the answers that ensue. This effectively limits the range of possible discoveries, because some are not answers to meaningful questions within a given framework while other observations still are taken as given axioms, which means they cannot be the target of further scientific investigation. In contemporary linguistics, one very prominent such paradigm is that of generative grammar , single-handedly established in 1957 by Noam Chomsky in his seminal work Syntactic Structures . While serious criticism has been leveled over time against this initial exposition as well as Chomsky's subsequent elaborations on it (see Pullum 2011; Sampson 2015; and Sampson 2005 for a book-length treatment), the book undeniably attracted significant numbers of brilliant young minds under the wings of its research program, which went from aspiring challenger in the domain of linguistics to established heavyweight in a comparatively short period of time (the transition had been achieved by the mid-1970s at the latest). In the process, it co-opted or spawned various other sub-fields of linguistics, and even rebranded itself, such that Cartesian linguistics , cognitive linguistics and most recently biolinguistics are all labels which suggest a strong generativist presence. One serious competitor to the Chomskyan account of language that has emerged over the years is the field of evolutionary linguistics . It might seem strange at first glance why biolinguistics and evolutionary linguistics should be at odds. As their names indicate, they both aspire to a close relationship with biology, which seems to indicate their research agendas and outlooks should largely overlap. Yet their fundamental assumptions about what constitutes language are so irreconcilable that they might as well be considered to deal with different objects of study. Of the two, it is evolutionary linguistics which leads to questions and investigations which can be conceived of as scientific (in the Popperian sense of involving falsifiable hypotheses instead of being merely speculative), consequently yielding the most useful insights – in the fairly pedestrian sense that these can be intersubjectively replicated without resorting to an argument from authority, which makes them a better foundation to build upon, because the superadded structures are less likely to crumble should said authority ever change their mind, as Chomsky has done several times already. Wittgenstein on language: From logical calculus to language games Let us now take a short détour through the development of Ludwig Wittgenstein's thoughts on language, so that we may couch our later discussion of the differences between generative grammar / biolinguistics and evolutionary linguistics in terms of a contrast that is perhaps more familiar. The imagery in the title of the present essay was borrowed from Eric S. Raymond's book The Cathedral & the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary (Raymond 1999). In it, Raymond describes two models of collaborative software development, one of them very rigid, restrictive and hostile to newcomers (the \"cathedral\"), the other overwhelmingly inclusive, open to outside contributions and organic change, an effervescent hive of activity (the \"bazaar\"), whose unexpected but empirically demonstrable virtues he has come to embrace. This architectural metaphor also happens to be very apt when characterizing Wittgenstein's view of language in the two major stages of his thought, as represented by his two books Tractatus Logico-Philosophicus and Philosophical Investigations . In the Tractatus , Wittgenstein has a \"preconceived idea of language as an exact calculus operated according to precise rules\" (McGinn 2006, 12) and formalizing this system of rules leads him to the following dogmatic conclusion: \"what can be said at all can be said clearly, and what we cannot talk about we must pass over in silence\" (Wittgenstein 2001, 3). The deontic force of the final injunction should be taken with a grain of salt; it could perhaps be rephrased in the following less epigraph-worthy manner: there is a sharp logical boundary to be drawn between meaningful and nonsensical propositions, and the purpose of language is to construct meaningful ones, therefore it is futile (rather than strictly forbidden) to engage in nonsensical ones. There have been attempts to read the Tractatus in an ironic mode, as a consciously doomed, self-defeating attempt to circumscribe the limits to the expression of thought, which prefigures the much more subtle attitude towards language that Wittgenstein later exhibits in the Philosophical Investigations (see McGinn 2006, 5–6 and elsewhere for an overview of this so-called \"resolute\" reading). In my opinion, such a stance exhibits a blatant, possibly wilful disregard of his almost penitent tone in the preface to Philosophical Investigations : \"I could not but recognize grave mistakes in what I set out in that first book\" (Wittgenstein 2009, 4e). Where does Wittgenstein think he went wrong then? Arguably, the most serious misconception was conferring a privileged ontological status to language, seeing it as \"the unique correlate, picture, of the world\" (Wittgenstein 2009, 49e), whereas in fact, these referential properties are highly dependent on communicative context. It signifies only insofar as it has an effect on the addressee (another human being, or even myself) which to all practical intents and purposes the speaker can identify as somehow related to what she was trying to achieve with her utterance in the first place. But there is little meaning, in any practical sense, outside these highly particular, localized (in both physical and cultural space and time), embodied, grounded interactions. Wittgenstein calls these interactions \"language-games\" to \"emphasize the fact that the speaking of language is part of an activity, or of a form of life\" (Wittgenstein 2009, 15e, original emphasis). Of course, the notion of game still involves some kind of rules, but by focusing on the activity rather than its regulations, it is much easier to account for \"the case where we play, and make up the rules as we go along […] and even where we alter them – as we go along\" (Wittgenstein 2009, 44e). This is not to say that we cannot construct abstractions, although Wittgenstein himself is clearly in favor of systematically examining particular cases: \"In order to see more clearly, here as in countless similar cases, we must look at what really happens in detail , as it were from close up\" (Wittgenstein 2009, 30e). However, once we do abstract away, it is crucial to approach the resulting theory from a pragmatic standpoint: \"We want to establish an order in our knowledge of language: an order for a particular purpose, one out of many possible orders, not the order\" (Wittgenstein 2009, 56e, original emphasis). Generative grammar In many ways, Chomsky conceives of language as the early Wittgenstein did, i.e. under the cathedral metaphor. This may come across as a surprise because unlike Wittgenstein, he is not concerned with issues of meaningfulness, philosophical or otherwise. Indeed, it is one of his fundamental precepts that grammar can and should be dissociated from meaning, as demonstrated by his famous example sentence \"Colorless green ideas sleep furiously\", which he claims is perfectly grammatical yet meaningless 1 (Chomsky 2002, Chap. 2). Nevertheless, the goal for both is to describe language per se , in the abstract, without regard to its context-grounded use in actual communication. Both strive to give a highly formal definition of the system they think they are uncovering: while Wittgenstein attempts to establish a logical calculus of how propositions can be said to carry meaning in terms of their referential relationship to external reality, Chomsky tries to hint at a calculus which would determine which candidate sentences belong to the language encoded by this calculus, i.e. separate those that are grammatical from those that are not. Another way to put this is that the descriptive part of linguistics can be equated with formal language theory: a language is viewed as a (potentially infinite) set of symbol strings (sentences), and the linguist's task is to find the simplest and most elegant set of rules that would constitute the basis for a procedure to generate (hence generative grammar) all of them and only those, whether observed or potential. Chomsky himself gives the following definition: \"by a generative grammar I mean simply a system of rules that in some explicit and well-defined way assigns structural descriptions to sentences\" (Chomsky 1965, 8). Furthermore, \"Linguistic theory is concerned primarily with an ideal speaker-listener, in a completely homogeneous speech-community, who knows its language perfectly\" (Chomsky 1965, 3). Specifically, it is concerned with his \" competence (the speaker-hearer's [intrinsic] knowledge of his language)\", not his \" performance (the actual use of language in concrete situations)\" (Chomsky 1965, 4). Additionally, no claim is made as to the cognitive or neurophysiological accuracy of the mechanisms described, although to hedge his bets both ways, Chomsky adds that \"No doubt, a reasonable model of language use will incorporate, as a basic component, the generative grammar that expresses the speaker-hearer's knowledge of the language\" (Chomsky 1965, 9). In short, Chomsky consciously sets up the playing field for a thoroughly mentalistic, speculative discipline. At first, it might seem like reasonable approximation and a small concession to make, especially in the face of the sheer daunting complexity of all the intricate mechanisms that conspire to yield the phenomenon we call language, but only until one fully realizes the consequences of such a move. Observe for instance the carefully crafted loophole claiming that linguistics is primarily concerned with an ideal speaker-hearer's competence and that actual usage data is just circumstantial evidence. This effectively allows linguists to dismiss inconvenient edge cases or counterexamples to their theories purely on grounds of their being noisy data or slips of the tongue, which is something they are allowed to determine based on introspection. Mind you, this is not just a theoretical loophole; Chomsky himself has repeatedly relied on it, especially with respect to so-called linguistic universals (see e.g. Sampson 2005, 139 or 160). The net result is rampant, unchecked theorizing. One such example is the postulation of a two-layer linguistic analysis, the observed language data corresponding to a surface structure which provides hints as to an underlying, more regular deep structure to be uncovered. The deep structure is purportedly closer to the universal properties of language; both layers are linked by a system of transformations: We can greatly simplify the description of English and gain new and important insight into its formal structure if we limit the direct description in terms of phrase structure to a kernel of basic sentences (simple, declarative, active, with no complex verb or noun phrases), deriving all other sentences from these (more properly, from the strings that underlie them) by transformation, possibly repeated. (Chomsky 2002, 106–7) Constructing a formal framework for grammar modeling with cognitively unmotivated levels of abstraction might have been a valid goal (though arguably not within linguistics) if the result was indeed, as Chomsky claims it to be, maximally elegant, as simple as can be but no simpler. I was not able to track down a formal definition of this criterion, but simplicity is clearly discursively construed as a desirable quality: \"simple and revealing\" (Chomsky 2002, 11) or \"effective and illuminating\" (Chomsky 2002, 13) are Chomsky's choice epithets for what to look for in a grammar. But that is not true either: transformations are an unnecessary addition, singling them out as a separate category of operations adds nothing to the generative power of his system (Pullum 2011, 290). They are therefore a wart under any reasonable definition of \"simplicity\" and Chomsky thus manages to fall short of even the self-defined, theory-internal standards that are the only ones he allows his enterprise to be held to. Ontogeny and phylogeny As we have seen, generative grammar concerns itself with an ideal speaker-hearer's competence in a perfectly homogeneous community. The trouble is that such an impoverished model eschews any possibility of dynamism. The very dichotomy between grammatical and ungrammatical is intuitively problematic if we consider that judgments are bound to diverge when made in reference to different dialects, sociolects and idiolects, 2 not to mention that binary classification might be too reductive in some cases (how would you categorize, on first encounter, a construction which you passively understand but would never produce actively?). Though Chomsky sometimes mentions in passing the possibility of levels of grammaticalness which would allow a finer-grained analysis (e.g. Chomsky 2002, 16; Chomsky 1965, 11), it seems to be just another instance of hedging his bets, he never makes it a fundamental component of his theory. This would seem to indicate that Chomsky's theory of language has a serious problem in that it is unable to account for any phenomena that involve fluctuations in linguistic ability, including language emergence / diachronic change (phylogeny) and acquisition (ontogeny). Chomsky's response to this is that our language faculty is largely innate: we are genetically endowed with a language-acquisition device in our brains (Chomsky 1965, 31–33) which can supposedly infer the correct grammatical rules even given incomplete, limited and noisy input, which is what Chomsky argues children get (the \"poverty of stimulus\" argument), thanks to strong universal constraints on what a human language can be like. Under this account, the capacity for language, initially \"a language of thought, later externalized and used in many ways\" (Chomsky 2007, 24), appeared as a random mutation in a single individual and progressively spread through the population because it offered a considerable competitive advantage: \"capacities for complex thought, planning, interpretation\" (Chomsky 2007, 22). In his later career, Chomsky increasingly focused on exploring this purported shared genetic basis for human language, hence the aforementioned label \"biolinguistics\". Make no mistake, this in no way entails a turn from mentalism towards empirical neurophysiological or genetic investigation. Quite to the contrary, liberated from the constraints of having to account for individual existing languages in detail, he soars to new heights of abstractness in postulating the formal language underpinnings of human language. The principles and parameters model of Universal Grammar (Chomsky 1986) expands upon the notion of linguistic universals by splitting them up into two sets: principles, which are hardwired and immutable, and parameters, which are hardwired too but can be flipped on or off based on linguistic behavior observed by the child in her particular language community. Since the choices are heavily constrained, the learner can infer correct parameter settings in spite of deficient input. This line of research culminates in the so-called minimalist program (Chomsky 1995), where Chomsky identifies the \"core principle of language, [the operation of] unbounded Merge\" (Chomsky 2007, 22). Under the \"strong\" minimalist hypothesis, this would be the only principle necessary to account for human-like languages (Chomsky 2007, 20), which would paradoxically essentially discard all work (or should I say speculation?) previously done on the parameters side of the Universal Grammar project. All other universal characteristics of language could then be explained by newly introduced \"interface\" conditions, 3 i.e. constraints on how language inter-operates with other systems, including thought and physical language production (Chomsky 2007, 14); 4 all empirically documented variation between the world's languages would be chalked up to lexical differences (Chomsky 2007, 25). The poverty of stimulus and language universals arguments for innateness have been thoroughly debunked, especially in Geoffrey Sampson's book-length diatribe The ‘ Language Instinct ' Debate . In short, it turns out that some of the grammatical constructions which were assumed to be absent from a language learner's input yet acquired nonetheless have since been empirically proven to occur fairly commonly (Sampson 2005, 72–79). Moreover, there is no qualitative difference between a statement like \"the stimulus is too poor to allow language learning without a genetic basis\" and \"the stimulus is just rich enough etc.\", both are unverifiable unless we have already independently proven that language learning occurs one way or the other, so to adduce either of the statements as proof for the hypothesis at stake is misguided (Sampson 2005, 47–48). Finally, the alleged language universals turn out to be either false when checked against additional languages (Sampson 2005, 138–9) or so general as to be meaningless (Sampson 2005, Chap. 5). Irrespective of this, let us suppose for a moment that genetic mutation and subsequent inheritance do play a role in the emergence of language, and work out an account of language emergence consistent with this hypothesis. If language started out through mutation in a single individual as a purely internal advanced conceptualization faculty, then once it started to spread, what was the motivation for the genetically-endowed humans to externalize their thoughts? How did they know to which of their peers they could speak (which had inherited the mutation) and which not? And most importantly, how did they know which parameters of Universal Grammar to flip on and which off, if there was no prior language based on which to decide? Universal Grammar would have had to be fairly detailed in order for intersubjective agreement on the norms for the first ever human language to be reached on the basis of it alone. Yet as we have seen, Chomsky has been moving away from this notion – at the limit, the minimalist program posits only one very general mechanism required for language. The poverty of stimulus argument is turned against its creator as the argument from poverty of the machinery supposed to make up for the poverty of said stimulus. Alternatively, if we fully subscribe to the minimalist program and the notion that all the surface variety exhibited by language comes from the lexicon, then how are individual words created, how do they propagate? One might be tempted to say \"people just invented them\", but consider for a while that in the current setup, there is absolutely no mechanism that would explain how a community of speakers reaches agreement on their lexicon – this theory offers no incentive whatsoever for consensus to be reached; from its point of view, a solution where each speaker ends up with their own private lexicon is equally valid because indistinguishable on the basis of the theory's conceptual apparatus. Chomsky's ideas on phylogenesis appear thoroughly ridiculous when fully carried out to their logical consequences, and this can all be blamed on his sterile, idealized and static view of language which dismisses actual communication as a secondary purpose and therefore a peripheral issue. On a side note, it is hard to say which aspect of Chomsky's theory of language came first – whether innateness accommodated the mentalism and the concomitant quest for formal purity (botched as it may be) of generative grammar, whether it was the other way round, or whether they perhaps co-evolved in his mind. The facts are that Chomsky's initial publications on generative grammar concentrate on the formal language theory part (Chomsky 1956; Chomsky 2002), but he added the innateness argument fairly early on, even tacking a seemingly respectable philosophical lineage onto it in Cartesian Linguistics (Chomsky 2009), which pretends to trace back both innateness and mentalism to Descartes and the Port-Royal grammarians, binding them as two sides of the same coin. It is worth noting that in both formal language theory and history of linguistics / philosophy, Chomsky is more of a dabbler than an expert: he has provably borrowed most of his ideas in the former field from others, sometimes mangling them or extending them in unfortunate ways (Pullum 2011; Sampson 2015), and has thoroughly underresearched (or wilfully twisted?) his understanding of the latter, which has resulted in serious misrepresentations of the history of ideas (Miel 1969; Aarsleff 1970). Evolutionary linguistics There are various sub-fields of linguistics which are in discord with generative grammar, especially over the notion that performance data should be used only as evidence for guiding the speculation and detailed usage and frequency patterns should be disregarded; the primacy of syntax (as advocated by Chomsky) is also disputed. One of these sub-fields is obviously corpus linguistics, which takes a decidedly empiricist stance and starts by assembling a large body of language data (a corpus) from which patterns of language use are inferred. Nevertheless, not all of these compete with generative grammar at the fundamental explanatory level of how language came about phylogenetically and how it is transmitted by ontogenetic acquisition. We have repeatedly encountered Chomsky's emphasis on how communication, actual interactions between speakers, are just an afterthought in the system of language: evolutionary biologist Salvador Luria was the most forceful advocate of the view that communicative needs would not have provided \"any great selective pressure to produce a system such as language,\" with its crucial relation to \"development of abstract or productive thinking.\" His fellow Nobel laureate François Jacob (1977) added later that \"the role of language as a communication system between individuals would have come about only secondarily, as many linguists believe,\" (Chomsky 2007, 23) Part of this vehemence dovetails with the single individual mutation hypothesis of the origins of language – it helps if the significance of communication is downplayed in an account where communication is initially impossible, simply because there is no other language-endowed being to communicate with. If communication were language's killer feature, then the selective pressure for the incriminated gene to propagate would not kick in. The other part can reasonably be attributed to Chomsky's intent to make a clean break from a prior popular theory on language acquisition, epitomized by B. F. Skinner's 1957 monograph Verbal Behavior , which offered a heavily empiricist, behaviorist account of language learning in terms of a stimulus-response cycle. Characteristically, Chomsky's strategy is to trivialize the function of the stimulus, casually implying both that it might not be needed at all, and if it is, then details of the role it plays are of little interest: it would not be at all surprising to find that normal language learning requires use of language in real-life situations, in some way. But this, if true [sic!], would not be sufficient to show that information regarding situational context (in particular, a pairing of signals with structural descriptions that is at least in part prior to assumptions about syntactic structure) plays any role in determining how language is acquired, once the mechanism is put to work and the task of language learning is undertaken by the child. (Chomsky 1965, 33) In retrospect, Skinner's account may be simplistic in many ways, but the basic notion that one has to pay attention to stimuli and responses in the course of particular linguistic interactions is sound. In particular, a theory of language built on this foundation successfully copes with all of the impasses we have explored above regarding Chomsky's approach. One such framework is that of evolutionary linguistics. Evolutionary linguistics views language as a complex adaptive system with emergent properties (Steels 2015, 8–9). A complex adaptive system is a system which is not centrally organized, coordinated or designed: its \"macroscopic\" characteristics are said to \"emerge\" as the result of localized interactions between individual entities (agents) with similar \"microscopic\" characteristics (be they physical, behavioral or motivational). The whole is more than the sum of its parts, and none of the agents can be properly said to have designed the system, nor can they deliberately change it in an arbitrary way; but all are continuously shaping it by taking part in the interactions that constitute its fabric. Examples of complex adaptive systems include the dynamics of insect societies (beehives, ant nests) or patterns of collective motion in large animal groups (flocks of birds or shoals of fish). These and more are discussed in much greater depth in the first chapter of Pierre-Yves Oudeyer's book Self-Organization in the Evolution of Speech . Adaptiveness is a property that these systems acquire by virtue of not being hardwired on the macro level: they are defined functionally instead of structurally. If the conditions in the environment change, the system will adapt to keep fulfilling its function, because the agents are forced to modify their behavior in order to achieve their individual goals. Of course, they may fail to do so, in which case the system breaks down and ceases to exist. If we revert to the metaphor from the title of the present essay, according to Chomsky, language is a cathedral erected by a single unwitting architect, the random genetic mutation that endowed us with the language faculty. Conversely, Luc Steels and fellow evolutionary linguists argue that the apparent macroscopic orderliness of language is the result of a myriad interactions of multiple individual agents, as suggested by the the bazaar image. One form that linguistic research can take under this paradigm is formulating and running computational models which simulate the behavior of agent populations and study the microscopic conditions, i.e. the cognitive and physical abilities, motivations etc. of each agent, necessary for a system like language to emerge within the population and stabilize. By direct inspiration from Wittgenstein's Philosophical Investigations , the interactions between agents are termed \"language games\" (Steels 2015, 167–8); depending on the topic being investigated, the agents can play different types of language games with different rules. It is openly acknowledged that such simulations represent only a limited approximation of a well-defined subspace of the actual uses of language. In the research to date, rules are generally definite and set for the entire experiment, but simulating language games with fuzzy rules remains a perfectly valid research topic within this framework, in the Wittgensteinian spirit of allowing rules to be made up and modified \"as we go along\" (Wittgenstein 2009, 44e). A relatively simple game that agents can play is the so-called Guessing Game (see Chap. 2 of Steels 2015 for more details). In this scenario, a population of agents, embodied in physical robots, tries to establish a shared lexicon and coupled ontology for a simple world consisting of geometrical shapes. Each game is an interaction of two agents picked at random, in the context of a scene consisting of said geometrical shapes. One agent (the speaker) takes the initiative, selects a topic from the scene and names it; the other (the hearer) tries to guess which object the first one had in mind and points to it; the speaker decodes the pointing gesture and the game succeeds if he interprets it as referencing his original topic. If so, he acknowledges the match; otherwise, he points at the intended topic as a repair strategy. At the outset, neither the lexicon nor the ontology are given, only a set of sensors and actuators (which allow the agents to interact with the environment by taking in streams of raw perceptual data or producing sound and pointing gestures) and very general cognitive principles. These include an associative memory and feedback mechanisms to propagate failures and successes in conceptualization and communication to all components of the system and act on them. 5 New distinctions along the perceptual dimensions are introduced in a random fashion, 6 but those that lead to a successful unambiguous selection of a topic and communicative success are strengthened over the course of many interactions, while useless ones are dampened by lateral inhibition and eventually pruned. At the same time, speakers create new words for concepts that are as of yet missing from their lexicon, and hearers may adopt them into theirs for their conceptualization of the topic the speaker points at in case of failure. A similar feedback mechanism then ensures that highly successful words are preferred and come to dominate within the speech community. It is important to realize that at no time do the individual agents share the same ontology or lexicon: newly introduced distinctions and words are random and unique for each agent, agents simply gradually learn which of these are useful in achieving communicative success, which means that they naturally settle on ontologies and lexicons that are close enough to those of others in the population. This barely scratches the surface of how all these notions must be orchestrated for a working computer implementation of this model, not to mention the even more elaborate agent-based language game modeling experiments that are already being conducted, investigating for instance the emergence of grammar (see Part III of Steels 2015 for an overview of recent scholarship). We see that even a seemingly simple task like establishing a shared conceptualization of reality and agreeing on names for these concepts is a complex endeavor which relies on a highly sophisticated (though also highly general) machinery. Another key observation is that while agent-based models can be fully virtual, grounding them in physical reality (cf. the use of robots with sensors and actuators) brings additional challenges that enable researchers to reach vital insights which would otherwise be impossible. In particular, grounding introduces fuzziness on the sensory input channels (by virtue of different points of view for the two robots and analog-to-digital conversion) which the agents must cope with, or else the mechanisms they were endowed with cannot be considered as constituting a plausible, sufficient model of the dynamics of human language. Unlike in generative grammar, anything that is transient, imperfect, is eminently included in the purview of linguistic inquiry. Failures are very much part of the dynamics that steer the evolution of language. How could it adapt to the speakers' changing requirements if it did not include appropriate repair strategies? Indeed, how could it be bootstrapped at all? The reward is a model that successfully simulates not only language emergence, but also transmission: if virgin agents are added into an existing population, they gradually acquire its language (see Fig. 1). Figure 1. Communicative success in a population of agents with a steady influx of virgin agents and outflux of old ones (overall population size remains the same). The game starts in phase 1 with 20 virgin agents; phases 2 and 4 show the behavior of the model at an agent renewal rate of 1/1000 games, whereas phase 3 corresponds to a heavier rate of 1/100. (Figure from Steels 2015, 121.) Crucially, the drive to communicate, to interact, is built into the agents, they just keep playing games as long as they can. But if it was not, the simulation would have to be more complex and somehow elicit this drive by introducing appropriate ecological constraints, e.g. by requiring co-operation as a survival strategy (Steels 2015, 106). Otherwise, the agents would have no motivation to strive for communicative success in their mutual encounters, they would fail to reach intersubjective alignment of their conceptual spaces and lexicons, and language would not emerge. In other words, far from being an afterthought, successful communication with a partner, grounded in an external context, turns out to be a fundamental requirement to establish the kind of dynamics which allow languages to appear. Paradoxically, since it concerns itself with simulations and computational models, this branch of evolutionary linguistics is, like generative grammar, also highly speculative. However, unlike generative grammar, it is a kind of speculation which considers guidance by empirical observations a necessity, not a nuisance. Furthermore, simulations are meant to be tested: if an agent-based model of language emergence fails to converge on the result stipulated for that particular experiment, the model is plain wrong and the dynamics it is trying to put into place (cognitive strategies, feedback propagation etc.) need to be revised. There is thus a clear-cut criterion for validity. Lastly, even if a simulation works, an accompanying debate as to whether the mechanisms involved are actually plausible approximations of reality is considered an integral part of hypothesis evaluation, with evidence from strongly empirically grounded disciplines like biology and neurophysiology a vital element in the process. Conclusion Taking a cue from Wittgenstein's Philosophical Investigations , this essay should not be construed as an attempt to replace one doctrine with another, but to advocate a \"change of attitude\" (cf. McGinn 2013, 33) which allows asking more meaningful questions about language. This being said, on the evidence presented above, it is hard not to conclude that Noam Chomsky is fundamentally mistaken about the corrective that is necessary for language learning to take place. According to Chomsky, the criterion for evaluating linguistic rules lies within a dedicated language organ we are genetically endowed with; the innate structures themselves embody the metric by which conjectures pertaining to linguistic rules will be judged. By contrast, in the evolutionary linguistics perspective, genetics provide innate structures which are capable of random growth, but the feedback (reinforcement and pruning) which results in steering this growth in a particular direction comes from interactions with the environment. This theory presupposes much less specificity in the hardware infrastructure which makes this possible and so should be preferred both on grounds of simplicity and flexibility of the model, not to mention that it is biologically plausible and has been empirically verified to work. In the context of science, Chomsky's rhetorical strategy in and of itself is dishonest: he preaches formal rigor while practicing sleight of hand, and casually retreats to increasingly abstract ground on reaching an impasse. He thus carves out a region in discursive space which has no corresponding equivalent in a logically consistent conceptual space, without which a piece of discourse can hardly constitute a scientific theory. In other words, much like his famous example sentence \"Colorless green ideas sleep furiously\", his discourse is grammatical but largely nonsensical under the requirements on a system of thought which aspires to mirror reality in a coherent fashion. Requirements on scientific discourse notwithstanding, we as linguists should keep in mind that language in general is much more than a system for encoding logical propositions. Even Wittgenstein had to resign himself to the fact – or perhaps knew all along – that the Tractatus , which he framed as the ultimate solution to all metaphysical controversies, could only fan the flames of philosophical debate. It was after all addressed to a diverse community of people bound perhaps exclusively by their penchant for elaborate language games. References Aarsleff, Hans. 1970. \"The History of Linguistics and Professor Chomsky.\" Language 46 (3): 570–85. Chomsky, Noam. 1956. \"Three Models for the Description of Language.\" ———. 1965. Aspects of the Theory of Syntax . Cambridge, MA: The M.I.T. Press. ———. 1986. Knowledge of Language: Its Nature, Origin, and Use . Convergence. New York, Westport, London: Praeger. ———. 1995. The Minimalist Program . Cambridge, MA: The MIT Press. ———. 2002. Syntactic Structures . 2nd ed. Berlin, New York: Mouton de Gruyter. ———. 2007. \"Of Minds and Language.\" Biolinguistics , no. 1: 9–27. ———. 2009. Cartesian Linguistics: A Chapter in the History of Rationalist Thought . 3rd ed. Cambridge: Cambridge University Press. McGinn, Marie. 2006. Elucidating the Tractatus: Wittgenstein's Early Philosophy of Logic and Language . Oxford: Oxford University Press. ———. 2013. The Routledge Guidebook to Wittgenstein's Philosophical Investigations . The Routledge Guides to Great Books. Routledge. Miel, Jan. 1969. \"Pascal, Port-Royal, and Cartesian Linguistics.\" Journal of the History of Ideas 30 (2): 261–71. Oudeyer, Pierre-Yves. 2006. Self-Organization in the Evolution of Speech . Translated by James R. Hurford. Oxford, New York: OUP. Pullum, Geoffrey K. 2011. \"On the Mathematical Foundations of Syntactic Structures .\" Journal of Logic, Language and Information 20: 277–96. Raymond, Eric S. 1999. The Cathedral & the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary . O'Reilly Media. Sampson, Geoffrey. 2005. The \"Language Instinct\" Debate . 3rd ed. London, New York: Continuum. ———. 2015. \"Rigid Strings and Flaky Snowflakes.\" Language and Cognition 10: 1–17. Skinner, B. F. 1957. Verbal Behavior . The Century Psychology Series. New York: Appleton – Century – Crofts. Steels, Luc. 2015. The Talking Heads Experiment: Origins of Words and Meanings . Computational Models of Language Evolution 1. Berlin: Language Science Press. Wittgenstein, Ludwig. 2001. Tractatus Logico-Philosophicus . Routledge Classics. London, New York: Routledge. ———. 2009. Philosophical Investigations . Chichester, United Kingdom: Blackwell Publishing. Let us pretend for a moment that language games like \"poetry\" or the surrealist pastime of cadavre exquis do not exist; in these, the quoted sentence could appear as perfectly valid and meaningful, though perhaps not in the sense that Chomsky intended. \"Meaningful\" in a late-Wittgensteinian perspective could be paraphrased as \"accepted by at least one involved party as a valid turn within the context of a particular language game\". ↩ Arguing that this does not matter because we should be concerned with the ideal speaker-hearer's competence just takes us further down the impasse, because now we have to determine how to delimit the purported \"ideal\". ↩ This is the beauty of building empirically unmotivated, purely speculative theories: at any moment, one can freely accommodate a new element into the existing framework, substituting novelty and amalgamation for critical evaluation. ↩ Cf. also Sampson's riposte : \"If complex properties of some aspect of human behaviour have to be as they are as a matter of conceptual necessity, then there is no reason to postulate complex genetically inherited cognitive machinery determining those behaviour patterns\" (Sampson 2015, 9). ↩ This interconnected architecture stands in stark contrast to Chomsky's deliberately isolationist approach: \"the relation between semantics and syntax […] can only be studied after the syntactic structure has been determined on independent grounds\" (Chomsky 2002, 17). ↩ Wittgenstein only hints at the problem of conceptualization, but he is prescient in realizing it is not a given: \"The primary elements [of the objects which constitute the world in this particular language game] are the coloured squares. ‘But are these simple?' – I wouldn't know what I could more naturally call a ‘simple' in this language-game. But under other circumstances, I'd call a monochrome square, consisting perhaps of two rectangles or of the elements colour and shape, ‘composite'\" (Wittgenstein 2009, 27e). ↩","tags":"ling","url":"cathedral-and-bazaar.html"},{"title":"Configuring Emacs Daemon on Mac OS X","text":"I know I promised this article a loooong time ago (June 2014, when I first got a Mac, to judge by the previous timestamp in the header of this file), but since the historically attested readership of this blog is 2 + a bunch of my facebook friends who I nagged to read my attempt at explaining character encodings to non-technical people , I don't suppose it's as if a legion of fans have been restlessly looking forward to this one ;) Nevertheless, the distinct advantage is that my OS X Emacs setup has had the opportunity to grow more mature and also much simpler in the meantime, which means that if a third reader accidentally stumbles over this note (exploding my ratings...), they might actually find something genuinely useful here. tl;dr This article presents a way to start Emacs Daemon (a persistent Emacs session) from the GUI and subsequently connect to it (creating frames on demand) using an Automator script . The benefit is that you incur startup time lag only once (when you start the daemon) while still being able to close all frames when you're not using Emacs, keeping a clean workspace . This is especially useful if your Emacs is heavily customized and loading it takes a while . Another benefit is that whenever you open a frame connected to an Emacs daemon, all your previously open buffers are still there as you left them (as opposed to opening a fresh instance of Emacs). Skim over the code blocks to get the important gist without the verbose sauce. Tested on OS X 10.11 El Capitan, with Homebrew Emacs and Spacemacs config. Why Emacs Daemon, why this post Installing Emacs on a Mac in and of itself is not that much of a problem -- there are several options, ranging from Homebrew and Macports to Emacs for Mac OS X , Emacs Mac Port and Aquamacs . The last two in this list have some OS X specific tweaks (smooth scrolling, tabs, adapted keyboard shortcuts), which makes them perhaps more appealing out of the box but also less extensible, as some of the information out there about generic Emacs might not apply to them as straightforwardly or indeed at all. With that in mind, if you want to tinker with your Emacs config, it's a good idea to stick with Homebrew's fairly conservative version of Emacs: $ brew update $ brew install emacs --with-cocoa # this step gets you a standard OS X launcher icon $ brew linkapps emacs But now that you've got Emacs, and especially if you're transferring some heavy customization over from say Linux, you might be unhappy that each time you start it from cold, it takes a while, typically a few seconds. That's what emacs --daemon and emacsclient are for: Emacs is run as a daemon in the backround and you connect to it with client frames that spawn almost instantly . This also means that you can close all existing frames to keep your workspace clean if you won't be using Emacs for a while (hard to imagine, right, since you can even read xkcd from inside Emacs ) and then whip up a frame at the speed of a thought when need arises. Now this is all easy to achieve when using the terminal , but since you probably bought that Mac in great part for its shiny pretty elegant ergonomic GUI, you might want Emacs to use GUI frames instead of terminal ones and connect to the Emacs daemon (or start it if it's not running) by just clicking on an app icon in the launcher or finding it from Spotlight. That's where Automator comes in. An Automator script Automator is a built-in OS X app for creating custom automated user workflows for just about any installed app you might have or even OS functionality. Among other things, this means that it allows you to wrap the daemon auto-start functionality available from the terminal (as described in the previous paragraph) into an app launchable from the GUI. Let's get down to business: Launch Automator and create a new document. Select Application as its type. Search the Actions palette on the left for the Run Shell Script action and add it to your Automator document. In the Run Shell Script building block, change the following: set Shell to the shell you're using and whose init files have thus the PATH correctly set to the emacs and emacsclient executables (if you're using Homebrew, it probably told you how to properly set up your PATH as a post-install step) set Pass input to \"as arguments\" (if you then set this Automator app as the default for opening a given type of file , you'll be able to use emacsclient to open files by double-clicking on them in Finder) Finally, paste in the following code snippet and save the app e.g. as EmacsClient.app , preferably in your Applications folder so that it is easily accessible from the launcher. emacsclient --no-wait -c -a emacs \" $@ \" >/dev/null 2 > & 1 & EDIT : An earlier version of this article had nohup prepended to the command above; as pointed out in the comments by MaTres (thanks!), this is unnecessary . At the end of the day, your Automator EmacsClient.app should look something like this: The core of the command that you might want to tweak based on your particular Emacs setup is emacsclient --no-wait -c -a emacs ; mine is optimized to work with mostly stock Spacemacs config (see below). If it doesn't work, you might also want to try a simple emacsclient -c -a \"\" and variations; a good debugging technique is to try these out in the terminal: as soon as you get the line working there, it'll start working in the Automator task as well. \"$@\" is just the list of files (if any) passed to Emacs to open (the aforementioned double-click in Finder use case). The rest is some black magic to ensure that the shell which spawns the Emacs process (because this Automator app is after all, at heart, only a shell script) totally and utterly disowns it, so that the shell script is allowed to return and the Automator task completes as soon as Emacs has started (or the client has spawned a new frame). Otherwise, you'd end up with an irritating spinning cog wheel in your notification area which would stay there until you completely quit Emacs. Which is probably not what you want, since you're undergoing all this hassle in the first place to get a zen, distraction-free Emacs experience. The details of the various incantations are discussed in this Apple forum thread , but let's have a whirlwind tour for the moderately interested (my knowledge of Unix processes is far from perfect, so feel free to correct me on these points!): >/dev/null redirects standard output to oblivion and 2>&1 redirects standard error to standard output (i.e. also to oblivion), which persuades Automator that you're really not expecting to hear from the process via these standard streams ever again, so there's no point in keeping the shell script running. These can be shortened to &>/dev/null . the final & runs the command in the background, which ensures control of the shell is returned to the user as soon as the process is spawned; since there are no additional commands in the shell script and all remaining ties have been severed, Automator finally agrees that the task has probably done all it was expected to do and exits it. Wrapping up Whew! That's it. It's really not that complicated, it's just that my prose is verbose, so it makes it look like there's lots and lots to do. Trust me, there isn't. My first go at solving this usability problem -- the one I originally wanted to post way back in 2014 -- was a lengthy, godawful Applescript prone to subtle breakage. This is much better. And the ability to just use a single GUI app for transparently launching and connecting to the Emacs daemon is pure bliss. While you're at it, for an even better Emacs experience, go fetch the excellent Spacemacs Emacs config distribution , which pulls this venerable piece of software screaming into the 21st century. The best editor is neither Vim nor Emacs, its Vim + Emacs! The addictive icing of Vim modal editing on the outside, a creamy Elisp core -- what more could you want from life? ;) Oh and if, like me, you love Spacemacs' snappy icon with the Evil spaceship over planet Emacs -- or if, like me, you have OCD -- you'll definitely want to switch your Emacs logo to the Spacemacs one !","tags":"macOS","url":"emacs-daemon-osx.html"},{"title":"How computers handle text: a gentle but thorough introduction to Unicode","text":"Or, the absolute minimum every software developer linguist absolutely, positively must know about Unicode and character sets (no excuses!) Note : This text uses the Python programming language to give some hands-on experience of the concepts discussed. If you're not familiar with programming at all, much less with Python, my advice is: either: ignore the code, focus on the comments around it, they should be enough to follow the thread of the explanation; or, if you've got a little more time: Python is pretty intuitive, so you might want to have a look at the code examples anyway. In any case, the code might make more sense to you if you tinker with it in an interactive Python session: And now, without further ado... Much like any other piece of data inside a digital computer, text is represented as a series of binary digits (bits), i.e. 0's and 1's. A mapping between sequences of bits and characters is called an encoding. How many different characters your encoding can handle depends on how many bits you allow per character: with 1 bit you can have 2&#94;1 = 2 characters (one is mapped to 0, the other to 1) with 2 bits you can have 2&#94;2 = 2*2 = 4 characters (mapped to 00, 01, 10 and 11) with 3 bits you can have 2&#94;3 = 2*2*2 = 8 characters etc. Now, a short digression on representing numbers, to make sure we're all on the same page: in the context of computers, you often see the same number represented in three different ways: as a decimal number, using 10 digits 0--9 (e.g. 12) as a binary number, using only 2 digits, 0 and 1 (e.g. 1100, which equals decimal 12) as a hexadecimal number, using 16 digits: 0--9 and a--f (e.g. c, which also equals decimal 12) As a consequence, the need often arises to convert between these different numeral systems . If you don't want to do so by hand, Python has your back! The bin() function gives you a string representation of the binary form of a number: In [1]: bin ( 65 ) Out[1]: '0b1000001' As you can see, in Python, binary numbers are given a 0b prefix to distinguish them from regular (decimal) numbers. The number itself is what follows after (i.e. 1000001). Similarly, hexadecimal numbers are given an 0x prefix: In [2]: hex ( 65 ) Out[2]: '0x41' To convert in the opposite direction, i.e. to decimal, just evaluate the binary or hexadecimal representation of a number: In [3]: 0b1000001 Out[3]: 65 In [4]: 0x41 Out[4]: 65 Numbers using these different bases (base 2 = binary, base 10 = decimal, base 16 = hexadecimal) are mutually compatible, e.g. for comparison purposes: In [5]: 0b1000001 == 0x41 == 65 Out[5]: True Why are hexadecimals useful? They're primarily a more convenient, condensed way of representing sequences of bits: each hexadecimal digit can represent 16 different values, and therefore it can stand in for a sequence of 4 bits (2&#94;4 = 16). In [6]: 0xa == 0b1010 Out[6]: True In [7]: 0xb == 0b1011 Out[7]: True In [8]: # if we paste together hexadecimal a and b, it's the # same as pasting together binary 1010 and 1011 0xab == 0b10101011 Out[8]: True In other words, instead of binary 10101011, we can just write hexadecimal ab and save ourselves some space. Of course, this only works if shorter binary numbers are padded to a 4-bit width : In [9]: 0x2 == 0b10 Out[9]: True In [10]: 0x3 == 0b11 Out[10]: True In [11]: # if we paste together hexadecimal 2 and 3, we have to # paste together binary 0010 and 0011... 0x23 == 0b00100011 Out[11]: True In [12]: # ... not just 10 and 11 0x23 == 0b1011 Out[12]: False The padding has no effect on the value, much like decimal 42 and 00000042 are effectively the same numbers. Now back to text encodings. The oldest encoding still in widespread use is ASCII , which is a 7-bit encoding. What's the number of different sequences of seven 1's and 0's? In [13]: # this is how Python spells 2&#94;7, i.e. 2*2*2*2*2*2*2 2 ** 7 Out[13]: 128 This means ASCII can represent 128 different characters , which comfortably fits the basic Latin alphabet (both lowercase and uppercase), Arabic numerals, punctuation and some \"control characters\" which were primarily useful on the old teletype terminals for which ASCII was designed. For instance, the letter \"A\" corresponds to the number 65 ( 1000001 in binary, see above). \"ASCII\" stands for \" American Standard Code for Information Interchange\" -- which explains why there are no accented characters, for instance. Nowadays, ASCII is represented using 8 bits (= 1 byte), because that's the unit of computer memory which has become ubiquitous (in terms of both hardware and software assumptions), but still uses only 7 bits' worth of information. That extra bit means that there's room for another 128 characters in addition to the 128 ASCII ones , coming up to a total of 256. In [14]: 2 ** ( 7 + 1 ) Out[14]: 256 What happens in the range [128; 256) is not covered by the ASCII standard. In the 1990s, many encodings were standardized which used this range for their own purposes, usually representing additional accented characters used in a particular region. E.g. Czech (and Slovak, Polish...) alphabets can be represented using the ISO latin-2 encoding, or Microsoft's cp-1250 . Encodings which stick to the same character mappings as ASCII in the range [0; 128) and represent them physically in the same way (as 1 byte) , while potentially adding more character mappings beyond that, are called ASCII -compatible . ASCII compatibility is a good thing™, because when you start reading a character stream in a computer, there's no way to know in advance what encoding it is in (unless it's a file you've encoded yourself). So in practice, a heuristic has been established to start reading the stream assuming it is ASCII by default, and switch to a different encoding if evidence becomes available that motivates it. For instance, HTML files should all start something like this: <!DOCTYPE html> < html > < head > < meta charset = \"utf-8\" /> ... This way, whenever a program wants to read a file like this, it can start off with ASCII , waiting to see if it reaches the charset (i.e. encoding) attribute, and once it does, it can switch from ASCII to that encoding ( UTF-8 here) and restart reading the file, now fairly sure that it's using the correct encoding. This trick works only if we can assume that whatever encoding the rest of the file is in, the first few lines can be considered as ASCII for all practical intents and purposes. Without the charset attribute, the only way to know if the encoding is right would be for you to look at the rendered text and see if it makes sense; if it did not, you'd have to resort to trial and error, manually switching the encodings and looking for the one in which the numbers behind the characters stop coming out as gibberish and are actually translated into intelligible text. Let's take a look at printable characters in the Latin-2 character set . The character set consists of mappings between positive integers (whole numbers) and characters; each one of these is called a codepoint . The Latin-2 encoding then defines how to encode each of these integers as a series of bits (1's and 0's) in the computer's memory. In [15]: latin2_printable_characters = [] # the Latin-2 character set has 256 codepoints, corresponding to # integers from 0 to 255 for codepoint in range ( 256 ): # the Latin-2 encoding is simple: each codepoint is encoded # as the byte corresponding to that integer in binary byte = bytes ([ codepoint ]) character = byte . decode ( encoding = \"latin2\" ) if character . isprintable (): latin2_printable_characters . append (( codepoint , character )) latin2_printable_characters Out[15]: [(32, ' '), (33, '!'), (34, '\"'), (35, '#'), (36, '$'), (37, '%'), (38, '&'), (39, \"'\"), (40, '('), (41, ')'), (42, '*'), (43, '+'), (44, ','), (45, '-'), (46, '.'), (47, '/'), (48, '0'), (49, '1'), (50, '2'), (51, '3'), (52, '4'), (53, '5'), (54, '6'), (55, '7'), (56, '8'), (57, '9'), (58, ':'), (59, ';'), (60, '<'), (61, '='), (62, '>'), (63, '?'), (64, '@'), (65, 'A'), (66, 'B'), (67, 'C'), (68, 'D'), (69, 'E'), (70, 'F'), (71, 'G'), (72, 'H'), (73, 'I'), (74, 'J'), (75, 'K'), (76, 'L'), (77, 'M'), (78, 'N'), (79, 'O'), (80, 'P'), (81, 'Q'), (82, 'R'), (83, 'S'), (84, 'T'), (85, 'U'), (86, 'V'), (87, 'W'), (88, 'X'), (89, 'Y'), (90, 'Z'), (91, '['), (92, '\\\\'), (93, ']'), (94, '&#94;'), (95, '_'), (96, '`'), (97, 'a'), (98, 'b'), (99, 'c'), (100, 'd'), (101, 'e'), (102, 'f'), (103, 'g'), (104, 'h'), (105, 'i'), (106, 'j'), (107, 'k'), (108, 'l'), (109, 'm'), (110, 'n'), (111, 'o'), (112, 'p'), (113, 'q'), (114, 'r'), (115, 's'), (116, 't'), (117, 'u'), (118, 'v'), (119, 'w'), (120, 'x'), (121, 'y'), (122, 'z'), (123, '{'), (124, '|'), (125, '}'), (126, '~'), (161, 'Ą'), (162, '˘'), (163, 'Ł'), (164, '¤'), (165, 'Ľ'), (166, 'Ś'), (167, '§'), (168, '¨'), (169, 'Š'), (170, 'Ş'), (171, 'Ť'), (172, 'Ź'), (174, 'Ž'), (175, 'Ż'), (176, '°'), (177, 'ą'), (178, '˛'), (179, 'ł'), (180, '´'), (181, 'ľ'), (182, 'ś'), (183, 'ˇ'), (184, '¸'), (185, 'š'), (186, 'ş'), (187, 'ť'), (188, 'ź'), (189, '˝'), (190, 'ž'), (191, 'ż'), (192, 'Ŕ'), (193, 'Á'), (194, 'Â'), (195, 'Ă'), (196, 'Ä'), (197, 'Ĺ'), (198, 'Ć'), (199, 'Ç'), (200, 'Č'), (201, 'É'), (202, 'Ę'), (203, 'Ë'), (204, 'Ě'), (205, 'Í'), (206, 'Î'), (207, 'Ď'), (208, 'Đ'), (209, 'Ń'), (210, 'Ň'), (211, 'Ó'), (212, 'Ô'), (213, 'Ő'), (214, 'Ö'), (215, '×'), (216, 'Ř'), (217, 'Ů'), (218, 'Ú'), (219, 'Ű'), (220, 'Ü'), (221, 'Ý'), (222, 'Ţ'), (223, 'ß'), (224, 'ŕ'), (225, 'á'), (226, 'â'), (227, 'ă'), (228, 'ä'), (229, 'ĺ'), (230, 'ć'), (231, 'ç'), (232, 'č'), (233, 'é'), (234, 'ę'), (235, 'ë'), (236, 'ě'), (237, 'í'), (238, 'î'), (239, 'ď'), (240, 'đ'), (241, 'ń'), (242, 'ň'), (243, 'ó'), (244, 'ô'), (245, 'ő'), (246, 'ö'), (247, '÷'), (248, 'ř'), (249, 'ů'), (250, 'ú'), (251, 'ű'), (252, 'ü'), (253, 'ý'), (254, 'ţ'), (255, '˙')] Using the 8th bit (and thus the codepoint range [128; 256)) solves the problem of handling languages with character sets different than that of American English, but introduces a lot of complexity -- whenever you come across a text file with an unknown encoding, it might be in one of literally dozens of encodings. Additional drawbacks include: how to handle multilingual text with characters from many different alphabets, which are not part of the same 8-bit encoding? how to handle writing systems which have way more than 256 \"characters\", e.g. Chinese, Japanese and Korean (CJK) ideograms? For these purposes, a standard character set known as Unicode was developed which strives for universal coverage of (ultimately) all characters ever used in the history of writing, even adding new ones like emojis . Unicode is much bigger than the character sets we've seen so far -- its most frequently used subset, the Basic Multilingual Plane , has 2&#94;16 codepoints, but overall the number of codepoints is past 1M and there's room to accommodate many more. In [16]: 2 ** 16 Out[16]: 65536 Now, the most straightforward representation for 2&#94;16 codepoints is what? Well, it's simply using 16 bits per character, i.e. 2 bytes. That encoding exists, it's called UTF-16 (\"UTF\" stands for \"Unicode Transformation Format\"), but consider the drawbacks: we've lost ASCII compatibility by the simple fact of using 2 bytes per character instead of 1 (encoding \"a\" as 01100001 or 00000000|01100001 , with the | indicating an imaginary boundary between bytes, is not the same thing) encoding a string in a language which is mostly written down using basic letters of the Latin alphabet now takes up twice as much space (which is probably not a good idea, given the general dominance of English in electronic communication) Looks like we'll have to think outside the box. The box in question here is called fixed-width encodings -- all of the encoding schemes we've encountered so far were fixed-width, meaning that each character was represented by either 7, 8 or 16 bits. In other word, you could jump around the string in multiples of 7, 8 or 16 and always land at the beginning of a character. (Not exactly true for UTF-16 , because it is something more than just a \"16-bit ASCII \": it has ways of handling characters beyond 2&#94;16 using so-called surrogate sequences -- but you get the gist.) The smart idea that some bright people have come up with was to use a variable-width encoding . The most ubiquitous one currently is UTF-8 , which we've already met in the HTML example above. UTF-8 is ASCII -compatible, i.e. the 1's and 0's used to encode text containing only ASCII characters are the same regardless of whether you use ASCII or UTF-8 : it's a sequence of 8-bit bytes. But UTF-8 can also handle many more additional characters, as defined by the Unicode standard, by using progressively longer and longer sequences of bits. In [17]: def print_utf8_bytes ( char ): \"\"\"Prints binary representation of character as encoded by UTF-8. \"\"\" # encode the string as UTF-8 and iterate over the bytes; # iterating over a sequence of bytes yields integers in the # range [0; 256); the formatting directive \"{:08b}\" does two # things: # - \"b\" prints the integer in its binary representation # - \"08\" left-pads the binary representation with 0's to a total # width of 8, which is the width of a byte binary_bytes = [ f \" {byte:08b} \" for byte in char . encode ( \"utf8\" )] print ( f \" {char!r} encoded in UTF-8 is: {binary_bytes} \" ) print_utf8_bytes ( \"A\" ) # the representations... print_utf8_bytes ( \"č\" ) # ... keep... print_utf8_bytes ( \"字\" ) # ... getting longer. 'A' encoded in UTF-8 is: ['01000001'] 'č' encoded in UTF-8 is: ['11000100', '10001101'] '字' encoded in UTF-8 is: ['11100101', '10101101', '10010111'] How does that even work? The obvious problem here is that with a fixed-width encoding, you just chop up the string at regular intervals (7, 8, 16 bits) and you know that each interval represents one character. So how do you know where to chop up a variable width-encoded string, if each character can take up a different number of bits? Essentially, the trick is to use some of the bits in the representation of a codepoint to store information not about which character it is (whether it's an \"A\" or a \"字\"), but how many bits it occupies . In other words, if you want to skip ahead 10 characters in a string encoded with a variable width-encoding, you can't just skip 10 * 7 or 8 or 16 bits; you have to read all the intervening characters to figure out how much space they take up. Take the following example: In [18]: for char in \"Básník 李白\" : print_utf8_bytes ( char ) 'B' encoded in UTF-8 is: ['01000010'] 'á' encoded in UTF-8 is: ['11000011', '10100001'] 's' encoded in UTF-8 is: ['01110011'] 'n' encoded in UTF-8 is: ['01101110'] 'í' encoded in UTF-8 is: ['11000011', '10101101'] 'k' encoded in UTF-8 is: ['01101011'] ' ' encoded in UTF-8 is: ['00100000'] '李' encoded in UTF-8 is: ['11100110', '10011101', '10001110'] '白' encoded in UTF-8 is: ['11100111', '10011001', '10111101'] Notice the initial bits in each byte of a character follow a pattern depending on how many bytes in total that character has: if it's a 1-byte character, that byte starts with 0 if it's a 2-byte character, the first byte starts with 11 and the following one with 10 if it's a 3-byte character, the first byte starts with 111 and the following ones with 10 This makes it possible to find out which bytes belong to which characters, and also to spot invalid strings, as the leading byte in a multi-byte sequence always \"announces\" how many continuation bytes (= starting with 10) should follow. So much for a quick introduction to UTF-8 (= the encoding), but there's much more to Unicode (= the character set). While UTF-8 defines only how integer numbers corresponding to codepoints are to be represented as 1's and 0's in a computer's memory, Unicode specifies how those numbers are to be interpreted as characters, what their properties and mutual relationships are, what conversions (i.e. mappings between (sequences of) codepoints) they can undergo, etc. Consider for instance the various ways diacritics are handled: \"č\" can be represented either as a single codepoint ( LATIN SMALL LETTER C WITH CARON -- all Unicode codepoints have cute names like this) or a sequence of two codepoints, the character \"c\" and a combining diacritic mark ( COMBINING CARON ). You can search for the codepoints corresponding to Unicode characters e.g. here and play with them in Python using the chr(0xXXXX) built-in function or with the special string escape sequence \\uXXXX (where XXXX is the hexadecimal representation of the codepoint) -- both are ways to get the character corresponding to the given codepoint: In [19]: # \"č\" as LATIN SMALL LETTER C WITH CARON, codepoint 010d print ( chr ( 0x010d )) print ( \" \\u010d \" ) č č In [20]: # \"č\" as a sequence of LATIN SMALL LETTER C, codepoint 0063, and # COMBINING CARON, codepoint 030c print ( chr ( 0x0063 ) + chr ( 0x030c )) print ( \" \\u0063\\u030c \" ) č č In [21]: # of course, chr() also works with decimal numbers chr ( 269 ) Out[21]: 'č' This means you have to be careful when working with languages that use accents, because to a computer, the two possible representations are of course different strings , even though to you, they're conceptually the same: In [22]: s1 = \" \\u010d \" s2 = \" \\u0063\\u030c \" # s1 and s2 look the same to the naked eye... print ( s1 , s2 ) č č In [23]: # ... but they're not s1 == s2 Out[23]: False Watch out, they even have different lengths ! This might come to bite you if you're trying to compute the length of a word in letters. In [24]: print ( \"s1 is\" , len ( s1 ), \"character(s) long.\" ) print ( \"s2 is\" , len ( s2 ), \"character(s) long.\" ) s1 is 1 character(s) long. s2 is 2 character(s) long. For this reason, even though we've been informally calling these Unicode entities \"characters\", it is more accurate and less confusing to use the technical term \"codepoints\". Generally, most text out there will use the first, single-codepoint approach whenever possible, and pre-packaged linguistic corpora will try to be consistent about this (unless they come from the web, which always warrants being suspicious and defensive about your material). If you're worried about inconsistencies in your data, you can perform a normalization : In [25]: from unicodedata import normalize # NFC stands for Normal Form C; this normalization applies a canonical # decomposition (into a multi-codepoint representation) followed by a # canonical composition (into a single-codepoint representation) s1 = normalize ( \"NFC\" , s1 ) s2 = normalize ( \"NFC\" , s2 ) s1 == s2 Out[25]: True Let's wrap things up by saying that Python itself uses Unicode internally, but the encoding it defaults to when opening an external file depends on the locale of the system (broadly speaking, the set of region, language and character-encoding related settings of the operating system). On most modern Linux and macOS systems, this will probably be a UTF-8 locale and Python will therefore assume UTF-8 as the encoding by default. Unfortunately, Windows is different. To be on the safe side, whenever opening files in Python, you can specify the encoding explicitly: In [26]: with open ( \"unicode.ipynb\" , encoding = \"utf-8\" ) as file : pass In fact, it's always a good idea to specify the encoding explicitly, using UTF-8 as a default if you don't know, for at least two reasons -- it makes your code more: portable -- it will work the same across different operating systems which assume different default encodings; and resistant to data corruption -- UTF-8 is more restrictive than fixed-width encodings, in the sense that not all sequences of bytes are valid UTF-8 . E.g. if one byte starts with 11, then the following one must start with 10 (see above). If it starts with anything else, it's an error. By contrast, in a fixed-width encoding, any sequence of bytes is valid. Decoding will always succeed, but if you use the wrong fixed-width encoding, the result will be garbage, which you might not notice. Therefore, it makes sense to default to UTF-8 : if it works, then there's a good chance that the file actually was encoded in UTF-8 and you've read the data in correctly; if it fails, you get an explicit error which prompts you to investigate further. Another good idea, when dealing with Unicode text from an unknown and unreliable source, is to look at the set of codepoints contained in it and eliminate or replace those that look suspicious. Here's a function to help with that: In [27]: import unicodedata as ud from collections import Counter import pandas as pd def inspect_codepoints ( string ): \"\"\"Create a frequency distribution of the codepoints in a string. \"\"\" char_frequencies = Counter ( string ) df = pd . DataFrame . from_records ( ( freq , char , f \"U+{ord(char):04x}\" , ud . name ( char ), ud . category ( char ) ) for char , freq in char_frequencies . most_common () ) df . columns = ( \"freq\" , \"char\" , \"codepoint\" , \"name\" , \"category\" ) return df Depending on your font configuration, it may be very hard to spot the two intruders in the sentence below. The frequency table shows the string contains regular LATIN SMALL LETTER T and LATIN SMALL LETTER G , but also their specialized but visually similar variants MATHEMATICAL SANS-SERIF SMALL T and LATIN SMALL LETTER SCRIPT G . You might want to replace such codepoints before doing further text processing... In [28]: inspect_codepoints ( \"Intruders here, good 𝗍hinɡ I checked.\" ) Out[28]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } freq char codepoint name category 0 5 e U+0065 LATIN SMALL LETTER E Ll 1 5 U+0020 SPACE Zs 2 3 r U+0072 LATIN SMALL LETTER R Ll 3 3 d U+0064 LATIN SMALL LETTER D Ll 4 3 h U+0068 LATIN SMALL LETTER H Ll 5 2 I U+0049 LATIN CAPITAL LETTER I Lu 6 2 n U+006e LATIN SMALL LETTER N Ll 7 2 o U+006f LATIN SMALL LETTER O Ll 8 2 c U+0063 LATIN SMALL LETTER C Ll 9 1 t U+0074 LATIN SMALL LETTER T Ll 10 1 u U+0075 LATIN SMALL LETTER U Ll 11 1 s U+0073 LATIN SMALL LETTER S Ll 12 1 , U+002c COMMA Po 13 1 g U+0067 LATIN SMALL LETTER G Ll 14 1 𝗍 U+1d5cd MATHEMATICAL SANS-SERIF SMALL T Ll 15 1 i U+0069 LATIN SMALL LETTER I Ll 16 1 ɡ U+0261 LATIN SMALL LETTER SCRIPT G Ll 17 1 k U+006b LATIN SMALL LETTER K Ll 18 1 . U+002e FULL STOP Po ... because of course, for a computer, the word \"thing\" written with two different variants of \"g\" is really just two different words, which is probably not what you want: In [29]: \"thing\" == \"thinɡ\" Out[29]: False Finally, to put things into perspective, here's a diagram what happens when processing text with Python (\"Unicode\" in the central box stands for Python's internal representation of Unicode, which is not UTF-8 nor UTF-16 ): (Image shamelessly hotlinked from / courtesy of the NLTK Book . Go check it out, it's an awesome intro to Python programming for linguists!) A terminological postscript: we've been using some terms a bit informally, but now that we have a practical intuition for what they mean, it's good to get the definitions straight in one's head. So, a character set is a mapping between codepoints (integers) and characters . We may for instance say that in our character set, the integer 99 corresponds to the character \"c\". On the other hand, an encoding is a mapping between a codepoint (an integer) and a physical sequence of 1's and 0's that represent it in memory . With fixed-width encodings, this mapping is generally straightforward -- the 1's and 0's directly represent the given integer, only in binary and padded with zeros to fit the desired width. With variable-width encodings, which have to explicitly encode information about how many bits are spanned by each codepoint, this straightforward correspondence breaks down. A comparison might be helpful here: as encodings, UTF-8 and UTF-16 both use the same character set -- the same integers corresponding to the same characters. But since they're different encodings , when the time comes to turn these integers into sequences of bits to store in a computer's memory, each of them generates a different one. For more on Unicode, a great read already hinted at above is Joel Spolsky's The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) . Another great piece of material is the Characters, Symbols and the Unicode Miracle video by the Computerphile channel on YouTube. To make the discussion digestible for newcomers, I sometimes slightly distorted facts about how things are \"really really\" done. And some inaccuracies may be genuine mistakes. In any case, please let me know in the comments! I'm grateful for feedback and looking to improve this material; I'll fix the mistakes and consider ditching some of the simplifications if they prove untenable :)","tags":"ling","url":"unicode.html"},{"title":"Úprava rozhraní konkordanceru KonText -- vylepšená verze","text":"Před nějakou dobou jsem zde vyvěsil skript , jehož pomocí lze lehce \"přeskládat\" a upravit rozhraní korpusového konkordanceru KonText : menu je umístěné po straně místo nahoře a permanentně rozbalené nad vyhledanou konkordancí je umístěn rychlý hledací box, v němž lze předchozí dotaz pohodlně upravit Víc o motivaci těchto úprav se dočtete v původním článku . Stále platí, že ČNK nemá v plánu tyto změny začlenit přímo do oficiální verze KonTextu, zejména proto, že rychlý hledací box sice v jistých situacích může být užitečný, nicméně oproti standardnímu formuláři Nový dotaz výrazně omezuje možnosti pro zadání dotazu. Vylepšená verze, která je k dispozici níže, odstraňuje některé předchozí nedostatky skriptu: rychlý hledací box nad konkordancí je větší, ukazuje vždy CQL podobu posledního zadaného dotazu 1 , a především zůstává zobrazený i během listování konkordancí (tj. není k dispozici jen na její první stránce). Dotaz lze nyní navíc pro větší přehlednost rozdělit do více řádků, takže opětovné vyhledávání se nově spouští stiskem kombinace kláves Ctrl+Enter (místo jen Enteru). Výsledné upravené rozhraní KonText vypadá stále podobně: Postup instalace skriptu Nová verze skriptu je k dispozici zde: Kroky k jeho zprovoznění zůstávají stejné: Nainstalovat si do svého prohlížeče plugin Tampermonkey , pokud používáte Chrome, nebo Greasemonkey , pokud používáte Firefox. (Pokud používáte Internet Explorer, budete muset dočasně přesedlat na Chrome nebo Firefox.) Testovaný je skript zatím jen na Chromu. Založit v daném pluginu nový skript (pro Chrome je tutorial zde , pro Firefox zde ). Smazat kostru nového skriptu a nahradit ji skriptem, který si zkopírujete výše. Skript uložit. Používat KonText jako normálně -- skript už by podle adresy měl sám poznat, že se má spustit. Pokud se tak nestane, nejspíš to znamená, že je prohlížečový plugin (Tampermonkey nebo Greasemonkey) deaktivovaný a je potřeba jej znovu aktivovat. V předchozí verzi se po aplikaci libovolného filtru změnil obsah hledacího boxu na parametry filtrování. ↩","tags":"ling","url":"kontext-interface-tweak-update.html"},{"title":"Úprava rozhraní konkordanceru KonText","text":"!POZOR! K dispozici je nyní vylepšená verze níže popsaného skriptu . Hledání v korpusech ČNK Český národní korpus je sbírka jazykových korpusů částečně vytvářených Ústavem Českého národního korpusu a částečně jinými institucemi. Všechny jsou hostované na jednom serveru a dostupné skrz různá vyhledávací rozhraní (tzv. konkordancery ), např. NoSke , Bonito či nejnověji KonText . Koncem března 2015 ovšem bude podpora starších rozhraní ukončena a nadále půjde k datům v ČNK přistupovat primárně pouze přes KonText. (Pokud vám odstavec výše nedává příliš smysl, s jazykovými korpusy se setkáváte poprvé, ale chcete se dozvědět víc, raději si místo tohoto postu přečtěte, k čemu je takový korpus dobrý , a zkuste si v něm něco pro zajímavost vyhledat . Pokud se vám při vzpomínce na Bonito či NoSke naopak zaskvěla slza v oku, čtěte dál!) KonText vs. Bonito / NoSke KonText má oproti starším rozhraním řadu výhod -- bohatší funkcionalitu, mnohé pomůcky, které vám pomohou se zadáním složitějších dotazů (sestavení morfologického tagu či podmínky within ), a v neposlední řadě mnohem lépe vypadá, což kupříkladu mně při práci působí jako balzám na duši. Nicméně dlouholetí uživatelé ČNK byli jednoduše zvyklí na některé aspekty Bonita a NoSke, které jim teď v KonTextu chybí. Onehdy při rozhovoru s jedním z nich vyplavaly na povrch jako hodně důležité dvě stížnosti: Vrchní menu v KonTextu je zákeřné, schovává se, člověk nemá přehled nad dostupnými funkcemi. Oproti tomu NoSke má menu po straně a je permanentně rozvinuté, takže uživatel má všechny možnosti interakce s konkordancí soustavně jako na dlani. Po zadání dotazu člověk často na základě konkordance zjistí, že jej potřebuje ještě trochu upravit / zjemnit. KonText si sice předchozí dotazy pamatuje, je ale potřeba se k nim doklikat; šikovnější by bylo, kdyby tato možnost byla dostupná přímo ze stránky konkordance v podobě nějakého zjednodušeného hledacího boxu. (NoSke tohle vlastně taky neumí, v Bonitu je to jednodušší.) V obou případech jde o smysluplné požadavky, jenže KonText je poměrně velká a složitá aplikace, takže i pokud se ČNK rozhodne do ní tyto podněty v nějaké podobě zapracovat (např. jako možnost přepnutí zobrazení menu), bude nějakou chvíli trvat, než se implementace navrhne, vytvoří, řádně otestuje a konečně dostane k uživatelům. Nicméně aby bylo možné alespoň vyzkoušet, jak by zmíněné změny vypadaly v praxi, dal jsem dohromady krátký skript, který již v prohlížeči nahraný KonText trochu \"přestaví\" a upraví. Výsledek vypadá následovně: Rovnou předesílám: ten skript je nevzhledný bastl přilepený na KonText zvnějšku; proto taky bylo možné jej dát dohromady poměrně rychle, protože si neklade nárok na spolehlivost, která se vyžaduje od oficiální verze KonTextu. Je to spíš prototyp, jehož účelem je otestovat výše popsané změny v praxi a získat představu o tom, zda a do jaké míry jsou přínosné. (Vlastní zkušenost: po chvíli používání mi přijde přídatný hledací box nad konkordancí hodně šikovný a užitečný.) Teď k jádru pudla: pokud máte zájem, můžete si KonText takto k obrazu svému (resp. k obrázku o odstavec výš) upravit také a vyzkoušet, jak vám takové nastavení vyhovuje. Když se vám jedna z úprav bude líbit (nebo vás u toho napadne jiná, kterou by si KonText zasloužil), můžete pak zadat požadavek na nový feature . Návod, jak si KonText upravit, následuje níže. Postup instalace skriptu Skript samotný je k dispozici zde: K jeho zprovoznění jsou potřeba následující kroky: Nainstalovat si do svého prohlížeče plugin Tampermonkey , pokud používáte Chrome, nebo Greasemonkey , pokud používáte Firefox. (Pokud používáte Internet Explorer, budete muset dočasně přesedlat na Chrome nebo Firefox.) Testovaný je skript zatím jen na Chromu. Založit v daném pluginu nový skript (pro Chrome je tutorial zde , pro Firefox zde ). Smazat kostru nového skriptu a nahradit ji skriptem, který si zkopírujete výše. Skript uložit. Používat KonText jako normálně -- skript už by podle adresy měl sám poznat, že se má spustit. Pokud se tak nestane, nejspíš to znamená, že je prohlížečový plugin (Tampermonkey nebo Greasemonkey) deaktivovaný a je potřeba jej znovu aktivovat. Omezení Skript má pravděpodobně hromadu drobných much, na které se mi zatím nepodařilo přijít -- budu se je snažit průběžně opravovat, když na ně padnu, nebo když mi o nich dáte vědět . Krom toho má i některé mouchy, o nichž už vím, ale bohužel toho s nimi nejde moc dělat. Asi nejnápadnější je, že přidaný hledací box funguje jen na těch stránkách, kde je původní dotaz i součástí adresy URL (což nejsou všechny -- třeba když začnete listovat konkordancí na druhou stránku a dál, dotaz je z adresy vyjmut a pomocný hledací box tedy zmizí ). Ale vzhledem k tomu, že jeho hlavní účel má být možnost lehce upravit dotaz po prvním rychlém nahlédnutí do konkordance, snad to nebude takový problém. Pokud někdy bude podobný box řádně přidán přímo do KonTextu, takovými nedostatky samozřejmě trpět nebude. A ještě k používání přidaného hledacího boxu : Typ dotazu, který je do něj potřeba zadat, je stejný jako ten, který jste při prvotním vyhledání konkordance zadali na stránce Nový dotaz . Pokud tento prvotní dotaz byl Základní dotaz, můžete pomocí rychlého boxu zadat jiný Základní dotaz; pokud to byl CQL dotaz, můžete ho upravit zas jen na další CQL dotaz. Důvodem je, že smyslem tohoto pomocného boxu není nahradit plnohodnotný formulář pro zadání dotazu, jen poskytnout rychlou možnost, jak již zadaný dotaz upravit . Pomocný hledací box se objeví i poté, co na konkordanci provedete filtrování. V takové situaci se dá použít k tomu, abyste pozměnili zadání aktuálního filtru , tj. filtrování se provede znovu na původní konkordanci, ne na této již filtrované. Pokud chcete opakovaně filtrovat tu samou konkordanci a postupně podle daných kritérií vyřazovat / přidávat řádky, je potřeba místo hledacího boxu opakovaně použít menu Filtr . Komu si stěžovat, když to nebude fungovat Skript je volně šiřitelný pod licencí GNU GPL v3 , takže se na něj neváže žádná záruka. Když se vám ale nebude dařit jej zprovoznit, rád se pokusím pomoct! Stačí se ozvat na adresu uvedenou zde .","tags":"ling","url":"kontext-interface-tweak.html"},{"title":"Beyond semantic versioning? (cross-post)","text":"Background Ever since I first read about semantic versioning , I've thought of it as a neat idea. But only recently did it occur to me that what I liked about the idea was its goal, much less its execution (more on that below). What made it obvious was this lengthy discussion about breaking changes introduced in v1.7 of underscore.js without an accompanying major version bump. Even though I still think sticking to semver is the right thing to do if your community of users expects you to (even if you don't personally like the system), I am convinced there are fundamentally better ways of dealing with the problem of safely and consistently updating dependencies. It made me want to add my two cents to the discussion , as someone who's more of a dabbler in programming and not really part of the community, so feel free to ignore me :) I attach my commentary below for reference (it's virtually the same text as in the link above). tl;dr semver is trying to do the right thing, but doing it wrong -- instead of implicitly encoding severity of change information in version numbers , explicit keywords like :patch, :potentially-breaking or :major-api-change would make much more sense. More verbosely I've always found the goals of semver worthy, but this thread has made me realize that while its aims are commendable, its methods are kind of broken: semver tries to take an existing semiotic system (= version numbers), which has developed informally and is therefore a loose convention rather than an exact spec, and reinterpret it in terms of an exact spec (or impose that spec on it). trouble is, the prior informal meaning won't go away so easily (why should it?), especially for projects that have been around longer than semver. the problem then is, since the two systems (the informal one and semver) look the same in terms of their symbolic representation, it's hard to guess which one you're dealing with by just eyeballing the version number of a library (or project in general). it's like if someone decided that \"f*ck\" should mean \"orchid\" from now on, because it's nicer -- on hearing the word, you'd never know if it's being used as the original profanity, or in its new meaning. homonymy is a pain to deal with when it's accidental (cf. NLP), so why introduce it on purpose? the job that semver set out to do should be fulfilled by a new formal means which is instantly recognizable, not by hijacking an existing one and overlaying additional interpretation on it and thus making it ambiguous . even if version numbers hadn't existed before semver, they're terribly inadequate for the purpose of conveying information about the severity of changes introduced by an update (though I understand their appeal to mathematically-minded people). they're inadequate because they're implicit -- it's a bit like if someone decided they don't need hash maps because they can make do with arrays by remembering the order in which they're adding in the key-val pairs. if I remember the order, then I know which key the given index implicitly refers to, and the result is as good as a hash map, isn't it? except it isn't. keys are useful because they have explicit semantics , making it instantly clear what kind of value you're retrieving. in the same way, encoding the information about the severity of changes into version numbers makes it implicit (in addition to being ambiguous, as stated previously). why not use explicit keyword tags along with the version number (which can be romantic, semantic -- whichever floats the dev team's boat and best reflects the progress of the project) to give a heads up as to the nature of the update? e.g. :patch, :potentially-breaking, :major-api-change etc. granted, even language is a code which needs to be learned, like semver (gross oversimplification here, but let's not get into the details of language acquisition), but since it's widely established and conventionalized for conveying the kinds of meanings semver is trying to convey, why not just use it when it's available ? why use a system (version numbers) which is less well-suited to the purpose and ambiguous to boot? (on the other hand, numbers are eminently well-suited for keeping track of which version is newer than which and how much so -- the original purpose of version numbering -- because they are designed to have orderings defined on them. by contrast, words would do a terrible job at this. if you care to indicate the evolution of your codebase, you might introduce your own disciplined romantic or sentimental versioning scheme, which ironically is a more meaningful and ergo semantic way of doing versioning than semver, because it sticks to the conventional semantics of numbers (the closer the numbers, the more similar the versions). if you don't care about this, which is perfectly fine, you might as well use dates for version numbers.) keyword tags have the advantage that they're instantly human-readable by anyone who has a basic command of English. if there is sufficient will in the community, a useful subset can be frozen in a binding spec, so that they are machine-readable as well. I'm not sure whether these keywords should be an appendix to the version number (like v2.3.4-:potentially-breaking), or whether the information they provide should be more extensive and included in a formalized preamble to the changelog (finally forcing people to at least take a glance at it ;) ). using the latter approach, the information provided could be (optionally) even more targeted, e.g. detailing explicitly which parts of the API are affected in a non-backwards compatible manner by the update. anyways, just a few ideas :) I am not primarily a coder, so there may be obvious drawbacks to this scheme that I can't see or which have already been discussed by the community on multiple occasions which have escaped my attention. in which case, please bear with me and excuse my lack of sophistication.","tags":"floss","url":"beyond-semver.html"},{"title":"Filling (hardwrapping) paragraphs in Airmail with `par`","text":"tl;dr Jump directly to the proposed solution . Tested on OS X 10.9 (Mavericks). Back story Airmail is a great application -- being very happy with Gmail's in-browser UI, it's honestly the first e-mail desktop client that I ever felt even remotely tempted to use. It has: a sleek, functional design almost flawless integration with Gmail (except for categories -- but there's a not-too-hackish way to deal with those) a Markdown compose mode (yay!) -- and tons of other good stuff. Especially that last feature almost got me sold -- you see, I like my e-mail hardwrapped (what Emacs calls \"filling paragraphs\"), because most of the time, I view it on monitors that are too wide for soft line wrapping to achieve a comfortable text width. (By the way, Airmail's layout deals with this issue very elegantly, but I know I won't be using only Airmail. Plus there are the obvious netiquette issues -- lines \"should be\" wrapped at 72 characters etc.) In Gmail, I therefore use plain-text compose, which is fine for the purposes described above, but frustrating whenever you want to apply formatting (obviously, you can't -- it's plain text). I tried using the usual replacements for formatting like stars & co., and I don't know about your grandma, but mine certainly doesn't take *...* to mean emphasis. I thought the Markdown compose mode in Airmail would solve my problems -- I could apply formatting if and when I wanted (using the frankly more streamlined process of typing it in rather than fumbling around for the right button in the GUI) and fill my paragraphs, because I somehow automatically assumed there'd by a hard-wrap feature like in any decent editor (read: emacs or vi). Markdown is plain text after all, isn't it? Long story short, as of yet, there isn't . There isn't even one for the plain-text compose mode, as far as I'm aware. So I added my two cents to this feature request thread and went back to the Gmail in-browser UI. Solution But then I realized (it took me a while, I'm still very much an OS X newbie): in OS X, you can define custom actions with shortcuts 1 for any application using Automator Services these actions can be easily set to receive text selected in the application as input these actions can also involve shell scripts there already is a great (command line) program for filling paragraphs -- it's called par , and as much as I admire what Airmail's developers have achieved, it's unlikely that they'd come up with a more sophisticated hard-wrapping algorithm than par 's simply as a side project for Airmail (see the EXAMPLES section in man par ) With that in mind, you can have hard-wrapping in Markdown or plain-text Airmail compose at your fingertips in no time flat. If you don't have homebrew , start by installing that (or any other ports manager that will allow you to install par ; I'll assume homebrew below) by pasting ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" at a Terminal prompt. Then: install par with brew install par at a Terminal prompt open Automator (e.g. by typing \"Automator\" into Spotlight) and create a new Service select the applications for which you want the service to be active (for me, that's just Airmail) and tick the \"Output replaces selected text\" box drag the \"Run Shell Script\" action onto the workflow canvas, and as the shell script, paste in PARINIT = \"rTbgqR B=.,?_A_a Q=_s>|\" /usr/local/bin/par 79 the $PARINIT environment variable contains the default recommended settings for par (if you want to customize its behavior, you can -- good luck wrapping your head around par 's manpage, though) you should set the full path to the par executable, the shell spawned by the Service might not inherit your $PATH -- for par installed via homebrew , it's /usr/local/bin/par the parameter at the end is the max number of characters per line -- mailing list etiquette stipulates 72, I personally prefer the pythonesque 79, but it's your choice At this point, your service should look something like in the screenshot below: Save it, open Keyboard preferences (type \"Keyboard\" into Spotlight), navigate to Shortcuts → Services → Text and set a keyboard shortcut for your newly created Service, e.g. Cmd+Opt+P. Next time you compose an e-mail in Airmail, just select the entire text when you're done (Cmd+A), press Cmd+Opt+P, and voilà! Your lines have been hardwrapped, your paragraphs filled :) (Same thing, I know.) If the shortcut doesn't appear to work 1 , try fiddling around with it, resetting it (maybe the one you've chosen conflicts with a pre-existing one?), restarting Airmail, logging out and back in, rebooting... The custom shortcut part is unfortunately the least reliable aspect of this whole setup. Automator is a great idea, I was pleasantly surprised by it when I started using OS X a few days back, but it could seriously use some bug-squashing. If you fail miserably at getting the shortcut to work, you can still access your fill paragraph service via the menu (select the text you want to hard-wrap, then navigate to Airmail → Services → <name of your fill paragraph service>). Clicking around in a GUI is tedious (though hey -- it's the Apple way after all, isn't it?), but it shouldn't be too much of a bother since you need to do it only once per e-mail. Bottom line : I am now officially completely sold on Airmail (even bought the released version instead of using the free beta) and look forward to the joy of using it! EDIT: In order to have the least trouble possible getting the shell script up and running as a Service , two rules of thumb: Leave it completely up to OS X where it stores the Service (.workflow) file. This will probably be in ~/Library/Services , and I learnt the hard way not to tinker with it -- if Services is a symlink instead of a real directory, the OS won't discover new Service files (though old ones will still be accessible). If the Service doesn't show up in the keyboard shortcuts menu after creation, try refreshing the service list with /System/Library/CoreServices/pbs -update . Those shortcuts are in fact quite buggy, especially those that you want to be global (not specific to a concrete app) -- at least on Mavericks (OS X 10.9). They tend to get disabled on a whim, especially if you tinker with them, and are a pain to get working again (login, logout, reboot -- anything goes). If anyone knows why, please let me know! ↩ ↩","tags":"macOS","url":"fill-par-in-airmail.html"}]}