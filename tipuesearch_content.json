{"pages":[{"title":"about me","text":"Contact David Luke≈°, Personal I'm a phonetician/linguist with side interests in statistics, programming and NLP. In fact, I'm terminally curious about computer-related stuff in general, which has led me down more rabbit holes than I'd care to count. I hail from Prague, Czech Republic, a precious little gem of a city, but I've also lived in France and Belgium for a while when I was a kid. I play badminton and football (of the soccer persuasion) and enjoy reading American Jewish literature, as well as watching (in no particular order) Monty Python, Bill Bailey, Jon Stewart, Ricky Gervais, Stephen Fry and other assorted smart and funny people who, along with Frank Zappa and The Beatles, are collectively responsible for my lifelong crush on the English language. I used to play the saxophone reasonably well, but nowadays mostly enjoy playing the guitar atrociously bad and singing along. Academic and professional credentials Current position I'm a junior researcher at the Institute of the Czech National Corpus , Faculty of Arts , Charles University (Prague) . My duties/interests include: coordinating phonetic transcription for the ORTOFON corpus project data processing and functionality prototyping (mainly using Python and R) research into the scope of language variability in Czech using multi-dimensional analysis For further details and older history, see my CV (updated spring 2017). Code When coding, I feel most comfortable in Python, and I love spending time with Rust (though I'm nowhere near as proficient, and occasionally lost and frustrated). I find R useful (though convoluted on occasion), write JavaScript when forced to and avoid Perl as much as I can. I know my way around the Unix command line and enjoy using it. And even after years of casual acquaintance, I'm still not really sure how I feel about Lisp, though I'll take Emacs Lisp over Vimscript any day. Feel free to check out my GitHub profile . MA thesis If you're interested in the affinities between people's abilities to process language and music, my MA thesis was about Perceptual sensitivity to music and speech stimuli in the frequency and temporal domains and I'm moderately proud of it. Unfortunately, it's in Czech (with an English abstract). Here's a quick summary: I was curious to know whether having a good musical ear correlates with having a good \"speech perception ear\" (spoiler alert: it does), so I put together a listening test (using Praat ) in which I made participants listen to short utterances and melodies to see how well they could detect manipulations in pitch and timing. There's also a companion repository on GitHub which contains the listening test (so that anyone can run it in Praat) and raw results as they were collected (for reproducibility). BA thesis Way back when, my academic interests lay in literary theory, so I wrote a BA thesis on Philip Roth . I still like Philip Roth very much, but I mostly tend to steer away from literary theory nowadays because I realized I'd rather be wrapping my head around logically coherent complex abstract notions.","tags":"pages","url":"pages/about.html"},{"title":"How to mount a Flask app under a URL prefix (or really, any WSGI app)","text":"Intro In a hurry? Skip to the minimal working example below! So this is probably something that seasoned Flask webapp devs will find trivial, especially if they're also well-acquainted with WSGI itself (the Python web server standard that Flask and the other popular synchronous Python web frameworks comply with). But my background is not like that. I've only occasionally used Flask for a project here and there over the years (and I've been really happy with it, since it delivers on its promise of being \"micro\" and keeps out of the way most of the time). I learned it mostly through its own docs. I've been aware that WSGI is a thing, but it never occurred to me I should learn more about it, it always felt like the Flask docs (should) cover everything I need. Which they did, except for the issue mentioned in the title of the article: transparently mounting the app under a URL prefix. Up until yesterday (when I found out basically by sheer luck that this is addressed by the WSGI standard that Flask conforms to), I had no idea what the proper way to do it in Flask was. I never could find any guidance in the Flask docs; as far as I know, I don't think the idiomatic solution I'll be discussing below is covered in there (and I've tried searching them again now that I know what I'm looking for). So I always resorted to clunky workarounds in order to achieve this. I'm fairly sure I'm not the only person like this -- a happy occasional user of Flask whose one major frustration has been having to manually smuggle URL prefixes into his routes. If you have a similar background, then may Google be more clement than it was to me and rank this article on the first page of its results when you hit this particular roadblock ;) If you're a Flask / WSGI whiz, then your reaction will probably be \"But of course this is how it's done!\" So in order to get something out of this article, you can instead muse upon the relative merits and drawbacks of splitting up technical documentation in a way that's well-organized / clearly delineated / methodical / logical (= Flask info in Flask docs, WSGI info in WSGI docs) vs. in a way that's useful and accessible even to occasional users, beginners etc. (= Flask docs might want to cover the parts of WSGI that a budding Flask dev might want to care about, even though that means duplicating information). The problem As you're prototyping and developing a Flask app, routing is pretty easy and straightforward: @app . route ( \"/login\" , methods = [ \"GET\" , \"POST\" ]) def login_func (): ... But when the time comes to deploy it, you often realize you need to mount it under a URL prefix , e.g. /my-app . For the route above, that means that you want to trigger it when users navigate to example.com/my-app/login , not example.com/login . And similarly, you want this prefix added when generating internal redirects or links in your templates. So you think to yourself, this is surely such a common use case that there's definitely an example how to do this directly in Flask's Quickstart . Or failing that, definitely somewhere in the docs. So you search the docs for a reasonable query like \"url prefix\", which sounds like what you're interested in, and you learn about blueprints , which is where stuff starts to become confusing. Blueprints are for making reusable application components and they support registering at a URL prefix and/or a subdomain. They're also a fairly advanced feature useful in more complicated apps, so they sort of go against the grain of your expectations with Flask. You likely picked up Flask because you liked that a simple app can consist of just a single Python module and a few lines of code, but now it looks like if you want to run it in production in a flexible way, you'll need to learn about blueprints and rewrite it as one? Because it sure doesn't look like vanilla Flask apps support URL prefixes, the docs don't mention any promising leads... So you end up deciding to roll your own homegrown solution, which might be configurable but still boilerplate-y if you still have some mental energy left to invest into the problem: PREFIX = \"/my-app\" @app . route ( f \" { PREFIX } /login\" , methods = [ \"GET\" , \"POST\" ]) def login_func (): ... Or it might just be hardcoded to show you really don't care anymore, if you've just lost an hour searching for a Flask feature that would handle this in a civilized manner, and/or reading up on blueprints (before giving up): @app . route ( \"/my-app/login\" , methods = [ \"GET\" , \"POST\" ]) def login_func (): ... If that sounds like you (and it certainly sounds like me), then boy do I have good news for you :) Turns out there actually is a right way to do it! The idiomatic solution The title of the article hints at the reason why the right way to do it is so hard to figure out: it turns out that deploying a Flask app under a URL prefix isn't actually a feature of Flask per se, it's a feature of the Python WSGI standard that Flask conforms to. So that's probably why the way to achieve this isn't described in the Flask docs themselves, because I suppose you're expected to read up on the available WSGI knobs and handles separately...? Frankly, I don't think that's a realistic expectation :) I think this information would make a great addition to the Flask docs, maybe even directly in the Quickstart section. So what's the trick? The trick is to set the rather quirkily/quaintly named SCRIPT_NAME environment variable prior to starting the web server running the app. If you set SCRIPT_NAME=/my-app , WSGI guarantees that the web server running your app will strip this prefix from incoming URLs, and add it to outgoing URLs (redirects, in-app links in templates). That's it, no need to modify your app at all, whether to add a configurable or hardcoded prefix to all the routes, or to rewrite it as a blueprint. Nice. EDIT: As u/james_pic points out over on Reddit , reading SCRIPT_NAME from an env var is actually not required by the WSGI spec, it's a convenience feature provided by some WSGI servers, e.g. Gunicorn . Caveats Because of course, there are a few issues you might run into. Flask's builtin web server ignores SCRIPT_NAME EDIT: As should be clear from the previous edit, Flask's builtin web server only ignores the SCRIPT_NAME env var. It works perfectly well with the SCRIPT_NAME WSGI variable, which however requires quite a bit more work to set up. You need to add WSGI routing middleware to your application, e.g. like this: from werkzeug.middleware.dispatcher import DispatcherMiddleware from werkzeug.wrappers import Response app . wsgi_app = DispatcherMiddleware ( Response ( 'Not Found' , status = 404 ), { '/my-app' : app . wsgi_app } ) Presumably, you'd make the actual value of the prefix part of the app's config. (Code sample courtesy of u/james_pic over on Reddit , thank you!) When you find out about SCRIPT_NAME , your first intuition is to test if it indeed does what you need with Flask's builtin dev server, because, well, that's what you use for development. Unfortunately, it turns out that Flask's server couldn't care less about this env var, which might lead you to (wrongly) conclude that you're on the wrong path. I actually think this happened to me a few years back because I feel like I've tinkered with SCRIPT_NAME before, only to conclude it doesn't seem to do anything. The fix: just use a fully-featured production WSGI server that supports reading this configuration from an environment variable , e.g. Gunicorn . I promise SCRIPT_NAME works there. Alternatively, add some routing middleware of the kind sketched above to your app. Is this one of the reasons why Flask's builtin server warns \"This is a development server. Do not use it in a production deployment\"? I'd always thought it was for performance / stability / security reasons, but it looks like it also just isn't (fully) WSGI-compliant. Use url_for to generate internal links I.e. instead of writing href=\"/login\" in your templates or redirect(\"/login\") in your view functions, write href=\"{{ url_for('login_func') }}\" and redirect(url_for(\"login_func\")) . This will make sure the URLs are correctly prefixed with SCRIPT_NAME , if applicable. This is recommended as best practice by the Flask docs anyway, so it's not like it's additional hoops to jump through. Interestingly, this section in the Flask docs is the closest they brush with discussing mounting the app under a URL prefix. They tell you: If your application is placed outside the URL root, for example, in /myapplication instead of / , url_for() properly handles that for you. But they don't tell you how to place your application outside the URL root in the first place, which is kind of maddening. If you open the link to the url_for() docs , you are further teased with mentions of APPLICATION_ROOT and SERVER_NAME configuration values, which sound a lot like the kind of setting you were looking for, except the docs immediately dash your hopes and remain silent about what you really want, i.e. the SCRIPT_NAME env var: Configuration values APPLICATION_ROOT and SERVER_NAME are only used when generating URLs outside of a request context. I remember tearing my hair out in despair at this point of digging through the Flask docs and trying in vain (of course) to tinker with those configuration values a few years back. EDIT: Again, after reading u/james_pic 's very helpful and insightful comment on Reddit , I've realized the Flask docs technically do contain information which can help you figure this out, in the Application Dispatching section, specifically under Dispatch by Path . The trouble is that the section is introduced by the following sentence: Application dispatching is the process of combining multiple Flask applications on the WSGI level. Which sounds like it's once again a more advanced topic (kind of like blueprints) meant for people who are trying to run multiple apps under the same WSGI server at the same time (which you're not). The code examples are fairly complex to match, so at first sight, it doesn't look like this is what you should be spending your time reading if you just need to figure out something as basic as running your app under a URL prefix in production. Don't strip the prefix in your reverse proxy config If you're configuring your Flask app under a URL prefix, it's probably running behind a reverse proxy of some sort (Nginx, Apache, etc.). Reverse proxies can be configured to strip these prefixes when relaying requests to individual apps, but don't do this ! You want to leave the handling of the prefix to the WSGI server running your app, because the WSGI server has intimate knowledge about the app (it knows it can expect the app to talk WSGI), so it can be smart about how exactly prefix handling should be done. By contrast, the reverse proxy has very little information about your app -- it can rewrite incoming URLs in request headers fairly easily, but that's about it, it can hardly rewrite outgoing URLs in response bodies, that would be non-trivial. That's what sprinkling url_for() s where appropriate throughout your app is for (see previous section). So make sure your reverse proxy leaves the prefix alone. As an example, for Nginx, you don't want this, which would get rid of the /my-app/ prefix: location /my-app/ { proxy_pass http://127.0.0.1:8000/ ; # ... } Instead, you want this: location /my-app/ { proxy_pass http://127.0.0.1:8000/my-app/ ; # ... &#94;&#94;&#94;&#94;&#94;&#94;&#94; } Minimal working example Let's wrap up with a runnable demo. Put this in app.py : from flask import Flask , url_for app = Flask ( __name__ ) @app . route ( \"/\" ) def index (): return url_for ( \"index\" ) Run the app with a WSGI-compliant web server, e.g. Gunicorn (again, not Flask's builtin dev server, see above), while setting the SCRIPT_NAME env var: $ SCRIPT_NAME = /my-app gunicorn app:app Now try accessing / with curl : $ curl localhost:8000/ You should get an internal server error. If you inspect the traceback in the window where the app is running, you'll see this was due to the fact that the expected /my-app prefix wasn't found, so try adding it: $ curl localhost:8000/my-app/ /my-app/ Now everything works, even though your app's source code is clean and blissfully unaware of any prefix. Gunicorn strips away the prefix from the request before passing it on to your app's router, and url_for takes care of adding the prefix back into any internal link generated by your app, as you can see in the output ( /my-app/ instead of / ). Just to confirm Flask's builtin development server doesn't properly handle read SCRIPT_NAME from the environment -- run the app with it: $ FLASK_APP = app SCRIPT_NAME = /my-app flask run The prefix is ignored, which means that accessing / works (which we don't want): $ curl localhost:5000/ / And /my-app/ unfortunately doesn't: $ curl localhost:5000/my-app/ # outputs Flask's default 404 message","tags":"floss","url":"flask-wsgi-url-prefix.html"},{"title":"V≈°e, co jste kdy chtƒõli vƒõdƒõt o Pythonu...","text":"... ale b√°li jste se zeptat na hodinƒõ ;) In [1]: from utils import TableOfContents TableOfContents ( \"python_crash_course.ipynb\" ) Out[1]: Objekty Promƒõnn√© Typy a t≈ô√≠dy Importov√°n√≠ knihoven Funkce Lok√°ln√≠ vs. glob√°ln√≠ promƒõnn√© print vs. return Pokroƒçil√© zp≈Øsoby p≈ôed√°v√°n√≠ argument≈Ø Metody ƒå√≠sla ≈òetƒõzce Kolekce Seznamy n-tice Mno≈æiny Slovn√≠ky Vno≈ôen√© kolekce Nezabudovan√© kolekce Opakov√°n√≠ operac√≠: for-cyklus Logika Podm√≠nky: if, elif, else (a while-cyklus) Konvertory kolekc√≠ Drobnosti Pr√°ce se soubory V√Ωrazy vs. p≈ô√≠kazy Dokumentace Objekty Programov√°n√≠ v Pythonu spoƒç√≠v√° v manipulaci s r≈Øzn√Ωmi objekty -- ƒç√≠sli, ≈ôetƒõzci (p√≠smen), seznamy, funkcemi... Zast≈ôe≈°uj√≠c√≠ pojem \"objekt\" naznaƒçuje analogii s fyzick√Ωmi p≈ôedmƒõty. Ka≈æd√Ω p≈ôedmƒõt slou≈æ√≠ k nƒõƒçemu jin√©mu, nƒõco s n√≠m lze udƒõlat jednodu≈°e, nƒõco h≈Ø≈ô, nƒõco v≈Øbec; taky p≈ô√≠padnƒõ m≈Ø≈æe obsahovat dal≈°√≠ p≈ôedmƒõty. Podobnƒõ v Pythonu: ƒç√≠sla se hod√≠ na sƒç√≠t√°n√≠, ale nem≈Ø≈æeme zmƒõ≈ôit jejich d√©lku; a seznam jako≈æto objekt je vlastnƒõ kolekc√≠ dal≈°√≠ch objekt≈Ø, jeho prvk≈Ø. Kdy≈æ Python spust√≠te, naƒçte se pomƒõrnƒõ velk√© mno≈æstv√≠ zabudovan√Ωch objekt≈Ø (funkc√≠ a konstant), se kter√Ωmi m≈Ø≈æete rovnou zaƒç√≠t pracovat. Nap≈ô. funkce sorted vytvo≈ô√≠ z libovoln√©ho iterovateln√©ho objektu (= objektu, ze kter√©ho lze jeden po druh√©m tahat nƒõjak√© jeho d√≠lƒç√≠ prvky) se≈ôazen√Ω seznam: In [2]: # cokoli za znakem # na jak√©mkoli ≈ô√°dku Python ignoruje, # jsou to tzv. koment√°≈ôe, kter√© slou≈æ√≠ jen lidem sorted Out[2]: <function sorted(iterable, /, *, key=None, reverse=False)> ( Seznam v≈°ech zabudovan√Ωch funkc√≠ -- built-in functions -- v Pythonu ) Konstanta True zas oznaƒçuje \"pravdu\" (v logick√©m smyslu)... In [3]: True Out[3]: True ... tak≈æe ji Python vr√°t√≠ jako v√Ωslednou hodnotu p≈ôi vyhodnocen√≠ pravdiv√©ho v√Ωroku: In [4]: 2 + 2 == 4 Out[4]: True Ale dal≈°√≠ objekty si m≈Ø≈æeme libovolnƒõ vytv√°≈ôet. In [5]: # ƒç√≠sla 42 Out[5]: 42 In [6]: # ≈ôetƒõzce \"Hello, world!\" Out[6]: 'Hello, world!' Promƒõnn√© Abychom s objekty mohli d√°l pracovat, je pot≈ôeba je nƒõjak pojmenovat, jinak na nƒõ nem≈Ø≈æeme odkazovat. ≈†t√≠tek se jm√©nem m≈Ø≈æeme na objekt povƒõsit pomoc√≠ p≈ôi≈ôazovac√≠ho oper√°toru = . In [7]: vzd√°lenost = 10 In [8]: ƒças = 20 In [9]: rychlost = vzd√°lenost / ƒças In [10]: rychlost Out[10]: 0.5 Tƒõm ≈°t√≠tk≈Øm se jm√©ny se ≈ô√≠k√° promƒõnn√© , proto≈æe je lze libovolnƒõ kdykoli povƒõsit na jin√Ω objekt. In [11]: vzd√°lenost = 5 In [12]: rychlost = vzd√°lenost / ƒças In [13]: rychlost Out[13]: 0.25 Jsou i jin√© zp≈Øsoby, jak povƒõsit ≈°t√≠tek se jm√©nem na nƒõjak√Ω objekt. Nap≈ô. kdy≈æ teƒè vytvo≈ô√≠me funkci a pojmenujeme ji rychlost , tak ≈°t√≠tek rychlost strhneme z ƒç√≠sla 0.25, na kter√© jsme ho naposledy navƒõsili, a nalep√≠me ho na tu funkci (kter√° je z hlediska Pythonu zase jen dal≈°√≠m typem objektu, jako ƒç√≠sla, ≈ôetƒõzce atp.). In [14]: def rychlost ( vzd√°lenost , ƒças ): return vzd√°lenost / ƒças In [15]: rychlost Out[15]: <function __main__.rychlost(vzd√°lenost, ƒças)> Nyn√≠ tedy u≈æ promƒõnn√° rychlost neodkazuje na ƒç√≠slo 0.25, ale na funkci, kterou jsme pr√°vƒõ vytvo≈ôili. Funkci m≈Ø≈æeme zavolat , tj. spustit \"recept\" (s√©rii krok≈Ø, mal√Ω d√≠lƒç√≠ program), kter√Ω je v n√≠ ulo≈æen√Ω: In [16]: rychlost ( vzd√°lenost , ƒças ) Out[16]: 0.25 Nic nebr√°n√≠ tomu, abychom na existuj√≠c√≠ objekty navƒõsili ≈°t√≠tk≈Ø v√≠c, a m≈Ø≈æeme k tomu pou≈æ√≠t st√°vaj√≠c√≠ ≈°t√≠tky: In [17]: distance = vzd√°lenost time = ƒças speed = rychlost In [18]: speed ( distance , time ) Out[18]: 0.25 Jak to funguje? Python nejprve vyhodnot√≠ v√Ωraz napravo od oper√°toru = , tj. v tomto p≈ô√≠padƒõ jen dohled√° objekty, na kter√© odkazuj√≠ p≈Øvodn√≠ promƒõnn√©, a pak na ty sam√© objekty navƒõs√≠ dal≈°√≠ nov√© ≈°t√≠tky (uveden√© nalevo od oper√°toru = ). V≈°imnƒõte si, ≈æe jsme t√≠mto zp≈Øsobem jednodu≈°e p≈ôi≈ôadili dal≈°√≠ jm√©no i funkci rychlost : In [19]: speed Out[19]: <function __main__.rychlost(vzd√°lenost, ƒças)> To, zda dvƒõ jm√©na (dvƒõ promƒõnn√©) odkazuj√≠ na ten sam√Ω objekt, jednodu≈°e zjist√≠me pomoc√≠ oper√°toru is : In [20]: distance is vzd√°lenost Out[20]: True In [21]: distance is ƒças Out[21]: False In [22]: distance = ƒças In [23]: distance is ƒças Out[23]: True In [24]: speed is rychlost Out[24]: True POZN.: Rovnost ( x == y ) a identita ( x is y ) jsou dvƒõ r≈Øzn√© vƒõci. Dva r≈Øzn√© objekty (z hlediska identity) jsou si rovn√©, pokud reprezentuj√≠ tu samou hodnotu. Nap≈ô. dva r≈Øzn√© seznamy ƒç√≠sel od jedn√© do t≈ô√≠: In [25]: x = [ 1 , 2 , 3 ] y = [ 1 , 2 , 3 ] In [26]: x == y Out[26]: True In [27]: x is y Out[27]: False Je to podobnƒõ jako s jednovajeƒçn√Ωmi dvojƒçaty: vypadaj√≠ stejnƒõ, tak≈æe t≈ôeba pro √∫ƒçel focen√≠ jsou zamƒõniteln√°, ale to neznamen√°, ≈æe je to ten sam√Ω ƒçlovƒõk. Je≈°tƒõ p√°r slov ke konvenc√≠m pojmenov√°v√°n√≠ promƒõnn√Ωch v Pythonu: u drtiv√© vƒõt≈°iny promƒõnn√Ωch (vƒç. funkc√≠) je zvykem pou≈æ√≠vat jen mal√° p√≠smena, nap≈ô. promƒõnn√° obƒças se pou≈æ√≠vaj√≠ i ƒç√≠slice, nap≈ô. x1 , ale jm√©no promƒõnn√© ƒç√≠slic√≠ nesm√≠ zaƒç√≠nat kdy≈æ pojmenov√°n√≠ sest√°v√° z v√≠ce slov, oddƒõluj√≠ se pomoc√≠ podtr≈æ√≠tka, nap≈ô. v√≠ceslovn√°_pojmenov√°n√≠ (tzv. snake case ) promƒõnn√©, kter√© se pou≈æ√≠vaj√≠ nap≈ô√≠ƒç cel√Ωm programem (ƒçasto je definujeme na zaƒç√°tku programu a pak na nƒõ odkazujeme v r≈Øz√Ωch jeho ƒç√°stech), se nƒõkdy odli≈°uj√≠ pomoc√≠ velk√Ωch p√≠smen, nap≈ô. ABECEDA nebo RYCHLOST_SVƒöTLA jm√©na t≈ô√≠d (viz n√≠≈æe) jsou vƒõt≈°inou v tzv. camel case s velk√Ωm poƒç√°teƒçn√≠m p√≠smenem, nap≈ô. T≈ô√≠da ƒçi Jin√°T≈ô√≠da Jm√©no promƒõnn√© by nemƒõlo b√Ωt moc dlouh√©, ale z√°rove≈à by mƒõlo b√Ωt deskriptivn√≠ -- mƒõlo by srozumitelnƒõ naznaƒçovat, k ƒçemu promƒõnn√° slou≈æ√≠, jak se pou≈æ√≠v√°. Hezky a trefnƒõ pojmenovat promƒõnnou je ƒçasto tƒõ≈æ≈°√≠, ne≈æ by se na prvn√≠ pohled mohlo zd√°t . Kdy≈æ p√≠≈°ete program na jedno pou≈æit√≠, je to jedno, ale pokud po sobƒõ budete program je≈°tƒõ nƒõkdy ƒç√≠st (nebo nƒõkdo jin√Ω), budete za ka≈ædou smysluplnƒõ pojmenovanou promƒõnnou (a ka≈æd√Ω vhodnƒõ um√≠stƒõn√Ω koment√°≈ô) vdƒõƒçn√≠. Typy a t≈ô√≠dy Ka≈æd√Ω objekt v Pythonu spad√° do nƒõjak√© kategorie objekt≈Ø -- nap≈ô. ƒç√≠slo, ≈ôetƒõzec, seznam apod. -- podobnƒõ jako p≈ôedmƒõty v re√°ln√©m svƒõtƒõ spadaj√≠ do r≈Øzn√Ωch kategori√≠ (v≈°echny koƒçky do kategorie \"koƒçka\", v≈°echny tu≈æky do kategorie \"tu≈æka\" apod.). Tƒõmto kategori√≠m s v Pythonu ≈ô√≠k√° typy a u libovoln√©ho objektu lze zjistit jeho typ pomoc√≠ funkce type : In [28]: obj = 1 type ( obj ) Out[28]: int In [29]: obj = \"ahoj\" type ( obj ) Out[29]: str In [30]: obj = [ 1 , \"ahoj\" ] type ( obj ) Out[30]: list In [31]: type ( 1 ) == type ( 2 ) Out[31]: True In [32]: type ( \"a\" ) == type ( \"b\" ) Out[32]: True In [33]: # ov≈°em pozor type ( \"1\" ) Out[33]: str In [34]: type ( 1 ) == type ( \"1\" ) Out[34]: False Typy jako int , str nebo list jsou v Pythonu p≈ô√≠mo zabudovan√©, jsou k dispozici v≈ædycky. Ale ka≈æd√Ω program√°tor si m≈Ø≈æe definovat vlastn√≠ typy a d√°le s nimi pracovat. Tƒõmto u≈æivatelsky definovan√Ωm typ≈Øm se ≈ô√≠k√° t≈ô√≠dy , podle kl√≠ƒçov√©ho slov√≠ƒçka class , pomoc√≠ nƒõho≈æ se definuj√≠: In [35]: class FooBar : # kl√≠ƒçov√© slov√≠ƒçko pass je pr√°zdn√Ω p≈ô√≠kaz (Python na jeho z√°kladƒõ nic # nevykon√°) pass Instanci t≈ô√≠dy, tj. konkr√©tn√≠ objekt, jeho≈æ typem bude dan√° t≈ô√≠da, vytvo≈ô√≠me tak, ≈æe zavol√°me tzv. konstruktor , tj. funkci, kter√° n√°m instanci \"postav√≠\", \"zkonstruuje\". Poƒç√°teƒçn√≠ sestaven√≠ a nastaven√≠ objektu nƒõkdy naz√Ωv√°me inicializac√≠ . Jako konstruktor vƒõt≈°inou slou≈æ√≠ t≈ô√≠da samotn√°, kterou zavol√°me jako funkci: In [36]: obj = FooBar () type ( obj ) Out[36]: __main__.FooBar In [37]: type ( obj ) is FooBar Out[37]: True Oproti tomu instance zabudovan√Ωch typ≈Ø m≈Ø≈æeme vytv√°≈ôet i pomoc√≠ speci√°ln√≠ch z√°pis≈Ø, tzv. liter√°l≈Ø , nepot≈ôebujeme k tomu nutnƒõ funkce. V konstruktoru je proces vytv√°≈ôen√≠ objektu sv√Ωm zp≈Øsobem skryt√Ω, nen√≠ na prvn√≠ pohled jasn√©, co se v nƒõm odehr√°v√° (obecnƒõ m≈Ø≈æe konstruktorem b√Ωt libovoln√° funkce), kde≈æto v liter√°lu doslova (angl. literally ) prostƒõ jen vyp√≠≈°eme, jak m√° nƒõjak√Ω objekt vypadat, a Python n√°m ho podle toho poskl√°d√°: In [38]: # ƒç√≠seln√Ω liter√°l 42 Out[38]: 42 In [39]: # ≈ôetƒõzcov√Ω liter√°l \"Hello, world!\" Out[39]: 'Hello, world!' Liter√°ly jsou souƒç√°st√≠ syntaxe Pythonu, tj. pravidel pro skl√°d√°n√≠ program≈Ø tak, aby jim Python rozumƒõl a mohl je vykon√°vat. D≈Øle≈æit√© jsou je≈°tƒõ liter√°ly pro vytv√°≈ôen√≠ kolekc√≠ (viz odd. Kolekce). Chceme-li ovƒõ≈ôit, zda je nƒõjak√Ω objekt instanc√≠ nƒõjak√© dan√© t≈ô√≠dy ƒçi obecnƒõji typu, m≈Ø≈æeme pou≈æ√≠t funkci isinstance . In [40]: isinstance ( obj , FooBar ) Out[40]: True In [41]: isinstance ( 1 , int ) Out[41]: True T≈ô√≠dy se hod√≠ pro nƒõkter√© pokroƒçilej≈°√≠ program√°torsk√© √∫ƒçely (viz nap≈ô. notebook object_oriented.ipynb ). V tomto semestru se jim bl√≠≈æe nevƒõnujeme. Importov√°n√≠ knihoven Knihovna (t√©≈æ bal√≠ƒçek , angl. package ) je v programov√°n√≠ soubor funkc√≠, t≈ô√≠d atp., kter√© jsou ƒçasto u≈æiteƒçn√©, tak≈æe nem√° smysl, aby si je ka≈æd√Ω program√°tor implementoval znovu a znovu, kdy≈æ je pot≈ôebuje. Z√°rove≈à ale nejsou pot≈ôeba √∫plnƒõ po≈ô√°d, tak≈æe taky nem√° smysl, aby byly do Pythonu p≈ô√≠mo zabudovan√© (tj. aby byly k dispozici poka≈æd√©, kdy≈æ Python spust√≠me, podobnƒõ t≈ôeba jako funkce sorted ). V≈ædy jsou ale na dosah ruky, staƒç√≠ je importovat . Nap≈ô. knihovna random obsahuje funkcionalitu souvisej√≠c√≠ s generov√°n√≠m n√°hodn√Ωch ƒç√≠sel: In [42]: import random # n√°sleduj√≠c√≠mu p≈ô√≠kazu nen√≠ pot≈ôeba vƒõnovat p≈ô√≠li≈°nou # pozornost, jen zajist√≠, abychom poka≈æd√© dostali stejnou # s√©rii pseudon√°hodn√Ωch ƒç√≠sel; viz: # https://docs.python.org/3/library/random.html#random.seed random . seed ( 42 ) K objekt≈Øm, kter√© knihovna definuje, se pak dostaneme p≈ôes teƒçku. Nap≈ô. pokud chceme pomoc√≠ funkce randint vygenerovat n√°hodn√© ƒç√≠slo mezi jednou a deseti: In [43]: random . randint ( 1 , 10 ) Out[43]: 2 P≈ôi importu si lze knihovnu p≈ôejmenovat pomoc√≠ kl√≠ƒçov√©ho slov√≠ƒçka as , nap≈ô. pokud jm√©no random u≈æ v programu pou≈æ√≠v√°me pro nƒõco jin√©ho. In [44]: random = \"random. angl. n√°hodn√Ω\" import random as rnd rnd . randint ( 1 , 10 ), random Out[44]: (1, 'random. angl. n√°hodn√Ω') Taky je mo≈æn√© si z knihovny naimportovat p≈ô√≠mo jeden konkr√©tn√≠ objekt (funkci, t≈ô√≠du atp.): In [45]: from random import randint randint ( 1 , 10 ) Out[45]: 5 To se hod√≠ zejm. v p≈ô√≠padƒõ, kdy objekt budeme pou≈æ√≠vat ƒçasto a nechce se n√°m poka≈æd√© zdlouhavƒõ ps√°t jm√©no_knihovny.jm√©no_objektu . Pokud chceme, m≈Ø≈æeme si jich rovnou naimportovat i v√≠c z√°rove≈à: In [46]: from random import random , randint random (), randint ( 1 , 10 ) Out[46]: (0.24489185380347622, 3) Vƒõt≈°√≠ knihovny sest√°vaj√≠ z d√≠lƒç√≠ch modul≈Ø , kter√© p≈ôi importu oddƒõlujeme teƒçkami. In [47]: import os.path this_file = \"python_crash_course.ipynb\" os . path . isfile ( this_file ) Out[47]: True In [48]: import os.path as osp osp . realpath ( this_file ) Out[48]: '/home/david/src/dlukes.github.io/content/notebooks/python_crash_course.ipynb' In [49]: from os.path import splitext splitext ( this_file ) Out[49]: ('python_crash_course', '.ipynb') Funkce Funkce je jako recept -- s√©rie p≈ô√≠kaz≈Ø, mal√Ω d√≠lƒç√≠ program, kter√Ω m√° jm√©no, pomoc√≠ nƒõho≈æ ho m≈Ø≈æeme kdykoli spusit, tzv. zavolat . Funkci definujeme pomoc√≠ kl√≠ƒçov√©ho slov√≠ƒçka def ... In [50]: def hoƒè_kostkou (): # hlaviƒçka return randint ( 1 , 6 ) # tƒõlo ... a vol√°me tak, ≈æe nap√≠≈°eme jej√≠ jm√©no a za nƒõ kulat√© z√°vorky: In [51]: hoƒè_kostkou () Out[51]: 6 Recept ulo≈æen√Ω ve funkci se vykon√°v√° krok po kroku (≈ô√°dek po ≈ô√°dku), dokud Python nenaraz√≠ na kl√≠ƒçov√© slov√≠ƒçko return . V tu chv√≠li funkci ukonƒç√≠ a hodnotu v√Ωrazu, kter√Ω se nach√°z√≠ za return , vr√°t√≠ jako v√Ωsledek cel√© funkce. Tento v√Ωsledek m≈Ø≈æeme ulo≈æit do promƒõnn√©: In [52]: v√Ωsledek_hodu_kostkou = hoƒè_kostkou () In [53]: v√Ωsledek_hodu_kostkou Out[53]: 1 Do tƒõla funkce pat≈ô√≠ v≈°e, co n√°sleduje po hlaviƒçce a je odsazen√© alespo≈à o jednu √∫rove≈à d√°l ne≈æ samotn√° hlaviƒçka: In [54]: def function (): print ( \"Tohle pat≈ô√≠ do tƒõla funkce.\" ) print ( \"Tohle taky.\" ) print ( \"Tohle u≈æ ne.\" ) Tohle u≈æ ne. In [55]: function () Tohle pat≈ô√≠ do tƒõla funkce. Tohle taky. Funkce ƒçasto maj√≠ jeden nebo v√≠ce parametr≈Ø ƒçi argument≈Ø , tj. promƒõnn√Ωch, kter√© deklarujeme v hlaviƒçce funkce. Nap≈ô. n√°sleduj√≠c√≠ funkce m√° parametry poƒçet_stƒõn a n√°sobek . In [56]: def hoƒè_kostkou_a_vyn√°sob ( poƒçet_stƒõn , n√°sobek ): return randint ( 1 , poƒçet_stƒõn ) * n√°sobek Kdy≈æ funkci vol√°me, je pot≈ôeba za parametry dosadit konkr√©tn√≠ objekty, kter√© funkce pak v r√°mci receptu pou≈æije: In [57]: hoƒè_kostkou_a_vyn√°sob ( 20 , 100 ) Out[57]: 1800 POZN.: P≈ô√≠snƒõ vzato bychom mƒõli takto rozli≈°ovat mezi (form√°ln√≠mi) parametry (= promƒõnn√Ωmi, se kter√Ωmi pracujeme p≈ôi definici funkce a kter√© v tu chv√≠li nereprezentuj√≠ ≈æ√°dnou konkr√©tn√≠ hodnotu) a (konkr√©tnimi) argumenty (= konkr√©tn√≠mi objekty, kter√© funkci p≈ôed√°me ve chv√≠li, kdy ji vol√°me). Nicm√©nƒõ ƒçasto tento rozd√≠l nen√≠ p≈ô√≠li≈° d≈Øle≈æit√Ω, tak≈æe v praxi se oba term√≠ny pou≈æ√≠vaj√≠ zamƒõnitelnƒõ. Parametry mohou m√≠t defaultn√≠ hodnotu: In [58]: def hoƒè_kostkou_a_vyn√°sob ( poƒçet_stƒõn , n√°sobek = 100 ): return randint ( 1 , poƒçet_stƒõn ) * n√°sobek Pokud n√°m defaultn√≠ hodnota dan√©ho parametru vyhovuje, nemus√≠me ho pak p≈ôi vol√°n√≠ specifikovat: In [59]: hoƒè_kostkou_a_vyn√°sob ( 20 ) Out[59]: 300 Pokud n√°m naopak nevyhouje, nic n√°m v tom nebr√°n√≠: In [60]: hoƒè_kostkou_a_vyn√°sob ( 20 , 0.01 ) Out[60]: 0.19 Parametr≈Øm bez defaultn√≠ hodnoty se ≈ô√≠k√° povinn√© (je pot≈ôeba je specifikovat v≈ædy), parametr≈Øm s defaultn√≠ hodnotou pak voliteln√© . Parametry m≈Ø≈æeme funkci p≈ôed√°vat buƒè na z√°kladƒõ po≈ôad√≠ (tzv. poziƒçnƒõ )... In [61]: # poƒçet_stƒõn = 4, n√°sobek = 10 hoƒè_kostkou_a_vyn√°sob ( 4 , 10 ) Out[61]: 40 In [62]: # poƒçet_stƒõn = 10, n√°sobek = 4 hoƒè_kostkou_a_vyn√°sob ( 10 , 4 ) Out[62]: 4 ... nebo pomoc√≠ jejich jmen (pak hovo≈ô√≠me o pojmenovan√Ωch argumentech , angl. named nebo t√©≈æ keyword arguments ): In [63]: hoƒè_kostkou_a_vyn√°sob ( poƒçet_stƒõn = 4 , n√°sobek = 10 ) Out[63]: 10 In [64]: # v tomto p≈ô√≠padƒõ na po≈ôad√≠ nez√°le≈æ√≠ hoƒè_kostkou_a_vyn√°sob ( n√°sobek = 10 , poƒçet_stƒõn = 4 ) Out[64]: 10 Oba zp≈Øsoby m≈Ø≈æeme i kombinovat, dokonce je to velmi ƒçast√©. V Pythonu je vƒõt≈°inou zvykem, ≈æe se povinn√© parametry p≈ôed√°vaj√≠ poziƒçnƒõ (b√Ωv√° jich m√°lo a je pot≈ôeba je zad√°vat poka≈æd√©, tak≈æe si ƒçlovƒõk jejich po≈ôad√≠ zapamatuje) a voliteln√© parametry jsou pojmenovan√© (m≈Ø≈æe jich b√Ωt mnoho a pou≈æ√≠vaj√≠ se p≈ô√≠le≈æitostnƒõ, tak≈æe si ƒçlovƒõk jejich po≈ôad√≠ nezapamatuje; nav√≠c ƒçasto chceme specifikovat jen jeden a ostatn√≠m nechat defaultn√≠ hodnotu). V p≈ô√≠padƒõ na≈°√≠ funkce by to vypadalo takto: In [65]: hoƒè_kostkou_a_vyn√°sob ( 4 , n√°sobek = 10 ) Out[65]: 20 Lok√°ln√≠ vs. glob√°ln√≠ promƒõnn√© Parametry plus jak√©koli dal≈°√≠ promƒõnn√©, kter√© v r√°mci funkce vytvo≈ô√≠me, jsou soukrom√©, dostupn√© ƒçistƒõ jen funkci -- jsou to tzv. lok√°ln√≠ promƒõnn√© . T√©≈æ hovo≈ô√≠me o tom, ≈æe tyto promƒõnn√© maj√≠ lok√°ln√≠ dosah (angl. local scope ). To znamen√°, ≈æe s nimi m≈Ø≈æeme pracovat pouze v r√°mci tƒõla funkce. In [66]: def function ( parameter ): local_variable = 1 print ( \"My parameter:\" , parameter ) print ( \"My local variable:\" , local_variable ) In [67]: function ( 0 ) My parameter: 0 My local variable: 1 Mimo tƒõlo funkce function nejsou promƒõnn√© parameter a local_variable definovan√©: In [68]: parameter --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-68-cfbbb8ed4864> in <module> ----> 1 parameter NameError : name 'parameter' is not defined In [69]: local_variable --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-69-ffb5b39c1381> in <module> ----> 1 local_variable NameError : name 'local_variable' is not defined TIP: Nauƒçit se ƒç√≠st chybov√© hl√°≈°ky je k nezaplacen√≠, Python se v√°m s jejich pomoc√≠ sna≈æ√≠ ze v≈°ech sil poradit, v ƒçem je probl√©m. Zkuste si ka≈ædou chybovou hl√°≈°kou peƒçlivƒõ p≈ôeƒç√≠st, zamyslet se nad n√≠, pochopit, v ƒçem je probl√©m a kde p≈ôesnƒõ nastal. Funkce mohou taky pracovat s glob√°ln√≠mi promƒõnn√Ωmi , tj. s promƒõnn√Ωmi definovan√Ωmi samostatnƒõ, mimo nƒõjak√© funkce ƒçi t≈ô√≠dy. In [70]: global_variable = 2 In [71]: def function ( parameter ): local_variable = 1 print ( \"My parameter:\" , parameter ) print ( \"My local variable:\" , local_variable ) print ( \"My global variable:\" , global_variable ) In [72]: function ( 0 ) My parameter: 0 My local variable: 1 My global variable: 2 Takov√° funkce se ale snadno m≈Ø≈æe rozb√≠t -- staƒç√≠ dotyƒçnou glob√°ln√≠ promƒõnnou smazat... In [73]: del global_variable ... a kdy≈æ funkci p≈ô√≠≈°tƒõ zavol√°me, tak zkolabuje: In [74]: function ( 0 ) My parameter: 0 My local variable: 1 --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-74-e85ea32b6f77> in <module> ----> 1 function ( 0 ) <ipython-input-71-051e3887233a> in function (parameter) 3 print ( \"My parameter:\" , parameter ) 4 print ( \"My local variable:\" , local_variable ) ----> 5 print ( \"My global variable:\" , global_variable ) NameError : name 'global_variable' is not defined Snad je≈°tƒõ hor≈°√≠ je to, ≈æe u takov√© funkce nen√≠ mo≈æn√© jen na z√°kladƒõ jej√≠ho vol√°n√≠ odhadnout, co p≈ôesnƒõ udƒõl√°: In [75]: global_variable = 512 In [76]: function ( 0 ) My parameter: 0 My local variable: 1 My global variable: 512 In [77]: global_variable = 1024 In [78]: function ( 0 ) My parameter: 0 My local variable: 1 My global variable: 1024 V obou p≈ô√≠padech funkci vol√°me jako function(0) , ale v√Ωsledek je poka≈æd√© trochu jin√Ω. Mnohem lep≈°√≠ je tedy ps√°t funkce tak, aby pou≈æ√≠valy jen lok√°ln√≠ promƒõnn√© a nebyly z√°visl√© na tƒõch glob√°ln√≠ch. Nejde to v≈ædycky, ale vƒõt≈°inu ƒçasu ano. M√≠sto p≈ô√≠m√©ho odkazov√°n√≠ na glob√°ln√≠ promƒõnn√© je lep≈°√≠ d√°t funkci v√≠ce parametr≈Ø, p≈ôes nƒõ≈æ j√≠ m≈Ø≈æeme glob√°ln√≠ promƒõnn√© zprost≈ôedkovanƒõ p≈ôed√°vat, co≈æ je mnohem bezpeƒçnƒõj≈°√≠ a p≈ôehlednƒõj≈°√≠. In [79]: def function ( parameter1 , parameter2 ): local_variable = 1 print ( \"My first parameter:\" , parameter1 ) print ( \"My local variable:\" , local_variable ) print ( \"My other parameter:\" , parameter2 ) In [80]: global_variable = 2 function ( 0 , global_variable ) My first parameter: 0 My local variable: 1 My other parameter: 2 print vs. return Jak√Ω je rozd√≠l v tom, kdy≈æ zavol√°m n√°sleduj√≠c√≠ dvƒõ funkce? In [81]: def funkce1 (): print ( 1 ) In [82]: def funkce2 (): return 1 In [83]: funkce1 () 1 In [84]: funkce2 () Out[84]: 1 V obou p≈ô√≠padech se mi zobraz√≠ ƒç√≠slo 1, v druh√©m je nav√≠c vedle nƒõj ƒç√≠slo v hranat√Ωch z√°vork√°ch a dvojteƒçka. V ƒçem se to li≈°√≠? Rozd√≠l se l√©pe projev√≠, kdy≈æ se pokus√≠me v√Ωsledek funkce ulo≈æit do promƒõnn√©: In [85]: v√Ωsledek1 = funkce1 () 1 In [86]: v√Ωsledek1 In [87]: v√Ωsledek2 = funkce2 () In [88]: v√Ωsledek2 Out[88]: 1 funkce1 ƒç√≠slo 1 jen v r√°mci sv√©ho bƒõhu vytiskne na obrazovku, kde≈æto funkce2 ho vr√°t√≠ jako sv≈Øj v√Ωsledek, tak≈æe ho m≈Ø≈æeme ulo≈æit do promƒõnn√© a d√°l s n√≠m pracovat. Mo≈æn√° pom≈Ø≈æe, kdy≈æ se pod√≠v√°me na funkci, kter√° jedno ƒç√≠sle tiskne a jin√© vrac√≠: In [89]: def funkce3 (): print ( 1 ) return 2 In [90]: v√Ωsledek3 = funkce3 () 1 In [91]: v√Ωsledek3 Out[91]: 2 Zb√Ωv√° tedy jen ot√°zka: co vrac√≠ funkce1 ? Nic, kdy≈æ v≈Øbec ani neobsahuje kl√≠ƒçov√© slov√≠ƒçko return ? Sv√Ωm zp≈Øsobem ano, ale i nic je v Pythonu nƒõco ;) Co≈æ zn√≠ krypticky, ale je to jednoduch√©: Python m√° speci√°ln√≠ objekt (konstantu) None , kter√° reprezentuje \"nic\". Proto≈æe None je nic, tak se v√°m po vyhodnocen√≠ ani neuk√°≈æe: In [92]: None A kdykoli nƒõjak√° funkce dobƒõhne na konec sv√©ho receptu, ani≈æ by potkala return , tak implicitnƒõ automaticky vr√°t√≠ jako sv≈Øj v√Ωsledek None . V√≠m, ≈æe je trochu z√°ludn√©, ≈æe ono None nen√≠ \"vidƒõt\", ale v praxi si m≈Ø≈æeme to, ≈æe funkce1 skuteƒçnƒõ vr√°tila None , lehce ovƒõ≈ôit: In [93]: v√Ωsledek1 is None Out[93]: True Pokroƒçil√© zp≈Øsoby p≈ôed√°v√°n√≠ argument≈Ø V Pythonu lze tak√© vytvo≈ôit funkce, kter√© pracuj√≠ s libovoln√Ωm poƒçtem poziƒçn√≠ch nebo pojmenovan√Ωch argument≈Ø. Slou≈æ√≠ k tomu speci√°ln√≠ oper√°tory * a ** : In [94]: def flexibiln√≠_funkce ( poziƒçn√≠_argument , * zbytek_poziƒçn√≠ch_argument≈Ø , pojmenovan√Ω_argument , ** zbytek_pojmenovan√Ωch_argument≈Ø ): print ( \"poziƒçn√≠ argument:\" , poziƒçn√≠_argument ) print ( \"zbytek poziƒçn√≠ch argument≈Ø:\" , zbytek_poziƒçn√≠ch_argument≈Ø ) print ( \"pojmenovan√Ω argument:\" , pojmenovan√Ω_argument ) print ( \"zbytek pojmenovan√Ωch argument≈Ø:\" , zbytek_pojmenovan√Ωch_argument≈Ø ) In [95]: flexibiln√≠_funkce ( 1 , 2 , 3 , 4 , a = 5 , b = 6 , pojmenovan√Ω_argument = 7 , c = 8 ) poziƒçn√≠ argument: 1 zbytek poziƒçn√≠ch argument≈Ø: (2, 3, 4) pojmenovan√Ω argument: 7 zbytek pojmenovan√Ωch argument≈Ø: {'a': 5, 'b': 6, 'c': 8} Parametr *zbytek_poziƒçn√≠ch_argument≈Ø sesb√≠r√° zbyl√© poziƒçn√≠ argumenty, sestav√≠ z nich n-tici a tu ulo≈æ√≠ do promƒõnn√© zbytek_poziƒçn√≠ch_argument≈Ø ; podobnƒõ parametr **zbytek_pojmenovan√Ωch_argument≈Ø sesb√≠r√° zbyl√© pojmenovan√© argumenty, sestav√≠ z nich slovn√≠k a ulo≈æ√≠ ho do promƒõnn√© zbytek_pojmenovan√Ωch_argument≈Ø (v√≠c o n-tic√≠ch a slovn√≠c√≠ch viz odd. Kolekce). Jak vidno, m≈Ø≈æeme tyto speci√°ln√≠ parametry pojmenovat libovolnƒõ, speci√°ln√≠ chov√°n√≠ jim prop≈Øjƒçuje oper√°tor * , resp. ** . Nicm√©nƒõ konvenƒçnƒõ se v Pythonu pou≈æ√≠vaj√≠ jm√©na *args a **kwargs (jako keyword arguments ). A aby toho nebylo m√°lo, funguje to i \"naopak\": m√°me-li seznam/n-tici (nebo slovn√≠k) a chceme jeho polo≈æky p≈ôedat funkci jako poziƒçn√≠ (nebo pojmenovan√©) argumenty, m≈Ø≈æeme taky vyu≈æ√≠t oper√°toru * (nebo ** ): In [96]: def nudn√°_funkce ( a , b ): return a + b In [97]: poziƒçn√≠_argumenty = [ 1 , 2 ] In [98]: nudn√°_funkce ( * poziƒçn√≠_argumenty ) Out[98]: 3 In [99]: pojmenovan√©_argumenty = { \"a\" : 1 , \"b\" : 2 } In [100]: nudn√°_funkce ( ** pojmenovan√©_argumenty ) Out[100]: 3 U poziƒçn√≠ch argument≈Ø mus√≠ pochopitelnƒõ sedƒõt poƒçet... In [101]: poziƒçn√≠_argumenty = [ 1 , 2 , 3 ] In [102]: nudn√°_funkce ( * poziƒçn√≠_argumenty ) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-102-1d551cc00f6d> in <module> ----> 1 nudn√°_funkce ( * poziƒçn√≠_argumenty ) TypeError : nudn√°_funkce() takes 2 positional arguments but 3 were given ... a u pojmenovan√Ωch jm√©na: In [103]: pojmenovan√©_argumenty = { \"a\" : 1 , \"c\" : 2 } In [104]: nudn√°_funkce ( ** pojmenovan√©_argumenty ) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-104-75894a48cf5c> in <module> ----> 1 nudn√°_funkce ( ** pojmenovan√©_argumenty ) TypeError : nudn√°_funkce() got an unexpected keyword argument 'c' Metody Metody jsou funkce √∫zce spjat√© s objekty. Seznam metod, kter√© dan√Ω objekt podporuje, z√≠sk√°me tak, ≈æe nap√≠≈°eme jm√©no promƒõnn√©, kter√° objekt obsahuje, za nƒõ teƒçku a stiskneme tabul√°tor: obj .< TAB > In [105]: obj = \"ahoj\" obj . upper () Out[105]: 'AHOJ' Form√°lnƒõ je metoda jen funkce \"navƒõ≈°en√°\" na t≈ô√≠dƒõ: In [106]: class FooBar : def get_my_type ( self ): return type ( self ) In [107]: obj = FooBar () obj . get_my_type () Out[107]: __main__.FooBar Prvn√≠ argument metody ( self ) odkazuje na instanci t≈ô√≠dy, na kter√© jsme metodu zavolali, tj. v p≈ô√≠kladu v√Ω≈°e na ten sam√Ω objekt, na kter√Ω odkazuje promƒõnn√° obj . Na rozd√≠l od p≈ô√≠padn√Ωch dal≈°√≠ch argument≈Ø se metodƒõ p≈ôed√°v√° pomoc√≠ propojen√≠ p≈ôes teƒçku, ne v z√°vork√°ch za metodou, nep√≠≈°eme tedy obj.get_my_type(obj) . ƒå√≠sla V Pythonu jsou dva z√°kladn√≠ typy ƒç√≠sel: cel√° ƒç√≠sla (angl. integer ) -- typ int re√°ln√° ƒç√≠sla, reprezentovan√° pomoc√≠ tzv. pohybliv√© ≈ô√°dov√© ƒç√°rky (angl. floating point ) -- typ float Liter√°ly cel√Ωch ƒç√≠sel vypadaj√≠ nap≈ô. takhle: In [108]: 3 Out[108]: 3 In [109]: type ( 3 ) Out[109]: int In [110]: - 5 Out[110]: -5 In [111]: 9543761 Out[111]: 9543761 Pro lep≈°√≠ ƒçitelnost mohou obsahovat i podtr≈æ√≠tka: In [112]: 9_543_761 Out[112]: 9543761 Liter√°ly re√°ln√Ωch ƒç√≠sel se vƒõt≈°inou poznaj√≠ podle toho, ≈æe obsahuj√≠ desetinnou ƒç√°rku... tedy teƒçku proto≈æe je to podle angliƒçtiny: In [113]: 3.14 Out[113]: 3.14 In [114]: type ( 3.14 ) Out[114]: float In [115]: - 2.72 Out[115]: -2.72 In [116]: 0.1 Out[116]: 0.1 In [117]: # nulu m≈Ø≈æeme vynechat . 1 Out[117]: 0.1 Ov≈°em ne nutnƒõ -- Python m√° speci√°ln√≠ syntax pro tzv. vƒõdeck√Ω z√°pis ƒç√≠sel , a ƒç√≠sla zapsan√° t√≠mto zp≈Øsobem jsou v≈ædycky typu float , i kdy≈æ desetinnou teƒçku neobsahuj√≠. In [118]: # 1 √ó 10¬≥ 1e3 Out[118]: 1000.0 In [119]: # 1 √ó 10‚Åª¬≥ 1e-3 Out[119]: 0.001 In [120]: # 2.34 √ó 10¬≤ 2.34e2 Out[120]: 234.0 Kromƒõ cel√Ωch a re√°ln√Ωch ƒç√≠sel disponuje Python i komplexn√≠mi ƒç√≠sly: In [121]: 3 + 4 j Out[121]: (3+4j) In [122]: type ( 3 + 4 j ) Out[122]: complex Jestli jste o nich nikdy nesly≈°eli, tak je zase pus≈•te z hlavy :) Kromƒõ des√≠tkov√© (decim√°ln√≠) soustavy, na kterou jsme v≈°ichni zvykl√≠, maj√≠ cel√° ƒç√≠sla v Pythonu liter√°ly i v soustav√°ch jin√Ωch. Rozpozn√°me je pomoc√≠ prefix≈Ø: 0b ‚Üí bin√°rn√≠ (dvojkov√°) soustava -- pou≈æ√≠v√° ƒç√≠slice 0 a 1 0o ‚Üí okt√°ln√≠ (osmiƒçkov√°) soustava -- pou≈æ√≠v√° ƒç√≠slice 0‚Äì7 0x ‚Üí hexadecim√°ln√≠ (≈°estn√°ctkov√°) soustava -- pou≈æ√≠v√° ƒç√≠slice 0‚Äì9 a a‚Äìf In [123]: 0b11010 Out[123]: 26 In [124]: 0o32 Out[124]: 26 In [125]: 0x1a Out[125]: 26 Jak vid√≠me, p≈ôevod na des√≠tkovou soustavu je jednoduch√Ω -- staƒç√≠ vyhodnotit liter√°l v soustavƒõ jin√© a Python n√°m ho v odpovƒõdi p≈ôevede do des√≠tkov√©. K p≈ôevodu opaƒçn√Ωm smƒõrem existuj√≠ funkce, kter√© n√°m vr√°t√≠ ≈ôetƒõzec obsahuj√≠c√≠ z√°pis ƒç√≠sla v po≈æadovan√© soustavƒõ: In [126]: bin ( 26 ) Out[126]: '0b11010' In [127]: oct ( 26 ) Out[127]: '0o32' In [128]: hex ( 26 ) Out[128]: '0x1a' S ƒç√≠sly jdou prov√°dƒõt r≈Øzn√© v√Ωpoƒçty pomoc√≠ n√°sleduj√≠c√≠ch oper√°tor≈Ø , kter√© snad d√≠ky zku≈°enosti s kalkulaƒçkou budou p≈Øsobit p≈ôev√°≈ænƒõ povƒõdomƒõ. In [129]: # sƒç√≠t√°n√≠ 3 + 4 Out[129]: 7 In [130]: # odeƒç√≠t√°n√≠ 3 - 4 Out[130]: -1 In [131]: # n√°soben√≠ 3 * 4 Out[131]: 12 In [132]: # exponenciace 2 ** 3 Out[132]: 8 In [133]: # dƒõlen√≠ 5 / 3 Out[133]: 1.6666666666666667 U dƒõlen√≠ si v≈°imnƒõte, ≈æe v√Ωsledek je v≈ædy float , i kdy≈æ dƒõl√≠me dva int y a v√Ωsledkem je nƒõco, co my lid√© ch√°peme jako cel√© ƒç√≠slo: In [134]: 4 / 2 Out[134]: 2.0 In [135]: # celoƒç√≠seln√© dƒõlen√≠ 4 // 2 Out[135]: 2 In [136]: 5 // 3 Out[136]: 1 In [137]: # modulo (= zbytek po celoƒç√≠seln√©m dƒõlen√≠) 5 % 3 Out[137]: 2 Kdy≈æ chceme z√°rove≈à celoƒç√≠seln√© dƒõlen√≠ i zbytek po nƒõm, m≈Ø≈æeme pou≈æ√≠t zabudovanou funkci divmod . In [138]: divmod ( 5 , 3 ) Out[138]: (1, 2) In [139]: divmod ( 5 , 3 ) == ( 5 // 3 , 5 % 3 ) Out[139]: True To se hod√≠ nap≈ô. p≈ôi ƒçasov√Ωch p≈ôevodech -- kolik je 143 vte≈ôin minut? In [140]: divmod ( 143 , 60 ) Out[140]: (2, 23) ‚Üí 2 minuty a 23 vte≈ôin. Zabudovan√° funkce abs vr√°t√≠ absolutn√≠ hodnotu ƒç√≠sla: In [141]: abs ( - 4.1 ) Out[141]: 4.1 Pokud m√°me nƒõjakou kolekci ƒç√≠sel (viz odd. Kolekce), m≈Ø≈æeme identifikovat nejmen≈°√≠, resp. nejvƒõt≈°√≠ z nich pomoc√≠ zabudovan√Ωch funkc√≠ min a max . In [142]: from math import inf # inf reprezentuje nekoneƒçno inf Out[142]: inf In [143]: ƒç√≠sla = [ 0 , - inf , 3.14 , - 2.72 , inf ] In [144]: min ( ƒç√≠sla ) Out[144]: -inf In [145]: max ( ƒç√≠sla ) Out[145]: inf Oper√°tory maj√≠ r≈Øznou prioritu , stejnƒõ jako v matematice: In [146]: 2 ** 3 + 4 * 5 Out[146]: 28 Pokud si nejsme po≈ôad√≠m operac√≠ jisti, m≈Ø≈æeme ho pro jistotu specifikovat pomoc√≠ kulat√Ωch z√°vorek: In [147]: ( 2 ** 3 ) + ( 4 * 5 ) Out[147]: 28 Nebo pochopitelnƒõ i zmƒõnit: In [148]: 2 ** ( 3 + 4 ) * 5 Out[148]: 640 V≈°echny v√Ω≈°e uveden√© typy i z√°pisy ƒç√≠sel m≈Ø≈æeme p≈ôi v√Ωpoƒçtech libovolnƒõ kombinovat, jen je pot≈ôeba m√≠t na pamƒõti, ≈æe jakmile se n√°m jako d√≠lƒç√≠ v√Ωsledek objev√≠ float , i celkov√Ω v√Ωsledek bude float . In [149]: - 0x1f + 165 * - 0b1001 + 1.0 Out[149]: -1515.0 Ke konverzi z int u na float slou≈æ√≠ funkce float : In [150]: i = 3 In [151]: float ( i ) Out[151]: 3.0 Dok√°≈æe t√©≈æ p≈ôev√©st textov√Ω z√°pis ƒç√≠sla (≈ôetƒõzec) na re√°ln√© ƒç√≠slo: In [152]: float ( \"3.14\" ) Out[152]: 3.14 In [153]: float ( \"3\" ) Out[153]: 3.0 Analogicky funguje funkce int : In [154]: int ( 2.0 ) Out[154]: 2 In [155]: int ( \"2\" ) Out[155]: 2 In [156]: int ( 2.72 ) Out[156]: 2 V≈°imnƒõte si, ≈æe funkce int prostƒõ jen u≈ô√≠zne desetinn√° m√≠sta. Pokud chceme zaokrouhlit podle matematick√Ωch zvyklost√≠, je pot≈ôeba pou≈æ√≠t funkci round : In [157]: round ( 2.72 ) Out[157]: 3 P≈ôi poƒç√≠t√°n√≠ s float y je nutn√° jist√° obez≈ôetnost. Kv≈Øli tomu, jak jsou v pamƒõti poƒç√≠taƒçe reprezentov√°ny, nen√≠ jejich zachycen√≠ √∫plnƒõ p≈ôesn√©, a nƒõkter√© v√Ωsledky tak m≈Ø≈æou b√Ωt p≈ôekvapiv√©... In [158]: . 1 + . 2 Out[158]: 0.30000000000000004 ... co≈æ vede a≈æ k tomu, ≈æe nƒõkter√© oƒçek√°van√© rovnosti neplat√≠: In [159]: . 1 + . 2 == . 3 Out[159]: False P≈ôi porovn√°v√°n√≠ float ≈Ø je bezpeƒçnƒõj≈°√≠ m√≠sto oper√°toru == (striktn√≠ rovnosti) pou≈æ√≠vat funkci isclose z knihovny math (p≈ôibli≈æn√° rovnost). In [160]: from math import isclose isclose ( . 1 + . 2 , . 3 ) Out[160]: True S ƒç√≠seln√Ωmi promƒõnn√Ωmi ƒçasto nar√°≈æ√≠me na n√°sleduj√≠c√≠ situaci: vytvo≈ô√≠me promƒõnnou... In [161]: i = 0 i Out[161]: 0 ... a n√°slednƒõ ji pak pomoc√≠ nƒõjak√© aritmetick√© operace pot≈ôebujeme aktualizovat na z√°kladƒõ star√© hodnoty -- nap≈ô. p≈ôiƒç√≠st k p≈Øvodn√≠mu ƒç√≠slu nƒõjak√© nov√© a v√Ωsledek znovu ulo≈æit do t√© sam√© promƒõnn√©: In [162]: i = i + 2 i Out[162]: 2 Python umo≈æ≈àuje z√°pis i = i + x zkr√°tit na i += x , abychom nemuseli ps√°t i dvakr√°t: In [163]: i += 7 i Out[163]: 9 Tento zkr√°cen√Ω z√°pis funguje s libovoln√Ωm oper√°torem. In [164]: i **= 2 i Out[164]: 81 ≈òetƒõzce ≈òetƒõzce (angl. strings , v Pythonu typ str ) n√°m umo≈æ≈àuj√≠ reprezentovat text jako uspo≈ô√°danou s√©rii (≈ôetƒõzec) znak≈Ø. Liter√°ly ≈ôetƒõzc≈Ø maj√≠ r≈Øzn√© varianty, v≈ædy jsou ale ohraniƒçen√© jednoduch√Ωmi nebo dvojit√Ωmi uvozovkami. In [165]: \"hello\" Out[165]: 'hello' In [166]: 'hello' Out[166]: 'hello' In [167]: type ( \"hello\" ) Out[167]: str Chceme-li do ≈ôetƒõzce uvozen√©ho jednoduch√Ωmi (dvojit√Ωmi) uvozovkami vlo≈æit jednoduchou (dvojitou) uvozovku, je pot≈ôeba zru≈°it jej√≠ speci√°ln√≠ v√Ωznam \"ukonƒçovaƒç ≈ôetƒõzce\". K tomu slou≈æ√≠ zpƒõtn√© lom√≠tko. Angl. se ≈ô√≠k√° the backslash escapes the next character . In [168]: \" \\\" Hello, \\\" I said.\" Out[168]: '\"Hello,\" I said.' In [169]: ' \\' Hello, \\' I said.' Out[169]: \"'Hello,' I said.\" Nƒõkdy je ale jednodu≈°≈°√≠ prostƒõ jen prost≈ô√≠dat druh uvozovek, kter√© slou≈æ√≠ k delimitaci ≈ôetƒõzce, jak n√°m to ve sv√Ωch odpovƒõd√≠ch naznaƒçuje s√°m Python. Zpƒõtn√© lom√≠tko ale neslou≈æ√≠ jen k ru≈°en√≠ speci√°ln√≠ho v√Ωznamu uvozovek -- jin√Ωm znak≈Øm speci√°ln√≠ v√Ωznam naopak dod√°v√°. Slou≈æ√≠ tak sp√≠≈° jako p≈ôep√≠naƒç mezi doslovn√Ωm a speci√°ln√≠m v√Ωznamem. Speci√°ln√≠m sekvenc√≠m znak≈Ø , kter√© tvo≈ô√≠ zpƒõtn√© lom√≠tko + jeden ƒçi v√≠ce dal≈°√≠ch znak≈Ø a maj√≠ jin√Ω ne≈æ doslovn√Ω v√Ωznam, se angl. ≈ô√≠k√° escape sequences . Nap≈ô. sekvence \\n se promƒõn√≠ ve znak nov√©ho ≈ô√°dku, \\t pak ve znak tabul√°toru: In [170]: \"a \\t 1 \\n aa \\t 11\" Out[170]: 'a\\t1\\naa\\t11' Na prvn√≠ pohled to tak nevypad√°, ale to je jen kv≈Øli tomu, ≈æe n√°s Python chce na tyto speci√°ln√≠ znaky upozornit (co≈æ je dob≈ôe), tak≈æe kdy≈æ ≈ôetƒõzec jen zobrazuje , reprezentuje tyto znaky pomoc√≠ dob≈ôe ƒçiteln√Ωch speci√°ln√≠ch sekvenc√≠. Aby se nov√© ≈ô√°dky a tabul√°tory projevily, mus√≠me ≈ôetƒõzec vytisknout : In [171]: print ( \"a \\t 1 \\n aa \\t 11\" ) a 1 aa 11 Co≈æ odpov√≠d√° zam√Ω≈°len√© podobƒõ, ale je mnohem tƒõ≈æ≈°√≠ poznat, co je v ≈ôetƒõzci skuteƒçnƒõ za znaky. √öplnƒõ jin√Ω ≈ôetƒõzec m≈Ø≈æe toti≈æ vyti≈°tƒõn√Ω vypadat zcela identicky: In [172]: print ( \"a 1 \\n aa 11 \" ) a 1 aa 11 Speci√°ln√≠ sekvence existuj√≠ t√©≈æ nap≈ô. pro vlo≈æen√≠ libovoln√©ho unicodov√©ho znaku, ve form√°tu \\uXXXX nebo \\UXXXXXXXX , kde XXX... je hexadecim√°ln√≠ z√°pis po≈ôadov√©ho ƒç√≠sla znaku v unicodov√© tabulce: In [173]: \"asi nƒõjak√Ω ƒç√≠nsk√Ω znak...? \\u4e3e \" Out[173]: 'asi nƒõjak√Ω ƒç√≠nsk√Ω znak...? ‰∏æ' Po≈ôadov√© ƒç√≠slo znaku lze v Pythonu z√≠skat pomoc√≠ funkce ord ... In [174]: ord ( \"ƒç\" ) Out[174]: 269 ... takto z√≠sk√°me jeho hexadecim√°ln√≠ z√°pis... In [175]: hex ( 269 ) Out[175]: '0x10d' ... a ten pak m≈Ø≈æeme pou≈æ√≠t ve speci√°ln√≠ sekvenci: In [176]: \"tohle by mƒõlo b√Ωt ƒç: \\u010d \" Out[176]: 'tohle by mƒõlo b√Ωt ƒç: ƒç' Inverzn√≠ funkc√≠ k funkci ord je funkce chr -- d√°me j√≠ po≈ôadov√© ƒç√≠slo znaku a ona n√°m vr√°t√≠ odpov√≠daj√≠c√≠ znak. In [177]: chr ( 269 ) Out[177]: 'ƒç' In [178]: chr ( 0x10d ) Out[178]: 'ƒç' V√≠ce o znac√≠ch, Unicodu a k√≥dov√°n√≠ textu obecnƒõ viz samostatn√Ω notebook unicode.ipynb . Python pozn√°, kdy≈æ zpƒõtn√© lom√≠tko nep≈ôedch√°z√≠ znaku ƒçi znak≈Øm, s n√≠m≈æ/nimi≈æ by tvo≈ôilo speci√°ln√≠ sekvenci, a vlo≈æ√≠ na takov√©m m√≠stƒõ doslovn√© zpƒõtn√© lom√≠tko, ale z√°rove≈à n√°m jemnƒõ naznaƒç√≠, ≈æe by byl rad≈°i, kdybychom speci√°ln√≠ p≈ôep√≠nac√≠ v√Ωznam zpƒõtn√©ho lom√≠tka v liter√°lu explicitnƒõ vypnuli... pomoc√≠ zpƒõtn√©ho lom√≠tka: In [179]: \"a \\ b\" Out[179]: 'a \\\\ b' In [180]: print ( \"a \\ b\" ) a \\ b In [181]: print ( \"a \\\\ b\" ) a \\ b In [182]: \"a \\ b\" == \"a \\\\ b\" Out[182]: True Z toho mj. plyne, ≈æe kdy≈æ chceme zpƒõtn√Ωch lom√≠tek napsat v√≠c za sebou, je jich pot≈ôeba v≈ædy dvojn√°sobek: In [183]: \"a \\\\\\\\ b\" Out[183]: 'a \\\\\\\\ b' In [184]: print ( \"a \\\\\\\\ b\" ) a \\\\ b Podobn√© liter√°ly zaƒçnou brzy vypadat nep≈ôehlednƒõ. ƒålovƒõk pak zatou≈æ√≠ v≈°echny speci√°ln√≠ sekvence zru≈°it, jen aby v≈°echny znaky fungovaly jednodu≈°e doslovnƒõ. I to je mo≈æn√©, pomoc√≠ tzv. surov√Ωch ≈ôetƒõzc≈Ø (angl. raw strings ). V nich je zpƒõtn√© lom√≠tko jen dal≈°√≠ znak: In [185]: r \"\\t \\\\\\\\ \\n\" Out[185]: '\\\\t \\\\\\\\\\\\\\\\ \\\\n' In [186]: print ( r \"\\t \\\\\\\\ \\n\" ) \\t \\\\\\\\ \\n Surov√© ≈ôetƒõzce se velmi dob≈ôe hod√≠ pro pr√°ci s regul√°rn√≠mi v√Ωrazy (viz notebook regex.ipynb ), kde se lom√≠tka ƒçasto pou≈æ√≠vaj√≠ a hod√≠ se nemuset p≈ôem√Ω≈°let o tom, jestl n√°m je Python nƒõjak nepom√≠ch√°. Form√°tovac√≠ ≈ôetƒõzce (angl. format strings ) umo≈æ≈àuj√≠ pomoc√≠ slo≈æen√Ωch z√°vorek vkl√°dat do ≈ôetƒõzc≈Ø libovoln√© v√Ωrazy: In [187]: x = 1 y = 2 f \" { x } + { y } = { x + y } \" Out[187]: '1 + 2 = 3' M√≠sto p√°r jednoduch√Ωch/dvojit√Ωch uvozovek m≈Ø≈æeme ≈ôetƒõzce t√©≈æ delimitovat p√°rem trojic jednoduch√Ωch/dvojit√Ωch uvozovek. Tyto liter√°ly m≈Ø≈æou obsahovat doslovn√© znaky nov√©ho ≈ô√°dku (ne jen speci√°ln√≠ sekvenci \\n ) jednoduch√©/dvojit√© uvozovky bez lom√≠tek (jen ne t≈ôi za sebou). In [188]: \"\"\"a b c\"\"\" Out[188]: 'a\\nb\\nc' In [189]: \"\"\" a b c \"\"\" Out[189]: '\\na\\nb\\nc\\n' In [190]: s = \"\"\"\"Hello,\" I said. \"Hi,\" she replied.\"\"\" In [191]: print ( s ) \"Hello,\" I said. \"Hi,\" she replied. Jinak v nich plat√≠ stejn√° pravidla jako v bƒõ≈æn√Ωch ≈ôetƒõzc√≠ch, m≈Ø≈æou b√Ωt takt√©≈æ surov√© nebo form√°tovac√≠ nebo oboj√≠ z√°rove≈à atp. ≈òetƒõzce maj√≠ mnoho u≈æiteƒçn√Ωch metod. Nƒõkolik jich zaƒç√≠n√° na is... a informuj√≠ n√°s o obsahu ≈ôetƒõzce: In [192]: \"hello\" . islower () Out[192]: True In [193]: \"HELLO\" . isupper () Out[193]: True In [194]: \"Hello\" . istitle () Out[194]: True In [195]: \"12\" . isnumeric () Out[195]: True Jin√© vytvo≈ô√≠ nov√Ω pozmƒõnƒõn√Ω ≈ôetƒõzec: In [196]: \"hello\" . upper () Out[196]: 'HELLO' In [197]: \"hello\" . title () Out[197]: 'Hello' Metoda strip osek√° z okraj≈Ø ≈ôetƒõzce pr√°zdn√© znaky (angl. whitespace )... In [198]: \" \\n ZZZ \\n \\t \\n \" . strip () Out[198]: 'ZZZ' ... pop≈ô√≠padƒõ libovoln√© znaky, kter√© j√≠ zad√°me: In [199]: \"bbabZZZaabaaa\" . strip ( \"ab\" ) Out[199]: 'ZZZ' Metoda split nasek√° ≈ôetƒõzec na d√≠lƒç√≠ ≈ôetƒõzce na pr√°zdn√Ωch znac√≠ch... In [200]: \"a b c\" . split () Out[200]: ['a', 'b', 'c'] ... nebo na ka≈æd√©m v√Ωskytu zadan√©ho pod≈ôetƒõzce: In [201]: \"a, b, c\" . split ( \", \" ) Out[201]: ['a', 'b', 'c'] Pr√°zdn√Ωch znak≈Ø m≈Ø≈æe b√Ωt libovoln√© mno≈æstv√≠ a pokud dƒõlen√≠m vznikne pr√°zdn√Ω ≈ôetƒõzec, tak je z v√Ωsledku odebr√°n... In [202]: \" a \\n b \\t c \\n \" . split () Out[202]: ['a', 'b', 'c'] ... ale pokud zad√°me pod≈ôetƒõzec, hled√° metoda doslova a do p√≠smene p≈ôesnƒõ jeho v√Ωskyty a p≈ô√≠padn√© pr√°zdn√© ≈ôetƒõzce ve v√Ωstupu zachov√°v√°: In [203]: \" a, b, , c\" . split ( \", \" ) Out[203]: [' a', 'b', '', ' c'] Opakem metody split je metoda join , kter√° pospojuje kolekci ≈ôetƒõzc≈Ø, kterou j√≠ p≈ôed√°me jako argument: In [204]: \"-\" . join ([ \"a\" , \"b\" , \"c\" ]) Out[204]: 'a-b-c' In [205]: # i ≈ôetƒõzec lze ch√°pat jako kolekci jednoznakov√Ωch ≈ôetƒõzc≈Ø \"-\" . join ( \"abc\" ) Out[205]: 'a-b-c' D≈Øle≈æit√© je, ≈æe ≈ôetƒõzce pat≈ô√≠ k tzv. nemodifikovateln√Ωm typ≈Øm, tj. a≈• s nimi dƒõl√°me co dƒõl√°me... In [206]: s = \" a,b,c \" s Out[206]: ' a,b,c ' In [207]: s . strip () . split ( \",\" ) Out[207]: ['a', 'b', 'c'] ... p≈Øvodn√≠ ≈ôetƒõzec v≈ædy z≈Østane nedotƒçen√Ω: In [208]: s Out[208]: ' a,b,c ' V√≠ce o (ne)modifikovateln√Ωch typech viz odd. Kolekce. Je≈°tƒõ p√°r slov k funkci print . Tato funkce vytiskne v≈°echny svoje poziƒçn√≠ argumenty oddƒõlen√© mezerami a na z√°vƒõr p≈ôilep√≠ znak nov√©ho ≈ô√°dku. In [209]: print ( 1 , \"a\" , []) 1 a [] Oddƒõlovaƒç (mezeru) i ukonƒçovaƒç (nov√Ω ≈ô√°dek) si m≈Ø≈æeme upravit pomoc√≠ pojmenovan√Ωch argument≈Ø sep a end -- m≈Ø≈æe to b√Ωt libovoln√Ω jin√Ω ≈ôetƒõzec. In [210]: print ( 1 , \"a\" , [], sep = \"__ODDƒöLOVAƒå__\" , end = \"__UKONƒåOVAƒå__\" ) 1__ODDƒöLOVAƒå__a__ODDƒöLOVAƒå__[]__UKONƒåOVAƒå__ Jak to, ≈æe print um√≠ vytisknout i jin√© objekty ne≈æ jen ≈ôetƒõzce? Ve skuteƒçnosti neum√≠, jen na ka≈æd√Ω poziƒçn√≠ argument zavol√° funkci str , kter√° ho pr√°vƒõ na ≈ôetƒõzec p≈ôevede. In [211]: str ( 1 ) Out[211]: '1' In [212]: str ([]) Out[212]: '[]' U objekt≈Ø, kter√© ji≈æ ≈ôetƒõzci jsou, ≈æ√°dn√Ω p≈ôevod pochopitelnƒõ nen√≠ pot≈ôeba, str jako v√Ωsledek vr√°t√≠ nezmƒõnƒõn√Ω argument: In [213]: str ( \"a\" ) Out[213]: 'a' Kolekce Kolekce je objekt, kter√Ω obsahuje dal≈°√≠ objekty, kter√© z nƒõj lze jeden po druh√©m vyt√°hnout. Mezi z√°kladn√≠ zabudovan√© typy kolekc√≠ pat≈ô√≠: seznamy -- typ list n-tice -- typ tuple mno≈æiny -- typ set slovn√≠ky -- typ dict Sv√Ωm zp≈Øsobem m≈Ø≈æeme i na ≈ôetƒõzce nahl√≠≈æet jako na kolekce znak≈Ø -- jak uvid√≠me n√≠≈æe, jde s nimi prov√°dƒõt stejn√© typy operac√≠. Stejnƒõ jako ostatn√≠ kolekce nap≈ô. reaguj√≠ na funkci len , kter√° vrac√≠ poƒçet prvk≈Ø v kolekci... In [214]: len ( \"abc\" ) Out[214]: 3 ... nebo na oper√°tor in , kter√Ω testuje p≈ô√≠tomnost prvku v kolekci... In [215]: \"b\" in \"abc\" Out[215]: True ... nebo na funkci sorted , kter√° um√≠ na z√°kladƒõ kolekce vyrobit se≈ôazen√Ω seznam jej√≠ch prvk≈Ø: In [216]: sorted ( \"bca\" ) Out[216]: ['a', 'b', 'c'] Seznamy Kl√≠ƒçov√© vlastnosti: uspo≈ô√°danost (angl. se uspo≈ô√°dan√° kolekce ≈ôekne ordered collection ): zachov√°v√° po≈ôad√≠ prvk≈Ø prvky se mohou opakovat modifikovatelnost (angl. mutability ): seznam lze kdykoli upravovat (p≈ôid√°vat/ub√≠rat prvky) Liter√°l seznamu sest√°v√° z hranat√Ωch z√°vorek, mezi nƒõ≈æ vyp√≠≈°eme poƒç√°teƒçn√≠ prvky seznamu oddƒõlen√© ƒç√°rkami: In [217]: [ 1 , \"a\" ] Out[217]: [1, 'a'] In [218]: # opakovan√© prvky [ 1 , 1 , 1 ] Out[218]: [1, 1, 1] In [219]: # pr√°zdn√Ω seznam [] Out[219]: [] Kromƒõ liter√°lu m≈Ø≈æeme seznam vytvo≈ôit je≈°tƒõ pomoc√≠ funkce list , nap≈ô. promƒõnit ≈ôetƒõzec na seznam znak≈Ø: In [220]: list ( \"abc\" ) Out[220]: ['a', 'b', 'c'] Argument pro funkci list m≈Ø≈æe b√Ωt jak√Ωkoli iterovateln√Ω objekt, tj. objekt, ze kter√©ho lze opakovanƒõ tahat dal≈°√≠ objekty jako kr√°l√≠ky z klobouku (iterace = opakov√°n√≠). Vƒõt≈°inou se jedn√° o r≈Øzn√© typy kolekc√≠, ale ne nutnƒõ. Nap≈ô. funkce range um√≠ vytvo≈ôit objekt, z nƒõj≈æ lze postupnƒõ tahat ƒç√≠sla v zadan√©m rozpƒõt√≠: In [221]: list ( range ( 3 )) Out[221]: [0, 1, 2] In [222]: # pr√°zdn√Ω seznam list () Out[222]: [] Vytvo≈ôme si nyn√≠ seznam na hran√≠: In [223]: zv√≠≈ôata = \"koƒçka pes morƒçe slon ≈æirafa koƒçka\" . split () zv√≠≈ôata Out[223]: ['koƒçka', 'pes', 'morƒçe', 'slon', '≈æirafa', 'koƒçka'] D√≠ky tomu, ≈æe je seznam uspo≈ô√°dan√Ω, m√° ka≈æd√Ω jeho prvek po≈ôadov√©ho ƒç√≠slo, tzv. index (ƒç√≠slov√°no od nuly). Pomoc√≠ index≈Ø lze k jednotliv√Ωm prvk≈Øm seznamu p≈ôistupovat (\"uk√°zat\" si na nƒõ -- index poch√°z√≠ z latinsk√©ho slova pro ukazov√°ƒçek). T√©to operaci se ≈ô√≠k√° indexace a vypad√° tak, ≈æe za seznam nap√≠≈°eme hranat√© z√°vorky s po≈æadovan√Ωm indexem: In [224]: zv√≠≈ôata [ 0 ] Out[224]: 'koƒçka' Z√°porn√° ƒç√≠sla indexuj√≠ odzadu: In [225]: zv√≠≈ôata [ - 1 ] Out[225]: 'koƒçka' Teoreticky lze indexaci p≈ôilepit p≈ô√≠mo za liter√°l seznamu, by≈• to v praxi nen√≠ moc u≈æiteƒçn√© a z√°pis vypad√° zvl√°≈°tnƒõ: In [226]: [ \"koƒçka\" , \"pes\" , \"morƒçe\" ][ 2 ] Out[226]: 'morƒçe' Dohledat index nƒõjak√©ho prvku m≈Ø≈æeme pomoc√≠ metody index : In [227]: zv√≠≈ôata . index ( \"morƒçe\" ) Out[227]: 2 In [228]: zv√≠≈ôata . index ( \"orangutan\" ) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-228-979eb2c01602> in <module> ----> 1 zv√≠≈ôata . index ( \"orangutan\" ) ValueError : 'orangutan' is not in list Spoƒç√≠tat poƒçet v√Ωskyt≈Ø nƒõjak√©ho prvku m≈Ø≈æeme pomoc√≠ metody count : In [229]: zv√≠≈ôata . count ( \"koƒçka\" ) Out[229]: 2 Kdy≈æ chceme vyt√°hnout ze seznamu ne jeden prvek, ale podseznam, pou≈æijeme p≈ôi indexaci tzv. v√Ω≈ôez (angl. slice): In [230]: zv√≠≈ôata [ 2 : 4 ] Out[230]: ['morƒçe', 'slon'] Prvn√≠ ƒç√≠slo je index prvku, j√≠m≈æ podseznam zaƒç√≠n√°, druh√© ƒç√≠slo je index prvn√≠ho prvku, kter√Ω ji≈æ do podseznamu nepat≈ô√≠ . To je na prvn√≠ pohled mo≈æn√° trochu zvl√°≈°tn√≠, ale v kombinaci s t√≠m, ≈æe vypu≈°tƒõn√≠m prvn√≠ho/druh√©ho ƒç√≠sla dos√°hneme toho, ≈æe v√Ω≈ôez bude od zaƒç√°tku/do konce, n√°m to umo≈æ≈àuje jednodu≈°e roz≈ô√≠znout seznam na nep≈ôekr√Ωvaj√≠c√≠ se ƒç√°sti: In [231]: hranice = 3 In [232]: zv√≠≈ôata [: hranice ] Out[232]: ['koƒçka', 'pes', 'morƒçe'] In [233]: zv√≠≈ôata [ hranice :] Out[233]: ['slon', '≈æirafa', 'koƒçka'] Kdy≈æ vynech√°me obƒõ ƒç√≠sla, vytvo≈ô√≠me podseznam odpov√≠daj√≠c√≠ p≈Øvodn√≠mu seznamu: In [234]: zv√≠≈ôata [:] Out[234]: ['koƒçka', 'pes', 'morƒçe', 'slon', '≈æirafa', 'koƒçka'] In [235]: # co≈æ je to sam√© jako toto zv√≠≈ôata [ 0 : len ( zv√≠≈ôata )] Out[235]: ['koƒçka', 'pes', 'morƒçe', 'slon', '≈æirafa', 'koƒçka'] Ve v√Ω≈ôezu m≈Ø≈æeme volitelnƒõ specifikovat je≈°tƒõ t≈ôet√≠ ƒç√≠slo, kter√© stanovuje, ≈æe do seznamu m√° b√Ωt zahrnut√Ω jen ka≈æd√Ω x-t√Ω prvek (nap≈ô. ka≈æd√Ω druh√Ω): In [236]: zv√≠≈ôata [ 1 : 4 : 2 ] Out[236]: ['pes', 'slon'] Kdy≈æ je toto ƒç√≠slo z√°porn√©, m≈Ø≈æeme podseznam vy≈ô√≠znou v opaƒçn√©m smƒõru (zprava doleva): In [237]: zv√≠≈ôata [ 4 : 1 : - 2 ] Out[237]: ['≈æirafa', 'morƒçe'] In [238]: # takto lze tedy p≈ôevr√°tit po≈ôad√≠ seznamu zv√≠≈ôata [:: - 1 ] Out[238]: ['koƒçka', '≈æirafa', 'slon', 'morƒçe', 'pes', 'koƒçka'] In [239]: # ale srozumitelnƒõj≈°√≠ je asi tento ekvivalentn√≠ z√°pis # pomoc√≠ funkce reversed list ( reversed ( zv√≠≈ôata )) Out[239]: ['koƒçka', '≈æirafa', 'slon', 'morƒçe', 'pes', 'koƒçka'] Kromƒõ vy≈ôez√°v√°n√≠ lze nov√© seznamy vytv√°≈ôet i spojov√°n√≠m seznam≈Ø pomoc√≠ oper√°toru + ... In [240]: zv√≠≈ôata + [ \"mastodont\" ] Out[240]: ['koƒçka', 'pes', 'morƒçe', 'slon', '≈æirafa', 'koƒçka', 'mastodont'] ... nebo klonov√°n√≠m pomoc√≠ oper√°toru * : In [241]: 2 * zv√≠≈ôata Out[241]: ['koƒçka', 'pes', 'morƒçe', 'slon', '≈æirafa', 'koƒçka', 'koƒçka', 'pes', 'morƒçe', 'slon', '≈æirafa', 'koƒçka'] ≈ò√≠kali jsme si, ≈æe seznamy jsou modifikovateln√©, ≈æe lze prvky libovolnƒõ ub√≠rat a p≈ôid√°vat. Nicm√©nƒõ v≈°echno, co jsme zat√≠m s na≈°√≠m seznamem zv√≠≈ôata prov√°dƒõli, na nƒõm nezk≈ôivilo ani vl√°sek: In [242]: zv√≠≈ôata Out[242]: ['koƒçka', 'pes', 'morƒçe', 'slon', '≈æirafa', 'koƒçka'] Zat√≠m jsme tedy tento seznam nijak nemodifikovali, v≈°echny operace, kter√© jsme na nƒõj aplikovali, vedly k tomu, ≈æe na jeho z√°kladƒõ vznikl nov√Ω, kter√Ω z toho p≈Øvodn√≠ho nƒõjak√Ωm zp≈Øsobem vych√°zel. K modifikaci seznamu slou≈æ√≠ nƒõkter√© jeho metody. Uka≈æme si to na p≈ô√≠kladu se≈ôazov√°n√≠. Samostatn√° funkce sorted vytvo≈ô√≠ na z√°kladƒõ p≈Øvodn√≠ho seznamu nov√Ω seznam , kter√Ω obsahuje stejn√© prvky , ov≈°em se≈ôazen√© , a vr√°t√≠ ho jako v√Ωsledek : In [243]: se≈ôazen√°_zv√≠≈ôata = sorted ( zv√≠≈ôata ) se≈ôazen√°_zv√≠≈ôata Out[243]: ['koƒçka', 'koƒçka', 'morƒçe', 'pes', 'slon', '≈æirafa'] M≈Ø≈æeme si jednodu≈°e ovƒõ≈ôit, ≈æe zv√≠≈ôata a se≈ôazen√°_zv√≠≈ôata jsou dva r≈Øzn√© objekty... In [244]: zv√≠≈ôata is se≈ôazen√°_zv√≠≈ôata Out[244]: False ... a ani si nejsou rovn√© (proto≈æe z√°le≈æ√≠ na po≈ôad√≠): In [245]: zv√≠≈ôata == se≈ôazen√°_zv√≠≈ôata Out[245]: False Naopak metoda sort se≈ôad√≠ p≈Øvodn√≠ seznam , na kter√©m ji zavol√°me, a vr√°t√≠ None : In [246]: # v tuto chv√≠li poprv√© modifikujeme seznam zv√≠≈ôata... zv√≠≈ôata . sort () In [247]: # ... co≈æ si m≈Ø≈æeme jednodu≈°e ovƒõ≈ôit (zmƒõnilo se po≈ôad√≠) zv√≠≈ôata Out[247]: ['koƒçka', 'koƒçka', 'morƒçe', 'pes', 'slon', '≈æirafa'] Seznamy zv√≠≈ôata a se≈ôazen√°_zv√≠≈ôata jsou nad√°le r≈Øzn√© objekty... In [248]: zv√≠≈ôata is se≈ôazen√°_zv√≠≈ôata Out[248]: False ... ale v tuto chv√≠li u≈æ jsou si rovn√© (proto≈æe teƒè u≈æ obsahuj√≠ stejn√© prvky ve stejn√©m po≈ôad√≠): In [249]: zv√≠≈ôata == se≈ôazen√°_zv√≠≈ôata Out[249]: True Nƒõkter√© dal≈°√≠ u≈æiteƒçn√© modifikuj√≠c√≠ metody objekt≈Ø typu list : In [250]: # p≈ôid√°n√≠ jednoho prvku na konec zv√≠≈ôata . append ( \"u≈æovka\" ) zv√≠≈ôata Out[250]: ['koƒçka', 'koƒçka', 'morƒçe', 'pes', 'slon', '≈æirafa', 'u≈æovka'] In [251]: # pomoc√≠ indexace lze nahradit jeden prvek... zv√≠≈ôata [ 4 ] = \"slonice\" zv√≠≈ôata Out[251]: ['koƒçka', 'koƒçka', 'morƒçe', 'pes', 'slonice', '≈æirafa', 'u≈æovka'] In [252]: # ... nebo cel√Ω podseznam: zv√≠≈ôata [ 4 : 6 ] = [ \"slon\" , \"≈æiraf√°ƒç\" ] zv√≠≈ôata Out[252]: ['koƒçka', 'koƒçka', 'morƒçe', 'pes', 'slon', '≈æiraf√°ƒç', 'u≈æovka'] In [253]: # p≈ôid√°n√≠ jednoho prvku na libovoln√Ω index zv√≠≈ôata . insert ( 1 , \"kocour\" ) zv√≠≈ôata Out[253]: ['koƒçka', 'kocour', 'koƒçka', 'morƒçe', 'pes', 'slon', '≈æiraf√°ƒç', 'u≈æovka'] In [254]: # p≈ôid√°n√≠ cel√© kolekce prvk≈Ø na konec zv√≠≈ôata . extend ([ \"kapr\" , \"≈°tika\" , \"cand√°t\" ]) zv√≠≈ôata Out[254]: ['koƒçka', 'kocour', 'koƒçka', 'morƒçe', 'pes', 'slon', '≈æiraf√°ƒç', 'u≈æovka', 'kapr', '≈°tika', 'cand√°t'] In [255]: # odebr√°n√≠ posledn√≠ho prvku zv√≠≈ôata . pop () Out[255]: 'cand√°t' In [256]: zv√≠≈ôata Out[256]: ['koƒçka', 'kocour', 'koƒçka', 'morƒçe', 'pes', 'slon', '≈æiraf√°ƒç', 'u≈æovka', 'kapr', '≈°tika'] In [257]: # odebr√°n√≠ prvku na libovoln√©m indexu zv√≠≈ôata . pop ( 0 ) Out[257]: 'koƒçka' In [258]: zv√≠≈ôata Out[258]: ['kocour', 'koƒçka', 'morƒçe', 'pes', 'slon', '≈æiraf√°ƒç', 'u≈æovka', 'kapr', '≈°tika'] In [259]: # odebr√°n√≠ prvn√≠ho v√Ωskytu konkr√©tn√≠ho prvku zv√≠≈ôata . remove ( \"pes\" ) zv√≠≈ôata Out[259]: ['kocour', 'koƒçka', 'morƒçe', 'slon', '≈æiraf√°ƒç', 'u≈æovka', 'kapr', '≈°tika'] In [260]: # p≈ôevr√°cen√≠ po≈ôad√≠ prvk≈Ø zv√≠≈ôata . reverse () zv√≠≈ôata Out[260]: ['≈°tika', 'kapr', 'u≈æovka', '≈æiraf√°ƒç', 'slon', 'morƒçe', 'koƒçka', 'kocour'] D≈Øle≈æit√Ωm d≈Øsledkem modifikovatelnosti je, ≈æe pokud na ten sam√Ω seznam odkazuje v√≠ce promƒõnn√Ωch (= m√° v√≠ce jmen), jak√°koli modifikace se projev√≠ pod v≈°emi jm√©ny . P≈ôedstavte si analogii: pokud m√°m bratra, kter√Ω se jmenuje Jan, tak kdy≈æ se Jan nech√° ost≈ô√≠hat, bude ost≈ô√≠han√Ω i m≈Øj bratr, proto≈æe je to jedna a tat√°≈æ osoba. Podobnƒõ se seznamy: In [261]: jan = [ \"nohy\" , \"trup\" , \"hlava\" , \"vlasy\" ] bratr = jan In [262]: jan is bratr Out[262]: True In [263]: # teƒè Jana \"ost≈ô√≠h√°me\" jan . pop () Out[263]: 'vlasy' In [264]: jan Out[264]: ['nohy', 'trup', 'hlava'] In [265]: bratr Out[265]: ['nohy', 'trup', 'hlava'] Modifikace je o≈°emetn√° zejm√©na v p≈ô√≠padƒõ, kdy je skryt√° v nƒõjak√© funkci a t√Ωk√° se argument≈Ø, kter√© funkci p≈ôed√°v√°me zvenƒç√≠. Pak n√°m v≈Øbec nemus√≠ doj√≠t, ≈æe argumenty vlastnƒõ modifikujeme: In [266]: def zkr√°≈°lit ( osoba ): # osobu zkr√°≈°l√≠me tak, ≈æe ji ost≈ô√≠h√°me osoba . pop () josef = [ \"nohy\" , \"trup\" , \"hlava\" , \"vlasy\" ] In [267]: zkr√°≈°lit ( josef ) Nic nen√≠ vidƒõt, zafungovala to v≈Øbec, poda≈ôilo se n√°m Josefa zkr√°≈°lit? No pro jistotu to zkus√≠me je≈°tƒõ jednou, t√≠m p≈ôece nem≈Ø≈æeme nic zkazit, ≈æe... In [268]: zkr√°≈°lit ( josef ) Tak co teƒè? Pod√≠vejme se na Josefa... In [269]: josef Out[269]: ['nohy', 'trup'] Je≈æi≈°marja chud√°k Josef, kde m√° hlavu?! (Je to trochu f√≥rek p√≥rek, ale snad si rozum√≠me, v ƒçem tkv√≠ nebezpeƒç√≠ :) ) Z√°vƒõrem -- s modifikac√≠ je pot≈ôeba b√Ωt opatrn√Ω a radƒõji s n√≠ ≈°et≈ôit: pokud mo≈æno p≈ô√≠mo nemodifikovat argumenty funkc√≠ kde se to hod√≠, pou≈æ√≠vat m√≠sto seznam≈Ø nemodifikovateln√© n-tice n-tice Kl√≠ƒçov√© vlastnosti jsou stejn√© jako u seznam≈Ø jen s t√≠m rozd√≠lem, ≈æe jsou nemodifikovateln√© . Lze s nimi tedy prov√°dƒõt ty sam√© operace, s v√Ωjimkou tƒõch modifikuj√≠c√≠ch. Liter√°ly n-tic v z√°sadƒõ mohou sest√°vat jen z prvk≈Ø oddƒõlen√Ωch ƒç√°rkami: In [270]: 1 , \"a\" Out[270]: (1, 'a') Nicm√©nƒõ v praxi je ƒçasto pot≈ôeba obalit n-tici do kulat√Ωch z√°vorek, proto≈æe ƒç√°rka m√° jako oper√°tor velmi n√≠zkou prioritu (viz odd. ƒå√≠sla). Je to podobn√©, jako kdy≈æ mus√≠me pou≈æ√≠t z√°vorky, aby sƒç√≠t√°n√≠ probƒõhlo p≈ôed dƒõlen√≠m -- nap≈ô. 2 + 3 √ó 4 = 14 vs. (2 + 3) √ó 4 = 20. Kdo se t√≠m nechce moc zab√Ωvat, m≈Ø≈æe z√°vorky okolo n-tic pou≈æ√≠vat po≈ô√°d. In [271]: ( 1 , \"a\" ) Out[271]: (1, 'a') Specifickou syntax m√° pr√°zdn√° n-tice... In [272]: () Out[272]: () In [273]: type (()) Out[273]: tuple In [274]: len (()) Out[274]: 0 ... a jednoprvkov√° n-tice (ƒç√°rka p≈ôed uzav√≠rac√≠ z√°vorkou je povinn√°): In [275]: ( \"a\" ,) Out[275]: ('a',) Jin√Ω iterovateln√Ω objekt na n-tici promƒõn√≠me pomoc√≠ funkce tuple : In [276]: tuple ([ 1 , \"a\" ]) Out[276]: (1, 'a') In [277]: tuple ( \"abc\" ) Out[277]: ('a', 'b', 'c') In [278]: tuple ( range ( 3 )) Out[278]: (0, 1, 2) Jedn√≠m z d≈Øsledk≈Ø nemodifikovatelnosti n-tic je, ≈æe do nich lze sice indexovat... In [279]: ntice = tuple ( \"abc\" ) ntice [ 1 ] Out[279]: 'b' ... ale u≈æ nelze pomoc√≠ indexace prvky nahrazovat jin√Ωmi: In [280]: ntice [ 1 ] = \"Z\" --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-280-53c8f0165f7f> in <module> ----> 1 ntice [ 1 ] = \"Z\" TypeError : 'tuple' object does not support item assignment Podobnƒõ ≈ôetƒõzce, kter√© jsou tak√© nemodifikovateln√©: In [281]: ≈ôetƒõzec = \"abc\" ≈ôetƒõzec [ 1 ] Out[281]: 'b' In [282]: ≈ôetƒõzec [ 1 ] = \"Z\" --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-282-a215aaf9569e> in <module> ----> 1 ≈ôetƒõzec [ 1 ] = \"Z\" TypeError : 'str' object does not support item assignment Mno≈æiny Kl√≠ƒçov√© vlastnosti: neuspo≈ô√°danost : prvky nemaj√≠ dan√© po≈ôad√≠ prvky se nesm√≠ opakovat modifikovatelnost Liter√°ly vypadaj√≠ n√°sledovnƒõ: In [283]: { 1 , \"a\" } Out[283]: {1, 'a'} Ov≈°em pozor, {} nen√≠ pr√°zdn√° mno≈æina, ale pr√°zdn√Ω slovn√≠k (viz n√≠≈æe)! Pr√°zdnou mno≈æinu z√≠sk√°me pomoc√≠ funkce set ... In [284]: set () Out[284]: set() ... kter√° n√°m poslou≈æ√≠ i k p≈ôevodu jin√© kolekce na mno≈æinu: In [285]: set ( \"koƒçiƒçka\" ) Out[285]: {'a', 'i', 'k', 'o', 'ƒç'} Prvky v mno≈æinƒõ se nesmƒõj√≠ opakovat v tom smyslu, ≈æe do mno≈æiny nelze vlo≈æit dva prvky a a b , o nich≈æ plat√≠, ≈æe a == b . Nicm√©nƒõ nen√≠ t≈ôeba se b√°t, kdy≈æ se o to pokus√≠me, nenastane chyba, mno≈æina prostƒõ druh√Ω pokus jednodu≈°e ignoruje: In [286]: { 1 , 2 , 1 } Out[286]: {1, 2} In [287]: set ([ 1 , 2 , 1 ]) Out[287]: {1, 2} Vytvo≈ôme si dvƒõ mno≈æiny na hran√≠: In [288]: set1 = { 1 , 2 , 3 } set2 = { 2 , 3 , 4 } D≈Øle≈æit√Ωm d≈Øsledkem neuspo≈ô√°danosti je, ≈æe do mno≈æiny nelze indexovat -- pt√°t se po \"prvn√≠m\" prvku mno≈æiny ned√°v√° smysl: In [289]: set1 [ 0 ] --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-289-c38563f1af7a> in <module> ----> 1 set1 [ 0 ] TypeError : 'set' object is not subscriptable Mno≈æiny podporuj√≠ r≈Øzn√© mno≈æinov√© operace. Nƒõkter√© jsou nemodifikuj√≠c√≠... In [290]: # sjednocen√≠ set1 . union ( set2 ) Out[290]: {1, 2, 3, 4} In [291]: # t√©≈æ mo≈æno pomoc√≠ oper√°toru | set1 | set2 Out[291]: {1, 2, 3, 4} In [292]: # pr≈Ønik set1 . intersection ( set2 ) Out[292]: {2, 3} In [293]: # t√©≈æ mo≈æno pomoc√≠ oper√°toru & set1 & set2 Out[293]: {2, 3} In [294]: # rozd√≠l set1 . difference ( set2 ) Out[294]: {1} In [295]: # t√©≈æ mo≈æno pomoc√≠ oper√°toru - set1 - set2 Out[295]: {1} In [296]: # vztahy mezi mno≈æinami set1 . issubset ( set2 ) Out[296]: False ... jin√© jsou modifikuj√≠c√≠: In [297]: # p≈ôid√°n√≠ prvku set1 . add ( 4 ) set1 Out[297]: {1, 2, 3, 4} In [298]: # zde k modifikaci nedojde, proto≈æe set1 u≈æ prvek 4 obsahuje set1 . add ( 4 ) set1 Out[298]: {1, 2, 3, 4} In [299]: # odebr√°n√≠ prvku set1 . remove ( 4 ) set1 Out[299]: {1, 2, 3} In [300]: # p≈ôid√°n√≠ v√≠ce prvk≈Ø, ov≈°em znovu pochopitelnƒõ s vy≈ôazen√≠m duplicit set1 . update ([ 0 , 1 , 2 ]) set1 Out[300]: {0, 1, 2, 3} In [301]: # pr≈Ønik, p≈ôiƒçem≈æ v√Ωsledek operace se ulo≈æ√≠ do p≈Øvodn√≠ mno≈æiny set1 set1 . intersection_update ( set2 ) set1 Out[301]: {2, 3} Teƒè u≈æ tedy plat√≠: In [302]: set1 . issubset ( set2 ) Out[302]: True In [303]: # t√©≈æ mo≈æno pomoc√≠ oper√°toru < (\"men≈°√≠ ne≈æ\") set1 < set2 Out[303]: True In [304]: set2 . issuperset ( set1 ) Out[304]: True In [305]: # t√©≈æ mo≈æno pomoc√≠ oper√°toru > (\"vƒõt≈°√≠ ne≈æ\") set2 > set1 Out[305]: True Kromƒõ toho, ≈æe n√°m mno≈æiny dob≈ôe poslou≈æ√≠, kdy≈æ chceme kolekci s duplicitami zredukovat na kolekci unik√°tn√≠ch objekt≈Ø (nap≈ô. kdy≈æ chceme seznam token≈Ø v textu p≈ôev√©st na mno≈æinu typ≈Ø ), maj√≠ je≈°tƒõ jednu v√Ωhodu: d√≠ky tomu, ≈æe prvky nemus√≠ respektovat po≈ôad√≠ zadan√© u≈æivatelem, m≈Ø≈æou b√Ωt internƒõ uspo≈ô√°dan√© tak, aby umo≈æ≈àovaly velmi rychle zjistit, zda mno≈æina dan√Ω prvek obsahuje ƒçi ne. V p≈ô√≠padƒõ seznamu je pot≈ôeba ho proch√°zet prvek po prvku, od zaƒç√°tku do konce, abychom zjistili, zda dan√Ω prvek obsahuje ƒçi ne. ƒå√≠m je seznam del≈°√≠, a ƒç√≠m d√°l v nƒõm prvek je (nebo pokud v nƒõm prvek v≈Øbec nen√≠), t√≠m d√©le to trv√°: In [306]: # seznam prvn√≠ho milionu ƒç√≠sel seznam = list ( range ( 1_000_000 )) In [307]: # Python mus√≠ seznam proj√≠t cel√Ω, prvek po prvku, aby zjistil, # ≈æe ≈ôetƒõzec \"a\" v na≈°em seznamu prvn√≠ho milionu ƒç√≠sel nen√≠ \"a\" in seznam Out[307]: False In [308]: %% timeit # speci√°ln√≠ direktiva %%timeit nen√≠ p≈ô√≠mo souƒç√°st√≠ Pythonu, # poskytuje ji prost≈ôed√≠ Jupyter; slou≈æ√≠ k tomu, ≈æe spust√≠ # danou bu≈àku opakovanƒõ, poka≈æd√© zmƒõ≈ô√≠, jak dlouho trv√° # bu≈àku vykonat, a pak n√°m uk√°≈æe pr≈Ømƒõrn√Ω ƒças \"a\" in seznam 8.28 ms ¬± 26.2 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each) Oproti tomu v mno≈æinƒõ lze d√≠ky jej√≠mu intern√≠mu uspo≈ô√°d√°n√≠ dohledat prvek velmi rychle: In [309]: mno≈æina = set ( seznam ) In [310]: \"a\" in mno≈æina Out[310]: False In [311]: %% timeit \"a\" in mno≈æina 26.3 ns ¬± 0.0176 ns per loop (mean ¬± std. dev. of 7 runs, 10000000 loops each) U seznamu trv√° na≈°e operace v ≈ô√°du des√≠tek milisekund (10‚Åª¬≥ s), kde≈æto u mno≈æiny jsou to des√≠tky nanosekund (10‚Åª‚Åπ s) -- je tedy milionkr√°t rychlej≈°√≠! Nemodifikovateln√Ωm protƒõj≈°kem typu set je typ frozenset . Slovn√≠ky Slovn√≠ky se od ostatn√≠ch kolekc√≠, kter√© jsme doposud potkali, li≈°√≠ v tom, ≈æe obsahuj√≠ dva typy prvk≈Ø: kl√≠ƒçe (angl. keys ), podle nich≈æ se ve slovn√≠ku hled√° a hodnoty (angl. values ), kter√© pod kl√≠ƒçi dohled√°me Je to podobnƒõ jako s jazykov√Ωmi slovn√≠ky (proto se tak v Pythonu jmenuj√≠): kdy≈æ ve slovn√≠ku hled√°m slovo \"koƒçka\", tak \"koƒçka\" je kl√≠ƒç, a definice, kterou najdu, je jeho odpov√≠daj√≠c√≠ hodnota. Stƒõ≈æejn√≠ vlastnosti slovn√≠k≈Ø: neuspo≈ô√°danost : prvky nemaj√≠ dan√© po≈ôad√≠ kl√≠ƒçe se nesm√≠ opakovat , ale hodnoty klidnƒõ m≈Ø≈æou modifikovatelnost Liter√°ly vypadaj√≠ n√°sledovnƒõ: In [312]: { \"a\" : 1 , \"b\" : 2 } Out[312]: {'a': 1, 'b': 2} Prvek p≈ôed dvojteƒçkou je v≈ædy kl√≠ƒç, za dvojteƒçkou pak jeho odpov√≠daj√≠c√≠ hodnota. Kdy≈æ stejn√Ω kl√≠ƒç specifikujeme v√≠ckr√°t, nenastane chyba, jen posledn√≠ asociovan√° hodnota p≈ôema≈æe ty p≈ôedchoz√≠: In [313]: { \"a\" : 1 , \"a\" : 2 , \"a\" : 3 } Out[313]: {'a': 3} In [314]: # pr√°zdn√Ω slovn√≠k {} Out[314]: {} In [315]: type ({}) Out[315]: dict Slovn√≠ky lze vytv√°≈ôet i pomoc√≠ funkce dict , a to v z√°sadƒõ dvƒõma zp≈Øsoby. Prvn√≠ mo≈ænost je, ≈æe do slovn√≠ku \"nasypeme\" kolekci sest√°vaj√≠c√≠ z dvojic objekt≈Ø, z nich≈æ prvn√≠ v≈ædy bude br√°n jako kl√≠ƒç a druh√Ω jako jeho asociovan√° hodnota: In [316]: # nap≈ô. seznam n-tic o dvou polo≈æk√°ch... dict ([( \"a\" , 1 ), ( \"b\" , 2 )]) Out[316]: {'a': 1, 'b': 2} In [317]: # ... nebo klidnƒõ seznam seznam≈Ø o dvou polo≈æk√°ch... dict ([[ \"a\" , 1 ], [ \"b\" , 2 ]]) Out[317]: {'a': 1, 'b': 2} In [318]: # ... nebo t≈ôeba n-tice ≈ôetƒõzc≈Ø o dvou znac√≠ch atp. dict (( \"ab\" , \"cd\" )) Out[318]: {'a': 'b', 'c': 'd'} Druh√° mo≈ænost je, ≈æe vyu≈æijeme pojmenovan√© argumenty -- jm√©na argument≈Ø pak skonƒç√≠ jako kl√≠ƒçe, argumenty samotn√© jako hodnoty: In [319]: dict ( a = 1 , b = 2 ) Out[319]: {'a': 1, 'b': 2} Vytvo≈ôme si slovn√≠k na hran√≠: In [320]: slovn√≠k = { \"pes\" : \"nejlep≈°√≠ p≈ô√≠tel ƒçlovƒõka\" , \"koƒçka\" : \"vypoƒç√≠tav√° potvora\" } Slovn√≠ky jsou sice stejnƒõ jako mno≈æiny neuspo≈ô√°dan√©, ale indexaci na rozd√≠l od nich podporuj√≠ . Jen se jako index nepou≈æ√≠v√° po≈ôadov√© ƒç√≠slo (proto≈æe ≈æ√°dn√© po≈ôad√≠ neexistuje), ale kl√≠ƒç : In [321]: slovn√≠k [ \"koƒçka\" ] Out[321]: 'vypoƒç√≠tav√° potvora' P≈ôes metodu keys se dostaneme ke v≈°em kl√≠ƒç≈Øm ve slovn√≠ku... In [322]: slovn√≠k . keys () Out[322]: dict_keys(['pes', 'koƒçka']) ... p≈ôes metodu values k hodnot√°m... In [323]: slovn√≠k . values () Out[323]: dict_values(['nejlep≈°√≠ p≈ô√≠tel ƒçlovƒõka', 'vypoƒç√≠tav√° potvora']) ... a p≈ôes metodu items k uspo≈ô√°dan√Ωm dvojic√≠m (kl√≠ƒç, hodnota) : In [324]: slovn√≠k . items () Out[324]: dict_items([('pes', 'nejlep≈°√≠ p≈ô√≠tel ƒçlovƒõka'), ('koƒçka', 'vypoƒç√≠tav√° potvora')]) Kdy≈æ se pokus√≠me indexovat podle kl√≠ƒçe, kter√Ω ve slovn√≠ku nen√≠, Python zkolabuje: In [325]: slovn√≠k [ \"morƒçe\" ] --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-325-21eeb4f972e5> in <module> ----> 1 slovn√≠k [ \"morƒçe\" ] KeyError : 'morƒçe' Abychom se kolapsu (a t√≠m i zastaven√≠ bƒõhu programu) vyhnuli, m≈Ø≈æeme pou≈æ√≠t metodu get , kter√° v p≈ô√≠padƒõ absence kl√≠ƒçe vr√°t√≠ m√≠sto hodnoty ve slovn√≠ku nƒõjak√Ω jin√Ω objekt (defaultnƒõ je to None , ale m≈Ø≈æeme specifikovat libovoln√Ω objekt). In [326]: slovn√≠k . get ( \"pes\" ) Out[326]: 'nejlep≈°√≠ p≈ô√≠tel ƒçlovƒõka' In [327]: slovn√≠k . get ( \"morƒçe\" ) In [328]: slovn√≠k . get ( \"morƒçe\" , \"MORƒåE VE SLOVN√çKU NEN√ç!\" ) Out[328]: 'MORƒåE VE SLOVN√çKU NEN√ç!' D√≠ky tomu, ≈æe jsou slovn√≠ky modifikovateln√©, m≈Ø≈æeme hodnoty odpov√≠daj√≠c√≠ jednotliv√Ωm kl√≠ƒç≈Øm libovolnƒõ mƒõnit... In [329]: slovn√≠k [ \"koƒçka\" ] = \"chlupat√Ω mazl√≠ƒçek\" slovn√≠k Out[329]: {'pes': 'nejlep≈°√≠ p≈ô√≠tel ƒçlovƒõka', 'koƒçka': 'chlupat√Ω mazl√≠ƒçek'} ... nebo p≈ôid√°vat nov√©: In [330]: slovn√≠k [ \"rybiƒçka\" ] = \"nƒõm√° slizk√° tv√°≈ô\" slovn√≠k Out[330]: {'pes': 'nejlep≈°√≠ p≈ô√≠tel ƒçlovƒõka', 'koƒçka': 'chlupat√Ω mazl√≠ƒçek', 'rybiƒçka': 'nƒõm√° slizk√° tv√°≈ô'} V prvn√≠m p≈ô√≠padƒõ jsme p≈ôepsali p≈Øvodn√≠ hodnotu odpov√≠daj√≠c√≠ kl√≠ƒçi \"koƒçka\" , ve druh√©m jsme p≈ôidali nov√Ω kl√≠ƒç \"rybiƒçka\" a k nƒõmu odpov√≠daj√≠c√≠ hodnotu. Syntax (z√°pis) ale vypad√° v obou p≈ô√≠padech stejnƒõ. Obƒças pot≈ôebujeme zajistit, aby za ≈æ√°dnou cenu nedo≈°lo k p≈ôeps√°n√≠ existuj√≠c√≠ hodnoty (viz prvn√≠ p≈ô√≠pad v√Ω≈°e). K tomu poslou≈æ√≠ metoda setdefault -- pokud kl√≠ƒç u≈æ ve slovn√≠ku existuje, p≈Øvodn√≠ hodnotu nezmƒõn√≠, jen ji vr√°t√≠... In [331]: slovn√≠k . setdefault ( \"koƒçka\" , \"tuhle novou hodnotu do slovn√≠ku neprocpu :(\" ) Out[331]: 'chlupat√Ω mazl√≠ƒçek' In [332]: slovn√≠k Out[332]: {'pes': 'nejlep≈°√≠ p≈ô√≠tel ƒçlovƒõka', 'koƒçka': 'chlupat√Ω mazl√≠ƒçek', 'rybiƒçka': 'nƒõm√° slizk√° tv√°≈ô'} ... ale pokud neexistuje, kl√≠ƒç a odpov√≠daj√≠c√≠ hodnotu do slovn√≠ku p≈ôid√° (a hodnotu taky vr√°t√≠): In [333]: slovn√≠k . setdefault ( \"morƒçe\" , \"ale tuhle procpu :)\" ) Out[333]: 'ale tuhle procpu :)' In [334]: slovn√≠k Out[334]: {'pes': 'nejlep≈°√≠ p≈ô√≠tel ƒçlovƒõka', 'koƒçka': 'chlupat√Ω mazl√≠ƒçek', 'rybiƒçka': 'nƒõm√° slizk√° tv√°≈ô', 'morƒçe': 'ale tuhle procpu :)'} Metoda setdefault tedy funguje podobnƒõ jako metoda get s t√≠m rozd√≠lem, ≈æe v p≈ô√≠padƒõ nep≈ô√≠tomnosti kl√≠ƒçe ve slovn√≠ku defaultn√≠ hodnotu nejen vr√°t√≠, ale i ulo≈æ√≠ do slovn√≠ku. Metoda update , kter√° p≈ôid√°v√° nov√© prvky do slovn√≠ku / upravuje st√°vaj√≠c√≠, nab√≠z√≠ co do argument≈Ø, kter√© zvl√°dne zpracovat, podobn√© mo≈ænosti jako samotn√° funkce dict : In [335]: # libovoln√° kolekce dvojic prvk≈Ø... slovn√≠k . update ([( \"a\" , 1 ), ( \"b\" , 2 )]) slovn√≠k Out[335]: {'pes': 'nejlep≈°√≠ p≈ô√≠tel ƒçlovƒõka', 'koƒçka': 'chlupat√Ω mazl√≠ƒçek', 'rybiƒçka': 'nƒõm√° slizk√° tv√°≈ô', 'morƒçe': 'ale tuhle procpu :)', 'a': 1, 'b': 2} In [336]: slovn√≠k . update (( \"ab\" , \"cd\" )) slovn√≠k Out[336]: {'pes': 'nejlep≈°√≠ p≈ô√≠tel ƒçlovƒõka', 'koƒçka': 'chlupat√Ω mazl√≠ƒçek', 'rybiƒçka': 'nƒõm√° slizk√° tv√°≈ô', 'morƒçe': 'ale tuhle procpu :)', 'a': 'b', 'b': 2, 'c': 'd'} In [337]: # ... jin√Ω slovn√≠k... slovn√≠k . update ({ \"b\" : \"z\" }) slovn√≠k Out[337]: {'pes': 'nejlep≈°√≠ p≈ô√≠tel ƒçlovƒõka', 'koƒçka': 'chlupat√Ω mazl√≠ƒçek', 'rybiƒçka': 'nƒõm√° slizk√° tv√°≈ô', 'morƒçe': 'ale tuhle procpu :)', 'a': 'b', 'b': 'z', 'c': 'd'} In [338]: # ... nebo pojmenovan√© argumenty: slovn√≠k . update ( ≈æralok = \"drav√° mo≈ôsk√° paryba\" , k≈Ø≈à = \"tam nƒõkde v pastvin√°ch\" ) slovn√≠k Out[338]: {'pes': 'nejlep≈°√≠ p≈ô√≠tel ƒçlovƒõka', 'koƒçka': 'chlupat√Ω mazl√≠ƒçek', 'rybiƒçka': 'nƒõm√° slizk√° tv√°≈ô', 'morƒçe': 'ale tuhle procpu :)', 'a': 'b', 'b': 'z', 'c': 'd', '≈æralok': 'drav√° mo≈ôsk√° paryba', 'k≈Ø≈à': 'tam nƒõkde v pastvin√°ch'} Odeb√≠rat prvky ze slovn√≠ku lze r≈Øzn√Ωmi zp≈Øsoby, podle toho, zda chceme odebrat konkr√©tn√≠ kl√≠ƒç a chceme je≈°tƒõ pracovat s jeho hodnotou... In [339]: # metoda pop sma≈æe prvek podle kl√≠ƒçe a vr√°t√≠ odpov√≠daj√≠c√≠ hodnotu slovn√≠k . pop ( \"a\" ) Out[339]: 'b' In [340]: slovn√≠k Out[340]: {'pes': 'nejlep≈°√≠ p≈ô√≠tel ƒçlovƒõka', 'koƒçka': 'chlupat√Ω mazl√≠ƒçek', 'rybiƒçka': 'nƒõm√° slizk√° tv√°≈ô', 'morƒçe': 'ale tuhle procpu :)', 'b': 'z', 'c': 'd', '≈æralok': 'drav√° mo≈ôsk√° paryba', 'k≈Ø≈à': 'tam nƒõkde v pastvin√°ch'} ... nebo zda hodnotu k niƒçemu nepot≈ôebujeme... In [341]: # oper√°tor del sma≈æe prvek podle kl√≠ƒçe a nevr√°t√≠ nic del slovn√≠k [ \"b\" ] In [342]: slovn√≠k Out[342]: {'pes': 'nejlep≈°√≠ p≈ô√≠tel ƒçlovƒõka', 'koƒçka': 'chlupat√Ω mazl√≠ƒçek', 'rybiƒçka': 'nƒõm√° slizk√° tv√°≈ô', 'morƒçe': 'ale tuhle procpu :)', 'c': 'd', '≈æralok': 'drav√° mo≈ôsk√° paryba', 'k≈Ø≈à': 'tam nƒõkde v pastvin√°ch'} ... nebo zda je n√°m jedno, kter√Ω prvek odebereme (nech√°me Python vybrat za n√°s): In [343]: # metoda popitem odebere nƒõjak√Ω prvek (nev√≠me p≈ôedem jak√Ω) # a vr√°t√≠ kl√≠ƒç a hodnotu jako n-tici slovn√≠k . popitem () Out[343]: ('k≈Ø≈à', 'tam nƒõkde v pastvin√°ch') In [344]: slovn√≠k Out[344]: {'pes': 'nejlep≈°√≠ p≈ô√≠tel ƒçlovƒõka', 'koƒçka': 'chlupat√Ω mazl√≠ƒçek', 'rybiƒçka': 'nƒõm√° slizk√° tv√°≈ô', 'morƒçe': 'ale tuhle procpu :)', 'c': 'd', '≈æralok': 'drav√° mo≈ôsk√° paryba'} D√≠ky neuspo≈ô√°danosti a z√°kazu opakov√°n√≠ je dohled√°v√°n√≠ kl√≠ƒç≈Ø ve slovn√≠ku velmi rychl√© (stejnƒõ jako u mno≈æin) a zapisuje se velmi jednodu≈°e: In [345]: \"koƒçka\" in slovn√≠k Out[345]: True Hodnoty ale ve slovn√≠ku takhle jednodu≈°e nedohled√°me: In [346]: \"nejlep≈°√≠ p≈ô√≠tel ƒçlovƒõka\" in slovn√≠k Out[346]: False Je pot≈ôeba je proj√≠t jednu po druh√© pomoc√≠ metody values , co≈æ na rozd√≠l od kl√≠ƒç≈Ø trv√° stejnƒõ dlouho jako u seznamu (asi jako kdybychom v jazykov√©m slovn√≠ku hledali tak, ≈æe bychom proƒç√≠tali jen definice): In [347]: \"nejlep≈°√≠ p≈ô√≠tel ƒçlovƒõka\" in slovn√≠k . values () Out[347]: True M≈Ø≈æeme si to ovƒõ≈ôit na libovoln√©m velk√©m slovn√≠ku. K vytvo≈ôen√≠ takov√©ho velk√©ho testovac√≠ho slovn√≠ku m≈Ø≈æeme pou≈æ√≠t zabudovanou funkci zip , kter√° \"sezipuje\" dohromady prvky nƒõkolika iterovateln√Ωch objekt≈Ø. Zn√≠ to slo≈æitƒõ, ale kdy≈æ se na ni pod√≠v√°me v akci, je to mysl√≠m jednoduch√©: In [348]: # v≈°e obal√≠me do funkce list, aby se v√Ωsledn√© \"sezipovan√©\" prvky # ulo≈æily do seznamu list ( zip ( \"abc\" , [ 1 , 2 , 3 ])) Out[348]: [('a', 1), ('b', 2), ('c', 3)] Pomoc√≠ funkce zip jednodu≈°e m≈Ø≈æeme vytvo≈ôit slovn√≠k, v nƒõm≈æ namapujeme prvn√≠ milion ƒç√≠sel na sebe sama, ve stylu... In [349]: dict ( zip ( range ( 3 ), range ( 3 ))) Out[349]: {0: 0, 1: 1, 2: 2} ... akor√°t vƒõt≈°√≠. Takov√Ω slovn√≠k samoz≈ôejmƒõ nen√≠ k niƒçemu u≈æiteƒçn√Ω, jen je velk√Ω, tak≈æe dob≈ôe poslou≈æ√≠ k ilustraci rozd√≠lu rychlosti v hled√°n√≠ mezi kl√≠ƒçi a hodnotami. In [350]: libovoln√Ω_velk√Ω_slovn√≠k = dict ( zip ( range ( 1_000_000 ), range ( 1_000_000 ))) Hled√°n√≠ v hodnot√°ch: In [351]: \"a\" in libovoln√Ω_velk√Ω_slovn√≠k . values () Out[351]: False In [352]: %% timeit \"a\" in libovoln√Ω_velk√Ω_slovn√≠k.values() 11.4 ms ¬± 243 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each) Hled√°n√≠ v kl√≠ƒç√≠ch: In [353]: \"a\" in libovoln√Ω_velk√Ω_slovn√≠k Out[353]: False In [354]: %% timeit \"a\" in libovoln√Ω_velk√Ω_slovn√≠k 23.6 ns ¬± 0.00474 ns per loop (mean ¬± std. dev. of 7 runs, 10000000 loops each) Znovu pozorujeme rozd√≠l ≈°esti ≈ô√°d≈Ø: hled√°n√≠ v kl√≠ƒç√≠ch je milionkr√°t rychlej≈°√≠ ne≈æ hled√°n√≠ v hodnot√°ch. A kdy≈æ u≈æ m√°me slovn√≠ku pln√© zuby, m≈Ø≈æeme jeho obsah vymazat a zaƒç√≠t nanovo: In [355]: libovoln√Ω_velk√Ω_slovn√≠k . clear () In [356]: libovoln√Ω_velk√Ω_slovn√≠k Out[356]: {} Vno≈ôen√© kolekce Jak u≈æ jsme ostatnƒõ v nƒõkter√Ωch p≈ô√≠kladech v√Ω≈°e naznaƒçili, kolekce lze do sebe libovolnƒõ zano≈ôovat, ƒç√≠m≈æ m≈Ø≈æeme reprezentovat r≈Øzn√© slo≈æit√© strukturn√≠ vztahy. Liter√°ln√≠ z√°pis vno≈ôen√Ωch kolekc√≠ je celkem intuitivn√≠: In [357]: slovn√≠k = { \"hesla\" : { \"koƒçka\" : { \"definice\" : \"chlupat√Ω mazl√≠ƒçek\" , \"p≈ô√≠klad\" : \"Na oknƒõ sedƒõla koƒçka...\" , }, \"pes\" : { \"definice\" : \"nejlep≈°√≠ p≈ô√≠tel ƒçlovƒõka\" , \"p≈ô√≠klad\" : \"... a venku ≈°tƒõkal pes.\" , }, }, \"auto≈ôi\" : [ { \"jm√©no\" : \"John\" , \"p≈ô√≠jmen√≠\" : \"Doe\" }, { \"jm√©no\" : \"Jane\" , \"p≈ô√≠jmen√≠\" : \"Doe\" }, ], \"datum\" : ( 2018 , 11 , 7 ), } Naproti tomu indexace do vno≈ôen√Ωch kolekc√≠ obƒças lidem ƒçin√≠ p≈ôi prvn√≠m setk√°n√≠ pot√≠≈æe. Chceme-li nap≈ô. vyt√°hnout ze slovn√≠ku definici koƒçky, nƒõkdo m√° tendenci zkou≈°et n√°sleduj√≠c√≠ z√°pis, kter√Ω sv√Ωm vno≈ôen√≠m zrcadl√≠ vno≈ôen√≠ liter√°lu: In [358]: slovn√≠k [ \"hesla\" [ \"koƒçka\" ]] <>:1: SyntaxWarning: str indices must be integers or slices, not str; perhaps you missed a comma? <>:1: SyntaxWarning: str indices must be integers or slices, not str; perhaps you missed a comma? <ipython-input-358-c2f5af94dca9>:1: SyntaxWarning: str indices must be integers or slices, not str; perhaps you missed a comma? slovn√≠k[\"hesla\"[\"koƒçka\"]] --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-358-c2f5af94dca9> in <module> ----> 1 slovn√≠k [ \"hesla\" [ \"koƒçka\" ] ] TypeError : string indices must be integers Chybov√° hl√°≈°ka zn√≠ trochu krypticky, ale m≈Ø≈æeme z n√≠ odvodit, ≈æe se Python z≈ôejmƒõ sna≈æ√≠ indexovat do nƒõjak√©ho ≈ôetƒõzce (tj. vyt√°hnout znak z ≈ôetƒõzce), co≈æ jsme rozhodnƒõ nezam√Ω≈°leli, tak≈æe z√°pis mus√≠ b√Ωt ≈°patnƒõ. Co se tedy dƒõje? Pojƒème si v√Ωraz rozebrat tak, jak ho vid√≠ Python. Nejd≈ô√≠v vid√≠ promƒõnnou slovn√≠k , za n√≠ hranat√© z√°vorky. Usoud√≠ tedy (spr√°vnƒõ), ≈æe se sna≈æ√≠me p≈ôistoupit k nƒõjak√©mu kl√≠ƒçi ve slovn√≠ku. K jak√©mu kl√≠ƒçi? To je pot≈ôeba zjistit na z√°kladƒõ obsahu hranat√Ωch z√°vorek. Jin√Ωmi slovy, Python v tuto chv√≠li vid√≠ slovn√≠k[X] , kde X je z√°stupn√Ω znak pro kl√≠ƒç, kter√Ω chceme, aby ve slovn√≠ku dohledal. Dob≈ôe, tak d√°l. Abych mohl X dohledat, uva≈æuje Python, mus√≠m nejd≈ô√≠v zjistit, co je zaƒç. Hm, podle v≈°eho m√°m za X dosadit v√Ωraz \"hesla\"[Y] . Ten mus√≠m tedy vyhodnotit jako prvn√≠, abych mohl posl√©ze vyhodnotit v√Ωraz slovn√≠k[X] . Co je \"hesla\"[Y] za v√Ωraz? Je to v√Ωraz, v nƒõm≈æ m√°me ≈ôetƒõzec \"hesla\" , a pomoc√≠ indexace se z nƒõj sna≈æ√≠me vyt√°hnout znak, kter√Ω odpov√≠d√° indexu Y . Skvƒõle, tetel√≠ se Python, u≈æ tedy staƒç√≠ jen zjistit, jak√° je hodnota indexu Y , a m≈Ø≈æu zaƒç√≠t cel√Ω v√Ωraz vyhodnocovat, pƒõknƒõ zvnit≈ôka k vnƒõj≈°ku. Jen≈æe ouha, Y by mƒõlo b√Ωt po≈ôadov√© ƒç√≠slo po≈æadovan√©ho znaku, ale m√≠sto toho je to ≈ôetƒõzec \"koƒçka\" . Chud√°k Python nev√≠, co to znamen√°, vyt√°hnout \"koƒçka\" -t√Ω znak z ≈ôetƒõzce \"hesla\" , a tak to vzd√° a jen si postƒõ≈æuje, ≈æe indexy do ≈ôetƒõzc≈Ø by mƒõly b√Ωt cel√° ƒç√≠sla (\"string indices must be integers\"). Jin√Ωmi slovy, vyhodnocov√°n√≠ cel√©ho v√Ωrazu sel≈æe na tomto podv√Ωrazu: In [359]: \"hesla\" [ \"koƒçka\" ] <>:1: SyntaxWarning: str indices must be integers or slices, not str; perhaps you missed a comma? <>:1: SyntaxWarning: str indices must be integers or slices, not str; perhaps you missed a comma? <ipython-input-359-acc1bf557eb4>:1: SyntaxWarning: str indices must be integers or slices, not str; perhaps you missed a comma? \"hesla\"[\"koƒçka\"] --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-359-acc1bf557eb4> in <module> ----> 1 \"hesla\" [ \"koƒçka\" ] TypeError : string indices must be integers Jak vidno, chyba je stejn√°. Jak tedy na to? V≈ædycky je mo≈æn√© rozdƒõlit indexaci do vno≈ôen√© kolekce na v√≠c jednoduch√Ωch indexac√≠ s pou≈æit√≠m pomocn√Ωch promƒõnn√Ωch: In [360]: hesla = slovn√≠k [ \"hesla\" ] koƒçka = hesla [ \"koƒçka\" ] koƒçka Out[360]: {'definice': 'chlupat√Ω mazl√≠ƒçek', 'p≈ô√≠klad': 'Na oknƒõ sedƒõla koƒçka...'} Z v√Ω≈°e uveden√©ho z√°pisu ale plyne, ≈æe se mezipromƒõnn√© hesla m≈Ø≈æeme zbavit pomoc√≠ dosazov√°n√≠ . Zaƒçnƒõme od promƒõnn√© koƒçka , kter√° obsahuje zam√Ω≈°len√Ω v√Ωsledek (slovn√≠kov√© heslo pro koƒçku; mezery slou≈æ√≠ jen ke zv√Ωraznƒõn√≠ toho, jak prob√≠h√° dosazov√°n√≠): In [361]: koƒçka Out[361]: {'definice': 'chlupat√Ω mazl√≠ƒçek', 'p≈ô√≠klad': 'Na oknƒõ sedƒõla koƒçka...'} Za v√Ωraz koƒçka lze dosadit v√Ωraz hesla[\"koƒçka\"] (proto≈æe koƒçka = hesla[\"koƒçka\"] ): In [362]: hesla [ \"koƒçka\" ] Out[362]: {'definice': 'chlupat√Ω mazl√≠ƒçek', 'p≈ô√≠klad': 'Na oknƒõ sedƒõla koƒçka...'} V√Ωbornƒõ, v√Ωsledek z≈Østal stejn√Ω, tak≈æe dosazen√≠ plat√≠. D√°le m≈Ø≈æeme za v√Ωraz hesla dosadit v√Ωraz slovn√≠k[\"hesla\"] (proto≈æe hesla = slovn√≠k[\"hesla] ), p≈ôiƒçem≈æ √∫tr≈æek k√≥du [\"koƒçka\"] z≈Østane tam, kde byl: In [363]: slovn√≠k [ \"hesla\" ] [ \"koƒçka\" ] Out[363]: {'definice': 'chlupat√Ω mazl√≠ƒçek', 'p≈ô√≠klad': 'Na oknƒõ sedƒõla koƒçka...'} Skvƒõle, v√Ωsledek je st√°le stejn√Ω! Teƒè u≈æ se jen zbav√≠me mezer, aby byl z√°pis konvenƒçnƒõj≈°√≠, a vymysleli jsme zp≈Øsob, jak indexovat do vno≈ôen√Ωch kolekc√≠. In [364]: slovn√≠k [ \"hesla\" ][ \"koƒçka\" ] Out[364]: {'definice': 'chlupat√Ω mazl√≠ƒçek', 'p≈ô√≠klad': 'Na oknƒõ sedƒõla koƒçka...'} In [365]: # rok slovn√≠k [ \"datum\" ][ 0 ] Out[365]: 2018 In [366]: # p≈ô√≠kladov√° vƒõta pro psa slovn√≠k [ \"hesla\" ][ \"pes\" ][ \"p≈ô√≠klad\" ] Out[366]: '... a venku ≈°tƒõkal pes.' In [367]: # jm√©no druh√©ho autora slovn√≠k [ \"auto≈ôi\" ][ 1 ][ \"jm√©no\" ] Out[367]: 'Jane' A tak podobnƒõ. Nezabudovan√© kolekce Existuje samoz≈ôejmƒõ mnoho nezabudovan√Ωch kolekc√≠ -- nap≈ô. t≈ô√≠da FreqDist z knihovny nltk je taky kolekce (obsahuje d√≠lƒç√≠ prvky). Nezabudovan√© kolekce pochopitelnƒõ nemaj√≠ liter√°ly, je pot≈ôeba je inicializovat pomoc√≠ p≈ô√≠slu≈°n√©ho konstruktoru t≈ô√≠dy (viz odd. Typy a t≈ô√≠dy): In [368]: from nltk import FreqDist In [369]: FreqDist ( \"aababc\" ) Out[369]: FreqDist({'a': 3, 'b': 2, 'c': 1}) Opakov√°n√≠ operac√≠: for-cyklus ƒåasto chceme nƒõjakou operaci prov√©st opakovanƒõ, ale poka≈æd√© s trochu jin√Ωmi daty. Nap≈ô. chceme vytisknout ƒç√≠sla od 0 do 2. Mohli bychom samoz≈ôejmƒõ pou≈æ√≠t copy-paste, tj. ≈ô√°dek t≈ôikr√°t zkop√≠rovat a poka≈æd√© jen nahradit ƒç√≠slo... In [370]: print ( 0 ) print ( 1 ) print ( 2 ) 0 1 2 ... jen≈æe pak je probl√©m, ≈æe jakmile chceme akci trochu upravit, t≈ôeba vytisknout \"ƒå√≠slo: 0\" m√≠sto jen \"0\", a podobnƒõ pro ostatn√≠ ƒç√≠sla, mus√≠me prov√©st √∫pravu na ka≈æd√©m ≈ô√°dku: In [371]: print ( \"ƒå√≠slo:\" , 0 ) print ( \"ƒå√≠slo:\" , 1 ) print ( \"ƒå√≠sloo:\" , 2 ) ƒå√≠slo: 0 ƒå√≠slo: 1 ƒå√≠sloo: 2 P≈ôi takov√©m opisov√°n√≠ / kop√≠rov√°n√≠ je snadn√© udƒõlat na jednom m√≠stƒõ chybu (viz v√Ω≈°e \"ƒå√≠sloo\" m√≠sto \"ƒå√≠slo\") a najednou m√≠sto toho, abychom provedli t≈ôikr√°t tu samou operaci, provedeme dvakr√°t jednu a pot≈ôet√≠ trochu jinou. Proto je lep≈°√≠ se zamyslet, co se p≈ôi ka≈æd√©m opakov√°n√≠ mƒõn√≠ a co naopak z≈Øst√°v√° stejn√©, a zaznamenat to pomoc√≠ tzv. for-cyklu (angl. for-loop ): In [372]: for i in [ 0 , 1 , 2 ]: # hlaviƒçka print ( \"ƒå√≠slo:\" , i ) # tƒõlo ƒå√≠slo: 0 ƒå√≠slo: 1 ƒå√≠slo: 2 Takov√Ω for-cyklus m≈Ø≈æeme parafr√°zovat n√°sledovnƒõ: ka≈æd√© ƒç√≠slo ze seznamu [0, 1, 2] postupnƒõ ulo≈æ do promƒõnn√© i a proveƒè s√©rii operac√≠ popsan√Ωch v tƒõle cyklu. Zkr√°cenƒõ: pro ka≈æd√© (angl for each ) ƒç√≠slo ze seznamu proveƒè n√°sleduj√≠c√≠ operaci/operace. Hlaviƒçka for-cyklu popisuje promƒõnlivou ƒç√°st akce, kterou prov√°d√≠me (zde: promƒõnn√° i postupnƒõ nab√Ωv√° r≈Øzn√Ωch hodnot, kter√© ƒçerp√°me ze seznamu). Tƒõlo for-cyklu popisuje repetitivn√≠ ƒç√°st (zde: poka≈æd√© promƒõnnou i vytiskneme, spolu s prefixem \"ƒå√≠slo: \"). Podobnƒõ jako u funkc√≠ pozn√°me to, co je≈°tƒõ pat≈ô√≠ do tƒõla for-cyklu, a to, co u≈æ ne, podle odsazen√≠: In [373]: for i in [ 0 , 1 , 2 ]: print ( \"ƒå√≠slo:\" , i ) print ( \"J√° je≈°tƒõ do tƒõla cyklu pat≈ô√≠m!\" ) print ( \"J√° u≈æ ne :(\" ) ƒå√≠slo: 0 J√° je≈°tƒõ do tƒõla cyklu pat≈ô√≠m! ƒå√≠slo: 1 J√° je≈°tƒõ do tƒõla cyklu pat≈ô√≠m! ƒå√≠slo: 2 J√° je≈°tƒõ do tƒõla cyklu pat≈ô√≠m! J√° u≈æ ne :( Obecnƒõ m≈Ø≈æeme for-cyklus charakterizovat takto: for item in iterable : # do something with item Promƒõnn√° iterable m≈Ø≈æe obsahovat jak√Ωkoli iterovateln√Ω objekt (= objekt, ze kter√©ho lze tahat dal≈°√≠ objekty, viz odd. Kolekce). Seznamy u≈æ jsme v roli iterable vidƒõli v akci v√Ω≈°e, n-tice funguj√≠ stejnƒõ. ≈òetƒõzce taky, p≈ôiƒçem≈æ ve for-cyklu je proch√°z√≠me znak po znaku: In [374]: ≈ôetƒõzec = \"abc\" for znak in ≈ôetƒõzec : print ( znak ) a b c Mno≈æiny funguj√≠ podobnƒõ, jen nen√≠ zaruƒçen√© po≈ôad√≠, v nƒõm≈æ budeme prvky vytahovat: In [375]: mno≈æina = { 2 , 3 , 1 } for prvek in mno≈æina : print ( prvek ) 1 2 3 U slovn√≠k≈Ø je to trochu slo≈æitƒõj≈°√≠ -- z√°le≈æ√≠, zda chceme proch√°zet kl√≠ƒçe... In [376]: slovn√≠k = { \"a\" : 1 , \"b\" : 2 , \"c\" : 3 } for kl√≠ƒç in slovn√≠k : print ( kl√≠ƒç ) a b c In [377]: # nebo t√©≈æ explicitnƒõji for kl√≠ƒç in slovn√≠k . keys (): print ( kl√≠ƒç ) a b c ... hodnoty... In [378]: for hodnota in slovn√≠k . values (): print ( hodnota ) 1 2 3 ... nebo oboj√≠ z√°rove≈à: In [379]: # jak p≈ôesnƒõ tato syntax funguje si vysvƒõtl√≠me o kousek n√≠≈æ for kl√≠ƒç , hodnota in slovn√≠k . items (): print ( f \"Pod kl√≠ƒçem { kl√≠ƒç !r} je ulo≈æen√° hodnota { hodnota } .\" ) Pod kl√≠ƒçem 'a' je ulo≈æen√° hodnota 1. Pod kl√≠ƒçem 'b' je ulo≈æen√° hodnota 2. Pod kl√≠ƒçem 'c' je ulo≈æen√° hodnota 3. Ale iterovateln√Ω objekt nerovn√° se nutnƒõ jen kolekce. V kombinaci s for-cyklem se ƒçasto hod√≠ funkce range : In [380]: for i in range ( 3 ): print ( \"ƒå√≠slo:\" , i ) ƒå√≠slo: 0 ƒå√≠slo: 1 ƒå√≠slo: 2 Funkce range vytvo≈ô√≠ objekt, kter√Ω v hlaviƒçce for-cyklu postupnƒõ generuje ƒç√≠sla v zadan√©m rozpƒõt√≠ (v na≈°em p≈ô√≠padƒõ od 0 do 3, horn√≠ hranici vyj√≠maje). Proƒç je to u≈æiteƒçn√©? P≈ôedstavte si, ≈æe bychom chtƒõli operaci print(\"ƒå√≠slo:\", i) prov√©st pro prvn√≠ch milion nez√°porn√Ωch ƒç√≠sel. Oproti seznamu m√° funkce range dvƒõ v√Ωhody: pro n√°s je v√Ωhoda, ≈æe nemus√≠me ruƒçnƒõ vypisovat seznam s milionem ƒç√≠sel pro poƒç√≠taƒç je v√Ωhoda, ≈æe nemus√≠ vytv√°≈ôet cel√Ω seznam najednou a pak ho uchov√°vat v pamƒõti (milion ƒç√≠sel sice dne≈°n√≠ poƒç√≠taƒçe zvl√°dnou hravƒõ, ale obecnƒõ plat√≠, ≈æe pamƒõti nikdy nen√≠ neomezenƒõ) -- ƒç√≠sla tiskne jedno po druh√©m a jakmile jedno zpracuje, tak ho m≈Ø≈æe zapomenout Kdy≈æ promƒõnnou z hlaviƒçky for-cyklu nikde v tƒõle nepou≈æijeme, b√Ωv√° zvykem j√≠ d√°t speci√°ln√≠ jm√©no _ , ƒç√≠m≈æ ostatn√≠m program√°tor≈Øm naznaƒç√≠me, ≈æe se nemaj√≠ divit, ≈æe nen√≠ pou≈æit√°. In [381]: # t≈ôi hody kostkou for _ in range ( 3 ): print ( randint ( 1 , 6 )) 2 5 5 Obƒças se stane, ≈æe pot≈ôebujeme for-cyklem proj√≠t kolekci, jej√≠mi≈æ prvky jsou n-tice nebo seznamy, kter√© v r√°mci for-cyklu pot≈ôebujeme d√°le rozebrat na d√≠lƒç√≠ prvky. M≈Ø≈æeme samoz≈ôejmƒõ pou≈æ√≠t indexaci: In [382]: # Vƒõtu \"Pr≈°√≠.\" m√°me reprezentovanou jako seznam dvou token≈Ø. Ka≈æd√Ω token # je trojice ≈ôetƒõzc≈Ø: prvn√≠ oznaƒçuje slovn√≠ tvar, druh√Ω lemma (= # slovn√≠kovou podobu), t≈ôet√≠ slovn√≠ druh (V = sloveso, Z = interpunkce). vƒõta = [( \"Pr≈°√≠\" , \"pr≈°et\" , \"V\" ), ( \".\" , \".\" , \"Z\" )] for token in vƒõta : word = token [ 0 ] lemma = token [ 1 ] tag = token [ 2 ] print ( f \"WORD: { word !r} , LEMMA: { lemma !r} , TAG: { tag !r} \" ) WORD: 'Pr≈°√≠', LEMMA: 'pr≈°et', TAG: 'V' WORD: '.', LEMMA: '.', TAG: 'Z' Pr√°ci n√°m ale m≈Ø≈æe u≈°et≈ôit tzv. destrukturace (angl. destructuring , t√©≈æ se tomu nƒõkdy ≈ô√≠k√° unpacking nebo pattern matching ): m√°me-li uspo≈ô√°danou kolekci (seznam nebo n-tici) o X prvc√≠ch, m≈Ø≈æeme prvky rovnou namapovat na X promƒõnn√Ωch: In [383]: a , b , c = [ 1 , 2 , 3 ] In [384]: a Out[384]: 1 In [385]: b Out[385]: 2 In [386]: c Out[386]: 3 Jak to funguje? Strukturn√≠ vzorec ( pattern ) nalevo od = obsahuje t≈ôi voln√© \"sloty\" (odpov√≠daj√≠c√≠ t≈ôem promƒõnn√Ωm). Python se tento vzorec pokus√≠ p≈ôilo≈æit na datovou strukturu napravo od = (p≈ôedstavte si pr≈Øhledn√Ω pauzovac√≠ pap√≠r), namapovat ( match ) objekty, kter√© datov√° struktura obsahuje, na voln√© sloty, a strukturu tak rozebrat na d√≠lƒç√≠ prvky (proto hovo≈ô√≠me o destrukturaci nebo unpacking -- rozbalen√≠). Ne v≈ædycky se to samoz≈ôejmƒõ povede -- nap≈ô. kdy≈æ je slot≈Ø v√≠c ne≈æ prvk≈Ø... In [387]: a , b , c = ( 1 , 2 ) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-387-24077ba23852> in <module> ----> 1 a , b , c = ( 1 , 2 ) ValueError : not enough values to unpack (expected 3, got 2) ... nebo naopak: In [388]: a , b = ( 1 , 2 , 3 ) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-388-faee755feda6> in <module> ----> 1 a , b = ( 1 , 2 , 3 ) ValueError : too many values to unpack (expected 2) Strukturn√≠ vzorec m≈Ø≈æe obsahovat promƒõnnou s hvƒõzdiƒçkou (viz odd. Funkce), pak na sebe tato promƒõnn√° nav√°≈æe zb√Ωvaj√≠c√≠ prvky kolekce: In [389]: a , * rest = ( 1 , 2 , 3 , 4 , 5 , 6 ) In [390]: a Out[390]: 1 In [391]: rest Out[391]: [2, 3, 4, 5, 6] Kdy≈æ je kolekce vno≈ôen√°, m≈Ø≈æeme strukturn√≠ vzorec namapovat jen na nejvy≈°≈°√≠ √∫rove≈à... In [392]: a , b = ( 1 , ( 2 , 3 )) In [393]: a Out[393]: 1 In [394]: b Out[394]: (2, 3) ... nebo m≈Ø≈æe vzorec b√Ωt takt√©≈æ vno≈ôen√Ω: In [395]: a , ( b , c ) = ( 1 , ( 2 , 3 )) In [396]: a Out[396]: 1 In [397]: b Out[397]: 2 In [398]: c Out[398]: 3 Kdy≈æ d√°me destrukturaci dohromady s for-cyklem, zkr√°t√≠ a zp≈ôehledn√≠ se n√°m z√°pis: In [399]: for token in vƒõta : word , lemma , tag = token print ( f \"WORD: { word !r} , LEMMA: { lemma !r} , TAG: { tag !r} \" ) WORD: 'Pr≈°√≠', LEMMA: 'pr≈°et', TAG: 'V' WORD: '.', LEMMA: '.', TAG: 'Z' A dokonce m≈Ø≈æeme ubrat je≈°tƒõ jeden ≈ô√°dek, proto≈æe destrukturaci lze prov√©st p≈ô√≠mo v r√°mci hlaviƒçky for-cyklu: In [400]: for word , lemma , tag in vƒõta : print ( f \"WORD: { word !r} , LEMMA: { lemma !r} , TAG: { tag !r} \" ) WORD: 'Pr≈°√≠', LEMMA: 'pr≈°et', TAG: 'V' WORD: '.', LEMMA: '.', TAG: 'Z' Varianta s vno≈ôen√Ωm strukturn√≠m vzorcem m≈Ø≈æe nastat nap≈ô. v p≈ô√≠padƒõ, ≈æe si jednotliv√© tokeny oƒç√≠slujeme pomoc√≠ funkce enumerate , kter√° ke ka≈æd√©mu prvku ve zdrojov√© kolekci p≈ôid√° po≈ôadov√© ƒç√≠slo. Takhle to vypad√°, kdy≈æ si v√Ωsledek ulo≈æ√≠me do seznamu: In [401]: list ( enumerate ( vƒõta )) Out[401]: [(0, ('Pr≈°√≠', 'pr≈°et', 'V')), (1, ('.', '.', 'Z'))] A takhle bychom ji spolu s vno≈ôenou destrukturac√≠ mohli pou≈æ√≠t ve for-cyklu: In [402]: for i , ( word , lemma , tag ) in enumerate ( vƒõta ): print ( f \" { i + 1 } . WORD: { word !r} , LEMMA: { lemma !r} , TAG: { tag !r} \" ) 1. WORD: 'Pr≈°√≠', LEMMA: 'pr≈°et', TAG: 'V' 2. WORD: '.', LEMMA: '.', TAG: 'Z' V prvn√≠m opakov√°n√≠ for-cyklu provede Python tuto destrukturaci... In [403]: i , ( word , lemma , tag ) = ( 0 , ( \"Pr≈°√≠\" , \"pr≈°et\" , \"V\" )) ... v druh√©m pak tuto: In [404]: i , ( word , lemma , tag ) = ( 1 , ( \".\" , \".\" , \"Z\" )) Na z√°vƒõr p≈ô√≠klad pou≈æit√≠ destrukturace s hvƒõzdiƒçkou -- m≈Ø≈æe se hodit, kdy≈æ jsou jednotliv√© d√≠lƒç√≠ kolekce, kter√© postupnƒõ ve for-cyklu zpracov√°v√°me, r≈Øznƒõ dlouh√©: In [405]: for i , * rest in [( 1 ,), ( 2 , 3 ), ( 4 , 5 , 6 )]: print ( i ) print ( rest ) 1 [] 2 [3] 4 [5, 6] Obƒças se v r√°mci for-cykl≈Ø m≈Ø≈æou hodit kl√≠ƒçov√° slov√≠ƒçka break a continue . break okam≈æitƒõ ukonƒç√≠ for-cyklus. N√°sleduj√≠c√≠ for-cyklus by mƒõl sice teoreticky proj√≠t ƒç√≠sla od 0 do 4, ale zastav√≠ se u ƒç√≠sla 3: In [406]: for i in range ( 5 ): # syntax podm√≠nek viz odd. Podm√≠nky if i == 3 : break print ( i ) 0 1 2 continue ukonƒç√≠ souƒçasnou iteraci for-cyklu a zah√°j√≠ dal≈°√≠. N√°sleduj√≠c√≠ for-cyklus tedy projde ƒç√≠sla od 0 do 4, ale vytiskne jen ta lich√°, proto≈æe u sud√Ωch se k ≈ô√°dku s funkc√≠ print v≈Øbec nedostane: In [407]: for i in range ( 5 ): if i % 2 == 0 : continue print ( i ) 1 3 POZN.: V tƒõlech for-cykl≈Ø jsme v≈°ude pou≈æili funkci print , abychom mohli nahl√©dnout do jejich pr≈Øbƒõhu, tj. do toho, jak se jednotliv√© operace opakuj√≠. Bez funkce print to jde samoz≈ôejmƒõ taky, jen se n√°m nedostane ≈æ√°dn√© vizu√°ln√≠ zpƒõtn√© vazby, kter√° by n√°m pomohla pochopit, co se po spu≈°tƒõn√≠ for-cyklu odehr√°v√°: In [408]: num = 0 for x in range ( 101 ): num += x In [409]: num Out[409]: 5050 Jen je≈°tƒõ dopln√≠m, ≈æe elegantnƒõji m≈Ø≈æeme ƒç√≠sla od 0 do 100 seƒç√≠st pomoc√≠ funkce sum : In [410]: sum ( range ( 101 )) Out[410]: 5050 Logika V Pythonu lze pracovat i s logick√Ωmi v√Ωroky. Z√°kladem pro to jsou konstanty True a False , kter√© oznaƒçuj√≠ pravdu, resp. nepravdu, a kter√© Python vrac√≠, kdy≈æ posuzuje pravdivost nƒõjak√©ho logick√©ho v√Ωroku: In [411]: # rovnost 2 + 2 == 4 Out[411]: True In [412]: list ( \"abc\" ) == sorted ( \"cab\" ) Out[412]: True In [413]: # negace rovnosti 2 + 2 != 5 Out[413]: True In [414]: # p≈ô√≠tomnost prvku v kolekci \"a\" in \"abc\" Out[414]: True In [415]: # p≈ô√≠tomnost cel√©ho ƒç√≠sla v rozpƒõt√≠ cel√Ωch ƒç√≠sel 7 in range ( 5 , 10 ) Out[415]: True In [416]: # identita objektu jan = bratr = [ \"nohy\" , \"trup\" , \"hlava\" , \"vlasy\" ] jan is bratr Out[416]: True In [417]: # r≈Øzn√© typy nerovnost√≠ -- men≈°√≠ ne≈æ 2 < 3 Out[417]: True In [418]: # men≈°√≠ nebo rovno 3 <= 3 Out[418]: True In [419]: # vƒõt≈°√≠ 2 > 1 Out[419]: True In [420]: # vƒõt≈°√≠ nebo rovno 1 >= 1 Out[420]: True In [421]: 1 < 3 > 2 Out[421]: True In [422]: # v√Ωroky o ≈ôetƒõzc√≠ch \"abc\" . islower () Out[422]: True A tak podobnƒõ, viz p≈ô√≠klady operac√≠, kter√© vrac√≠ True nebo False , v p≈ôedchoz√≠ch odd√≠lech. Pravdivostn√≠ hodnotu ale maj√≠ v≈°echny objekty v Pythonu. M≈Ø≈æeme ji zjistit pomoc√≠ funkce bool : In [423]: bool ( \"abc\" ) Out[423]: True Pravdiv√© jsou skoro v≈°echny objekty kromƒõ: In [424]: # ƒç√≠sla 0 bool ( 0 ) Out[424]: False In [425]: # pr√°zdn√Ωch kolekc√≠ bool ([]) Out[425]: False In [426]: bool ({}) Out[426]: False In [427]: bool ( \"\" ) Out[427]: False In [428]: # None bool ( None ) Out[428]: False In [429]: # a pochopitelnƒõ samotn√© konstanty False bool ( False ) Out[429]: False Sestavovat z jednoduch√Ωch v√Ωrok≈Ø slo≈æitƒõj≈°√≠ lze pomoc√≠ oper√°tor≈Ø and , or a not : In [430]: # x and y plat√≠, kdy≈æ jsou oba v√Ωroky x a y pravdiv√© True and True Out[430]: True In [431]: True and False Out[431]: False In [432]: # x or y plat√≠, kdy≈æ je aspo≈à jeden z v√Ωrok≈Ø x a y pravdiv√Ω True or True Out[432]: True In [433]: True or False Out[433]: True In [434]: False or False Out[434]: False In [435]: # not invertuje pravdivostn√≠ hodnotu v√Ωroku not True Out[435]: False In [436]: not False Out[436]: True V praxi pou≈æijeme t≈ôeba n√°sledovnƒõ: In [437]: ≈ôetƒõzec = \"koƒçka\" len ( ≈ôetƒõzec ) > 3 and \"ƒç\" in ≈ôetƒõzec Out[437]: True Jak jsme psali v√Ω≈°e, pravdivostn√≠ hodnotu maj√≠ v≈°echny objekty v Pythonu, m≈Ø≈æeme tedy klidnƒõ napsat: In [438]: not \"abc\" Out[438]: False In [439]: True or 0 Out[439]: True D√≠ky tomu m≈Ø≈æeme trochu up≈ôesnit chov√°n√≠ oper√°tor≈Ø and a or . and zaƒçne vyhodnocovat v√Ωrazy (zleva doprava) a vr√°t√≠ buƒè prvn√≠ nepravdiv√Ω, nebo posledn√≠ zb√Ωvaj√≠c√≠: In [440]: # posledn√≠ zb√Ωvaj√≠c√≠ (pravdiv√Ω) True and 1 and \"abc\" and sorted Out[440]: <function sorted(iterable, /, *, key=None, reverse=False)> In [441]: # prvn√≠ nepravdiv√Ω True and 1 and \"abc\" and sorted and [] and 0 Out[441]: [] or taky zaƒçne vyhodnocovat v√Ωrazy (zleva doprava) a vr√°t√≠ buƒè prvn√≠ pravdiv√Ω, nebo posledn√≠ zb√Ωvaj√≠c√≠: In [442]: # prvn√≠ pravdiv√Ω 0 or [] or False or 42 or \"abc\" Out[442]: 42 In [443]: # posledn√≠ zb√Ωvaj√≠c√≠ (nepravdiv√Ω) 0 or [] or False or {} Out[443]: {} Jakmile and nebo or naraz√≠ na v√Ωraz, podle kter√©ho se m≈Ø≈æe rozhodnout, dal≈°√≠ u≈æ nevyhodnocuje. M≈Ø≈æeme si to uk√°zat, kdy≈æ jeden z v√Ωrazu bude vol√°n√≠ funkce, jej√≠m≈æ vedlej≈°√≠m efektem je, ≈æe nƒõco vytiskne na obrazovku. In [444]: def funkce (): print ( \"vol√°m funkci...\" ) return \"v√Ωsledek funkce\" In [445]: # zde m≈Ø≈æe and vr√°tit hned prvn√≠ v√Ωraz, tak≈æe se funkce nezavol√° 0 and funkce () Out[445]: 0 In [446]: # zde je pot≈ôeba po prvn√≠m v√Ωrazu pokraƒçovat v ovƒõ≈ôov√°n√≠ pravdivostn√≠ch # hodnot, tak≈æe se funkce zavol√° 1 and funkce () vol√°m funkci... Out[446]: 'v√Ωsledek funkce' Pomoc√≠ or tedy m≈Ø≈æeme nap≈ô. nahradit metodu setdefault (viz odd. Kolekce -- Slovn√≠ky) -- √∫pravu slovn√≠ku provedeme pouze v p≈ô√≠padƒõ, ≈æe dan√Ω kl√≠ƒç je≈°tƒõ neobsahuje: In [447]: slovn√≠k = dict ( a = 1 , b = 2 ) In [448]: # \"a\" u≈æ ve slovn√≠ku je, prvn√≠ v√Ωraz tedy plat√≠ a druh√Ω se ani # nevyhodnot√≠ \"a\" in slovn√≠k or slovn√≠k . update ( a = 42 ) Out[448]: True In [449]: slovn√≠k Out[449]: {'a': 1, 'b': 2} In [450]: # \"c\" ve slovn√≠ku nen√≠, prvn√≠ v√Ωraz tedy neplat√≠ a druh√Ω se # vyhodnot√≠ \"c\" in slovn√≠k or slovn√≠k . update ( c = 42 ) In [451]: slovn√≠k Out[451]: {'a': 1, 'b': 2, 'c': 42} M√°me-li kolekci objekt≈Ø a chceme ovƒõ≈ôit, zda je aspo≈à jeden z nich pravdiv√Ω , m≈Ø≈æeme pou≈æ√≠t zabudovanou funkci any : In [452]: any ([ True , False , False ]) Out[452]: True Chceme-li ovƒõ≈ôit, zda jsou v≈°echny pravdiv√© , pou≈æijeme zabudovanou funkci all : In [453]: all ([ True , True , True ]) Out[453]: True any a all jsou velmi u≈æiteƒçn√© v kombinaci s konvertory kolekc√≠ (viz odd. Konvertory kolekc√≠). Varov√°n√≠ na z√°vƒõr -- slo≈æitƒõj≈°√≠ logick√© v√Ωroky jsou zr√°dn√©, jejich √∫pravy jsou netrivi√°ln√≠ a ≈ô√≠d√≠ se p≈ôesnƒõ dan√Ωmi pravidly. Kdo si je podobnƒõ jako j√° ze st≈ôedn√≠ ≈°koly pamatuje sp√≠≈° matnƒõ, mƒõl by si d√°t extra pozor ;) Nap≈ô. v√Ωrok not (x and y) je ekvivalentn√≠ v√Ωroku not x or not y , ne v√Ωroku not x and not y , jak bychom mo≈æn√° mohli m√≠t tendenci naivnƒõ \"rozn√°sobit\". Slo≈æitƒõj≈°√≠ p≈ô√≠pady si na≈°tƒõst√≠ v≈ædy lze pro jistotu ovƒõ≈ôit pomoc√≠ pravdivostn√≠ tabulky , a≈• u≈æ si ji nakresl√≠me ruƒçnƒõ nebo si rovnost v√Ωrok≈Ø pro v≈°echny mo≈æn√© kombinace hodnot x a y zkontrolujeme pomoc√≠ Pythonu: In [454]: for x in [ True , False ]: for y in [ True , False ]: # ‚Üì v√Ωroky, jejich≈æ rovnost ovƒõ≈ôujeme ‚Üì is_equal = ( not ( x and y )) == ( not x or not y ) print ( f \"x is { x } , y is { y } , the equality is { is_equal } \" ) x is True, y is True, the equality is True x is True, y is False, the equality is True x is False, y is True, the equality is True x is False, y is False, the equality is True Aby rovnost dvou v√Ωraz≈Ø platila obecnƒõ, mus√≠ platit pro ka≈ædou variantu dosazen√≠ konkr√©tn√≠ch hodnot za promƒõnn√© (zde x a y ). O tom se m≈Ø≈æeme p≈ôesvƒõdƒçit buƒè ovƒõ≈ôen√≠m v√Ωstup≈Ø funkce print (viz p≈ôedchoz√≠ bu≈àka), nebo m≈Ø≈æeme k√≥d taky upravit jako konvertor kolekce v kombinaci s funkc√≠ all a dostaneme tak celkov√Ω v√Ωsledek rovnou: In [455]: all ( ( not ( x and y )) == ( not x or not y ) for x in [ True , False ] for y in [ True , False ] ) Out[455]: True A je≈°tƒõ pro srovn√°n√≠, jak by vypadaly v√Ωstupy v p≈ô√≠padƒõ, ≈æe si porovn√°van√© v√Ωrazy obecnƒõ vzato rovn√© nejsou (v≈°imnƒõte si, ≈æe pro nƒõkter√© kombinace hodnot x a y rovnost plat√≠, ale ne pro v≈°echny)... In [456]: for x in [ True , False ]: for y in [ True , False ]: is_equal = ( not ( x and y )) == ( not x and not y ) print ( f \"x is { x } , y is { y } , the equality is { is_equal } \" ) x is True, y is True, the equality is True x is True, y is False, the equality is False x is False, y is True, the equality is False x is False, y is False, the equality is True ... tak≈æe celkov√Ω verdikt je... In [457]: all ( ( not ( x and y )) == ( not x and not y ) for x in [ True , False ] for y in [ True , False ] ) Out[457]: False Podm√≠nky: if, elif, else (a while-cyklus) Na logick√Ωch v√Ωroc√≠ch lze d√°l stavƒõt vƒõtven√≠ programu pomoc√≠ podm√≠nek: In [458]: if 2 + 2 == 4 : # hlaviƒçka print ( \"matematika funguje\" ) # tƒõlo matematika funguje Syntax podm√≠nek je znovu postaven√° na principu hlaviƒçky a odsazen√©ho tƒõla, kter√© se vykon√°, pokud podm√≠nka uveden√° v hlaviƒçce plat√≠. if je jako v√Ωhybka: plat√≠-li podm√≠nka, vyd√° se program trochu jinou cestou, ne≈æ kdy≈æ neplat√≠. Alternativn√≠ch podm√≠nƒõn√Ωch cest m≈Ø≈æe b√Ωt v√≠ce, napoj√≠me je pomoc√≠ kl√≠ƒçov√©ho slov√≠ƒçka elif : In [459]: num = 2 if num == 0 : print ( \"nula\" ) elif num == 1 : print ( \"jedna\" ) elif num == 2 : print ( \"dva\" ) dva Na z√°vƒõr m≈Ø≈æeme p≈ôidat je≈°tƒõ jednu nepodm√≠nƒõnou alternativn√≠ cestu pomoc√≠ kl√≠ƒçov√©ho slov√≠ƒçka else . Tou se programu vyd√°, kdy≈æ se uk√°≈æe, ≈æe ani jedna z p≈ôedchoz√≠ch podm√≠nek neplat√≠: In [460]: num = 3 if num == 0 : print ( \"nula\" ) elif num == 1 : print ( \"jedna\" ) elif num == 2 : print ( \"dva\" ) else : print ( \"nƒõco jin√©ho\" ) nƒõco jin√©ho Je d≈Øle≈æit√© si uvƒõdomit, ≈æe vƒõtven√≠ skuteƒçnƒõ funguje jako v√Ωhybka: program se vyd√° prvn√≠ cestou, kde naraz√≠ na pravdivou podm√≠nku, a zb√Ωvaj√≠c√≠ ignoruje, i kdyby byly nakr√°snƒõ pravdiv√© taky: In [461]: if True : print ( \"podm√≠nka u t√©hle vƒõtve je v≈ædy pravdiv√°\" ) elif True : print ( \"u t√©hle taky, ale nen√≠ to nic platn√©, je a≈æ druh√°\" ) podm√≠nka u t√©hle vƒõtve je v≈ædy pravdiv√° Vizu√°lnƒõ si to m≈Ø≈æeme p≈ôedstavit takto: .---> if / /-----> elif / --------> elif \\ \\-----{ ... \\ `---> else D≈Øle≈æit√Ωm d≈Øsledkem je, ≈æe z√°le≈æ√≠ na po≈ôad√≠ podm√≠nek . Pravidlo prav√© ruky zn√≠, ≈æe specifiƒçtƒõj≈°√≠ podm√≠nky by mƒõly pro jistotu b√Ωt na zaƒç√°tku , jinak se k nim program nemus√≠ dostat, proto≈æe ho odchyt√≠ d≈ô√≠vƒõj≈°√≠ m√©nƒõ specifick√© podm√≠nky. Nap≈ô. podm√≠nka x >= 0 je m√©nƒõ specifick√° ne≈æ podm√≠nka x == 2 , tak≈æe kdy≈æ bude p≈ôed n√≠, sebere j√≠ v≈°echny potenci√°ln√≠ z√°kazn√≠ky: In [462]: for x in range ( 4 ): if x >= 0 : print ( f \" { x } je vƒõt≈°√≠ ne≈æ 0\" ) elif x == 2 : print ( \"HA! DVOJKA.\" ) 0 je vƒõt≈°√≠ ne≈æ 0 1 je vƒõt≈°√≠ ne≈æ 0 2 je vƒõt≈°√≠ ne≈æ 0 3 je vƒõt≈°√≠ ne≈æ 0 Srovnejte s v√Ωstupem, kdy≈æ po≈ôad√≠ podm√≠nek prohod√≠me: In [463]: for x in range ( 4 ): if x == 2 : print ( \"HA! DVOJKA.\" ) elif x >= 0 : print ( f \" { x } je vƒõt≈°√≠ ne≈æ 0\" ) 0 je vƒõt≈°√≠ ne≈æ 0 1 je vƒõt≈°√≠ ne≈æ 0 HA! DVOJKA. 3 je vƒõt≈°√≠ ne≈æ 0 Ka≈æd√© kl√≠ƒçov√© slov√≠ƒçko if odstartuje nov√© vƒõtven√≠, pod nƒõ≈æ spadaj√≠ p≈ô√≠padn√° n√°sleduj√≠c√≠ elif a else na stejn√© √∫rovni odsazen√≠: In [464]: num = 2 if num > 1 : print ( \"1. vƒõtven√≠: num je vƒõt≈°√≠ ne≈æ jedna\" ) elif num < 1 : print ( \"1. vƒõtven√≠: num je men≈°√≠ ne≈æ jedna\" ) else : print ( \"1. vƒõtven√≠: num je rovno jedn√©\" ) if num == 2 : print ( \"2. vƒõtven√≠: num je rovno dvƒõma\" ) 1. vƒõtven√≠: num je vƒõt≈°√≠ ne≈æ jedna 2. vƒõtven√≠: num je rovno dvƒõma Vizu√°lnƒõ si to m≈Ø≈æeme p≈ôedstavit takto: .---> if / /-----> elif / --------> elif \\ \\-----{ ... \\ `---> else --------> if Existuje t√©≈æ speci√°ln√≠ dvouƒçlenn√Ω oper√°tor if ... else , kter√Ω jako v√Ωsledek vr√°t√≠ jednu ze dvou mo≈ænost√≠ podle toho, jak dopadne pravdivostn√≠ test: v√Ωsledek_je_li_test_pravdiv√Ω if test else v√Ωsledek_je_li_test_nepravdiv√Ω Konkr√©tn√≠ p≈ô√≠klad bude asi srozumitelnƒõj≈°√≠: In [465]: \"A\" if True else \"B\" Out[465]: 'A' In [466]: \"A\" if False else \"B\" Out[466]: 'B' D√≠ky tomuto oper√°toru m√°me k dispozici kompaktnƒõj≈°√≠ z√°pis, kdy≈æ chceme hodnotu nƒõjak√© promƒõnn√© stanovit na z√°kladƒõ pravdivostn√≠ho testu. Bez oper√°toru if ... else bychom nap≈ô. museli ps√°t: In [467]: ledniƒçka = { \"puding\" , \"s√Ωr\" , \"sal√°t\" } if \"puding\" in ledniƒçka : reakce = \"hur√°, puding!\" else : reakce = \"nen√≠ puding :(\" reakce Out[467]: 'hur√°, puding!' Ale s pomoc√≠ oper√°toru if ... else staƒç√≠ napsat: In [468]: reakce = \"hur√°, puding!\" if \"puding\" in ledniƒçka else \"nen√≠ puding :(\" reakce Out[468]: 'hur√°, puding!' S podm√≠nkami souvis√≠ i jin√° podoba cyklu, tzv. while-cyklus. While-cyklus se opakuje tak dlouho, dokud je podm√≠nka v hlaviƒçce pravdiv√°: In [469]: i = 0 while i < 3 : print ( \"ƒå√≠slo\" , i , \"je men≈°√≠ ne≈æ 3.\" ) i += 1 ƒå√≠slo 0 je men≈°√≠ ne≈æ 3. ƒå√≠slo 1 je men≈°√≠ ne≈æ 3. ƒå√≠slo 2 je men≈°√≠ ne≈æ 3. Konvertory kolekc√≠ Mnoho funkc√≠ pracuje s kolekcemi, nap≈ô. zabudovan√° funkce sorted nebo konstruktor FreqDist z knihovny nltk . Nƒõkdy je u≈æiteƒçn√© m√≠t mo≈ænost kolekci trochu upravit ve chv√≠li, kdy ji takov√© funkci p≈ôed√°v√°me. K tomu p≈ôesnƒõ slou≈æ√≠ konvertory kolekce . Kamkoli m≈Ø≈æeme d√°t norm√°ln√≠ kolekci... In [470]: vƒõta = \"Bylo n√°s pƒõt .\" . split () vƒõta Out[470]: ['Bylo', 'n√°s', 'pƒõt', '.'] In [471]: sorted ( vƒõta ) Out[471]: ['.', 'Bylo', 'n√°s', 'pƒõt'] ... m≈Ø≈æeme propa≈°ovat m√≠sto n√≠ i konvertor: In [472]: sorted ( slovo for slovo in vƒõta if slovo . islower ()) Out[472]: ['n√°s', 'pƒõt'] Jejich syntax p≈ôipom√≠n√° syntax for-cyklu, jen jsou jednotliv√© prvky p≈ôeskl√°dan√© a mo≈ænosti jsou omezenƒõj≈°√≠ ne≈æ v pln√©m for-cyklu. Abychom se v jejich z√°pisu l√©pe zorientovali, vyu≈æijeme toho, ≈æe uvnit≈ô jak√Ωchkoli z√°vorek m≈Ø≈æeme k√≥d v Pythonu libovolnƒõ nasekat na ≈ô√°dky a p≈ôidat odsazen√≠ , aby se n√°m l√©pe ƒçetl: In [473]: sorted ( slovo for slovo in vƒõta if slovo . islower () ) Out[473]: ['n√°s', 'pƒõt'] Teƒè u≈æ je l√©pe vidƒõt, ≈æe konvertor kolekce m√° t≈ôi ƒç√°sti. ƒåtou se odprost≈ôedka: convert ( item ) # 3. for item in collection # 1. if test ( item ) # 2. Popis jednotliv√Ωch f√°z√≠ vypad√° n√°sledovnƒõ: Proch√°z√≠me kolekci prvek po prvku. Nepovinnƒõ m≈Ø≈æeme prov√©st nƒõjak√Ω test; pokud ho aktu√°ln√≠ prvek nespln√≠, bude z v√Ωsledku vy≈ôazen. T√≠m m≈Ø≈æeme zdrojovou kolekci profiltrovat . Nakonec spoƒç√≠t√°me hodnotu, kterou za dan√Ω prvek ze zdrojov√© kolekce za≈ôad√≠me do v√Ωsledku. Tato hodnota m≈Ø≈æe b√Ωt p≈Øvodn√≠ prvek samotn√Ω, m≈Ø≈æe b√Ωt vypoƒç√≠tan√° na z√°kladƒõ prvku, nebo s n√≠m taky v≈Øbec nemus√≠ souviset. Nejjednodu≈°≈°√≠ konvertor kolekce, kter√Ωm kolekce projede nezmƒõnƒõn√°, vypad√° takto: item # 3. for item in collection # 1. # 2. nic In [474]: sorted ( slovo # 3. for slovo in vƒõta # 1. # 2. ) Out[474]: ['.', 'Bylo', 'n√°s', 'pƒõt'] Chceme-li m√≠sto slov samotn√Ωch do v√Ωsledku za≈ôadit n-tici (slovo, d√©lka_slova) , mus√≠me upravit f√°zi 3: In [475]: sorted ( ( slovo , len ( slovo )) # 3. √∫prava zde for slovo in vƒõta # 1. # 2. ) Out[475]: [('.', 1), ('Bylo', 4), ('n√°s', 3), ('pƒõt', 3)] Chceme-li nav√≠c zahodit v≈°echna slova, kter√° nesest√°vaj√≠ z mal√Ωch p√≠smen, mus√≠me doplnit f√°zi 2: In [476]: sorted ( ( slovo , len ( slovo )) # 3. for slovo in vƒõta # 1. if slovo . islower () # 2. √∫prava zde ) Out[476]: [('n√°s', 3), ('pƒõt', 3)] Konvertor kolekce lze v≈ædy p≈ôeskl√°dat na norm√°ln√≠ for-cyklus, nap≈ô. ten bezprost≈ôednƒõ p≈ôedch√°zej√≠c√≠: In [477]: pomocn√Ω_seznam = [] for slovo in vƒõta : if slovo . islower (): pomocn√Ω_seznam . append (( slovo , len ( slovo ))) sorted ( pomocn√Ω_seznam ) Out[477]: [('n√°s', 3), ('pƒõt', 3)] Naopak p≈ôeskl√°dat for-cyklus na konvertor kolekce poka≈æd√© nejde, proto≈æe for-cyklus poskytuje mnohem vƒõt≈°√≠ volnost. V√Ωmƒõnou za toto omezen√≠ poskytuj√≠ konvertory kolekce oproti for-cykl≈Øm nƒõkolik v√Ωhod: √∫spornƒõj≈°√≠ syntax (nesna≈æ√≠ se pokr√Ωt plnou flexibilitu for-cykl≈Ø) p≈ôi bƒõhu programu zab√≠raj√≠ m√©nƒõ ƒçasu a pamƒõti (nap≈ô. nen√≠ pot≈ôeba vytv√°≈ôet ≈æ√°dn√© pomocn√© kolekce typu pomocn√Ω_seznam ) ve for-cyklu se kv≈Øli jeho flexibilitƒõ snadno m≈Ø≈æe schovat mnohem vƒõt≈°√≠ mno≈æstv√≠ chyb Vztah konvertor≈Ø kolekc√≠ k for-cykl≈Øm je tedy podobn√Ω jako vztah (nemodifikovateln√Ωch) n-tic k (modifikovateln√Ωm) seznam≈Øm: konvertory (a n-tice) jsou mnohem m√©nƒõ flexibiln√≠, ale ve chv√≠li, kdy tu flexibilitu nepot≈ôebujete, je dobr√© m√≠t mo≈ænost se j√≠ explicitnƒõ vzd√°t a uchr√°nit se tak mo≈æn√Ωch chyb, kter√© by z n√≠ mohly plynout. Kdy≈æ nen√≠ konvertor kolekce jedin√Ωm argumentem funkce, je pot≈ôeba ho uz√°vorkovat... In [478]: sorted (( slovo for slovo in vƒõta if slovo . islower ()), reverse = True ) Out[478]: ['pƒõt', 'n√°s'] ... jinak n√°s Python vypl√≠sn√≠: In [479]: sorted ( slovo for slovo in vƒõta if slovo . islower (), reverse = True ) File \"<ipython-input-479-bd2750ac97af>\" , line 1 sorted(slovo for slovo in vƒõta if slovo.islower(), reverse=True) &#94; SyntaxError : Generator expression must be parenthesized Existuje speci√°ln√≠ syntax na to, kdy≈æ chceme v√Ωsledky z konvertoru kolekce nasypat do seznamu (angl. se tomuto z√°pisu ≈ô√≠k√° list comprehension )... In [480]: [ slovo for slovo in vƒõta if slovo . islower ()] Out[480]: ['n√°s', 'pƒõt'] ... do mno≈æiny (angl. set comprehension )... In [481]: { len ( slovo ) for slovo in vƒõta } Out[481]: {1, 3, 4} ... nebo do slovn√≠ku (angl. dict comprehension ): In [482]: { slovo : len ( slovo ) for slovo in vƒõta } Out[482]: {'Bylo': 4, 'n√°s': 3, 'pƒõt': 3, '.': 1} U ostatn√≠ch kolekc√≠ speci√°ln√≠ syntax neexistuje, ale nen√≠ proƒç si zoufat, jejich konstruktory vƒõt≈°inou podporuj√≠ inicializaci na z√°kladƒõ zdrojov√©ho iterovateln√©ho objektu, tak≈æe staƒç√≠ vepsat konvertor kolekce do konstruktoru: In [483]: tuple ( slovo . lower () for slovo in vƒõta ) Out[483]: ('bylo', 'n√°s', 'pƒõt', '.') In [484]: from nltk import FreqDist FreqDist ( slovo . lower () for slovo in vƒõta ) Out[484]: FreqDist({'bylo': 1, 'n√°s': 1, 'pƒõt': 1, '.': 1}) Konvertory kolekc√≠ jsou velmi u≈æiteƒçn√© v kombinaci s logick√Ωmi funkcemi all a any (viz v√Ω≈°e odd. Logika): In [485]: all ( x < 10 for x in range ( 5 )) Out[485]: True In [486]: any ( x == 2 for x in range ( 5 )) Out[486]: True In [487]: all ( x == 2 for x in range ( 5 )) Out[487]: False Konvertor≈Øm kolekce t√©≈æ ≈ô√≠k√°me gener√°torov√© v√Ωrazy (angl. generator expression ), podle toho, ≈æe jejich v√Ωsledkem je gener√°tor -- objekt, kter√Ω generuje dal≈°√≠ objekty (na z√°kladƒõ prvk≈Ø p≈Øvodn√≠ kolekce). Kdy≈æ ho vytvo≈ô√≠me samostatnƒõ, m≈Ø≈æeme si ho i prohl√©dnout: In [488]: gen = ( slovo for slovo in vƒõta ) In [489]: gen Out[489]: <generator object <genexpr> at 0x7fea335dd7b0> In [490]: type ( gen ) Out[490]: generator Gener√°tory poslou≈æ√≠ kdekoli, kde je pot≈ôeba iterovateln√Ω objekt (podobnƒõ jako kolekce nebo funkce range ). Nav√≠c z nich lze prvky vytahovat po jednom pomoc√≠ zabudovan√© funkce next : In [491]: next ( gen ) Out[491]: 'Bylo' Existuj√≠ dvƒõ ≈°irok√© kategorie iterovateln√Ωch objekt≈Ø: kolekce a nƒõco, co bychom mohli souhrnnƒõ nazvat potenci√°ln√≠ ƒçi virtu√°ln√≠ kolekce (sem pat≈ô√≠ gener√°tory, v√Ωstupy funkc√≠ range , reversed , enumerate apod.) Rozd√≠l mezi re√°lnou kolekc√≠ o milionu prvk≈Ø a potenci√°ln√≠ kolekc√≠ o milionu prvk≈Ø je v tom, ≈æe v p≈ô√≠padƒõ re√°ln√© kolekce mus√≠ cel√Ω milion prvk≈Ø z√°rove≈à fyzicky existovat v pamƒõti poƒç√≠taƒçe. U potenci√°ln√≠ kolekce staƒç√≠ m√≠t recept, jak ten milion prvk≈Ø vytvo≈ôit... a≈æ budou pot≈ôeba. ƒåasto nepot≈ôebujeme v≈°echny prvky kolekce najednou, staƒç√≠ n√°m je zpracov√°vat jeden po druh√©m. Pak jsou potenci√°ln√≠ kolekce ide√°ln√≠ -- zab√≠raj√≠ mnohem m√©nƒõ pamƒõti. Jindy ale v≈°echny prvky najednou pot≈ôebujeme, typicky kdy≈æ si je chceme prohl√©dnout. Pak n√°m potenci√°ln√≠ kolekce moc neposlou≈æ√≠: In [492]: ( slovo for slovo in vƒõta ) Out[492]: <generator object <genexpr> at 0x7fea2bf89510> In [493]: enumerate ( slovo for slovo in vƒõta ) Out[493]: <enumerate at 0x7fea2bf86c80> In [494]: reversed ([ 1 , 2 , 3 ]) Out[494]: <list_reverseiterator at 0x7fea2bfbfd30> In [495]: range ( - 2 , 2 ) Out[495]: range(-2, 2) ≈òe≈°en√≠ je na≈°tƒõst√≠ jednoduch√© -- staƒç√≠ potenci√°ln√≠ kolekci donutit, aby vygenerovala v≈°echny prvky, kter√© v n√≠ d≈ô√≠maj√≠, a ulo≈æit je do re√°ln√© kolekce, nap≈ô. do seznamu: In [496]: list ( slovo for slovo in vƒõta ) Out[496]: ['Bylo', 'n√°s', 'pƒõt', '.'] In [497]: list ( enumerate ( slovo for slovo in vƒõta )) Out[497]: [(0, 'Bylo'), (1, 'n√°s'), (2, 'pƒõt'), (3, '.')] In [498]: list ( reversed ([ 1 , 2 , 3 ])) Out[498]: [3, 2, 1] In [499]: list ( range ( - 2 , 2 )) Out[499]: [-2, -1, 0, 1] Drobnosti Pr√°ce se soubory Nejjednodu≈°eji se v Pythonu pracuje se soubory v podobƒõ tzv. ƒçist√©ho textu (angl. plain text ; ƒçasto m√≠vaj√≠ p≈ô√≠ponu .txt ). K otev≈ôen√≠ souboru slou≈æ√≠ zabudovan√° funkce open . Chceme-li do souboru zapisovat, je pot≈ôeba specifikovat m√≥d w (jako write ): In [500]: text = \"koƒçka leze d√≠rou \\n pes oknem\" with open ( \"koƒçka.txt\" , \"w\" , encoding = \"utf-8\" ) as file : # hlaviƒçka file . write ( text ) # tƒõlo Argument encoding je nepovinn√Ω, Python ho automaticky stanov√≠ na z√°kladƒõ nastaven√≠ va≈°eho operaƒçn√≠ho syst√©mu (v podstatƒõ v≈°ude kromƒõ Windows to bude UTF-8). Vzhledem k tomu, ≈æe va≈°e prvn√≠ volba k√≥dov√°n√≠ by v≈ædy mƒõla b√Ωt UTF-8 (viz notebook unicode.ipynb ), je dobr√© si zvyknout ho vypisovat explicitnƒõ a nenechat operaƒçn√≠ syst√©m rozhodovat za v√°s. Syntaxi s kl√≠ƒçov√Ωm slov√≠ƒçkem with se ≈ô√≠k√° context manager . V hlaviƒçce zavol√°me funkci open a jej√≠ v√Ωsledek ulo≈æ√≠me do promƒõnn√© file , kter√° reprezentuje otev≈ôen√Ω soubor. Tƒõlo je pak kontext, v jeho≈æ r√°mci s t√≠mto otev≈ôen√Ωm souborem m≈Ø≈æeme pracovat (v na≈°em p≈ô√≠padƒõ do nƒõj zapisovat). Jakmile kontext skonƒç√≠ (= zru≈°√≠me odsazen√≠), Python za n√°s soubor automaticky zase zav≈ôe, ani≈æ bychom museli ruƒçnƒõ volat metodu file.close() . Co v√≠c, pokud v r√°mci tƒõla nastane nƒõjak√Ω probl√©m (chyba), tak Python soubor taky zav≈ôe, je≈°tƒõ ne≈æ zkolabuje, abychom po sobƒõ nenechali nepo≈ô√°dek. Kdy≈æ chceme soubor znovu naƒç√≠st, m≈Ø≈æeme p≈ôi jeho otev√≠r√°n√≠ specifikovat m√≥d r (jako read ), ale nemus√≠me, proto≈æe je to default. P≈ôi naƒç√≠t√°n√≠ m√°me t≈ôi mo≈ænosti -- buƒè naƒçteme cel√Ω soubor najednou jako jeden dlouh√Ω ≈ôetƒõzec... In [501]: with open ( \"koƒçka.txt\" , encoding = \"utf-8\" ) as file : text = file . read () text Out[501]: 'koƒçka leze d√≠rou\\npes oknem' ... nebo cel√Ω soubor najednou jako seznam ≈ô√°dk≈Ø... In [502]: with open ( \"koƒçka.txt\" , encoding = \"utf-8\" ) as file : lines = file . readlines () lines Out[502]: ['koƒçka leze d√≠rou\\n', 'pes oknem'] ... nebo ho m≈Ø≈æeme zpracov√°vat ≈ô√°dek po ≈ô√°dku pomoc√≠ for-cyklu: In [503]: with open ( \"koƒçka.txt\" , encoding = \"utf-8\" ) as file : for line in file : print ( line , end = \"\" ) koƒçka leze d√≠rou pes oknem Posledn√≠ varianta se hod√≠ zejm√©na v p≈ô√≠padƒõ, ≈æe m√°me velk√Ω textov√Ω soubor, kter√Ω nepot≈ôebujeme (a t√≠m p√°dem ani nechceme) naƒç√≠tat cel√Ω najednou do pamƒõti. Nƒõkdy jsou plaintextov√© soubory strukturovan√© , tak≈æe naƒç√≠tat je ≈ô√°dek po ≈ô√°dku nen√≠ √∫plnƒõ nejlep≈°√≠ zp≈Øsob, jak s nimi pracovat. Nap≈ô. jupyterovsk√© notebooky jsou ve form√°tu JSON , kter√Ω se sna≈æ√≠ reprezentovat z√°kladn√≠ datov√© typy (ƒç√≠sla, ≈ôetƒõzce, seznamy a slovn√≠ky) tak, aby ≈°ly vymƒõ≈àovat mezi r≈Øzn√Ωmi programovac√≠mi jazyky. Kdy≈æ naƒçteme tento notebook jedn√≠m ze standardn√≠ch zp≈Øsob≈Ø popsan√Ωch v√Ω≈°e, tahle struktura se n√°m nevyjev√≠: In [504]: with open ( \"python_crash_course.ipynb\" , encoding = \"utf-8\" ) as file : nb = file . readlines () # prvn√≠ch deset ≈ô√°dk≈Ø souboru s notebookem nb [: 10 ] Out[504]: ['{\\n', ' \"cells\": [\\n', ' {\\n', ' \"cell_type\": \"code\",\\n', ' \"execution_count\": 1,\\n', ' \"metadata\": {},\\n', ' \"outputs\": [\\n', ' {\\n', ' \"data\": {\\n', ' \"text/html\": [\\n'] Vypadne na n√°s jen seznam ≈ôetƒõzc≈Ø (= ≈ô√°dk≈Ø). Podle jejich obsahu se zd√°, ≈æe tam nƒõjak√° struktura bude (vid√≠me odsazen√≠, uvozovky, hranat√© a slo≈æen√© z√°vorky), ale nem≈Ø≈æeme s n√≠ nijak pracovat, jedin√©, co m√°me k dispozici, jsou ≈ô√°dky textu. Na rozpozn√°n√≠ struktury vƒõt≈°iny bƒõ≈æn√Ωch plaintextov√Ωch form√°t≈Ø (tzv. parsov√°n√≠ ) na≈°tƒõst√≠ existuj√≠ knihovny: In [505]: import json with open ( \"python_crash_course.ipynb\" , encoding = \"utf-8\" ) as file : nb = json . load ( file ) Pomoc√≠ knihovny json za n√°s Python naparsuje strukturu textov√©ho souboru a p≈ôevede ji do podoby, s n√≠≈æ um√≠me pracovat (vno≈ôen√© kolekce): In [506]: type ( nb ) Out[506]: dict In [507]: nb . keys () Out[507]: dict_keys(['cells', 'metadata', 'nbformat', 'nbformat_minor']) In [508]: nb [ \"cells\" ][ 0 ][ \"source\" ] Out[508]: ['from utils import TableOfContents\\n', '\\n', 'TableOfContents(\"python_crash_course.ipynb\")'] A tak podobnƒõ. Pokud budete nƒõkdy pot≈ôebovat v Pythonu pracovat s jin√Ωmi typy soubor≈Ø ne≈æ s ƒçist√Ωm textem (nap≈ô. Excel, Word atp.), na 99 % p≈Øjde vygooglit nƒõjakou knihovnu, kter√° v√°m s t√≠m pom≈Ø≈æe. T≈ôeba s excelov√Ωmi tabulkami se dob≈ôe pracuje pomoc√≠ knihovny pandas . V√Ωrazy vs. p≈ô√≠kazy Nƒõkdy je u≈æiteƒçn√© m√≠t jemnƒõj≈°√≠ terminologick√© rozli≈°en√≠ pro r≈Øzn√© ƒç√°sti k√≥du. V√Ωraz (angl. expression ) je jak√Ωkoli kousek pythonovsk√©ho k√≥du, kter√Ω lze vyhodnotit a jeho v√Ωsledek ulo≈æit do promƒõnn√©. Jin√Ωmi slovy, v√Ωraz je cokoli, co m≈Ø≈æeme napsat napravo od oper√°toru = . V≈°echno ostatn√≠ v Pythonu jsou p≈ô√≠kazy (angl. statement ). P≈ô√≠klady: Hlaviƒçka for-cyklu ( for x in y: ) je p≈ô√≠kaz -- sama o sobƒõ nem√° ≈æ√°dn√Ω v√Ωsledek, kter√Ω by ≈°el ulo≈æit do promƒõnn√©. Kl√≠ƒçov√© slov√≠ƒçko for ale m≈Ø≈æe b√Ωt i souƒç√°st√≠ v√Ωrazu, konkr√©tnƒõ gener√°torov√©ho v√Ωrazu, jeho≈æ v√Ωsledkem je gener√°tor: In [509]: ( x for x in range ( 3 )) Out[509]: <generator object <genexpr> at 0x7fea2bfebf90> Podobn√Ω rozd√≠l existuje mezi hlaviƒçkou podm√≠nkov√©ho vƒõtven√≠ ( if x: , elif y: i else: jsou v≈°echno p≈ô√≠kazy) a oper√°torem if ... else , kter√Ω je souƒç√°st√≠ v√Ωrazu: In [510]: 42 if False else 0 Out[510]: 0 A do t≈ôetice v≈°eho dobr√©ho: p≈ôi≈ôazov√°n√≠ promƒõnn√© (nap≈ô. x = 3 ) je p≈ô√≠kaz tvo≈ôen√Ω: jm√©nem promƒõnn√© (zde x ) p≈ôi≈ôazovac√≠m oper√°torem = a libovoln√Ωm v√Ωrazem (zde ƒç√≠seln√Ω liter√°l 3 ) Dokumentace Dokumentace programu slou≈æ√≠ k tomu, aby byl ƒçiteln√Ω nejen pro Python, ale i pro n√°s a dal≈°√≠ program√°tory. To je d≈Øle≈æit√©, proto≈æe kdy≈æ je program nesrozumiteln√Ω, tƒõ≈æko se upravuje / opravuje. Slo≈æitƒõj≈°√≠ m√≠sta v k√≥du si zaslou≈æ√≠ koment√°≈ô . Je ale zbyteƒçn√© komentovat ka≈ædou operaci, zejm√©na kdy≈æ je jej√≠ z√°mƒõr i v√Ωsledek naprosto jasn√Ω u≈æ z k√≥du: In [511]: # seƒç√≠st dvƒõ jedniƒçky a v√Ωsledek ulo≈æit do promƒõnn√° dva dva = 1 + 1 Funkce, kter√© nejsou jen na jedno pou≈æit√≠, by mƒõly m√≠t dokumentaƒçn√≠ ≈ôetƒõzec (angl. docstring ). In [512]: def funkce (): \"\"\"Toto je dokumentaƒçn√≠ ≈ôetƒõzec. Prvn√≠ ≈ô√°dek by mƒõl obsahovat struƒçn√Ω popis, k ƒçemu funkce slou≈æ√≠. Dal≈°√≠ mohou obsahovat detaily, charakteristiku parametr≈Ø funkce atp. \"\"\" pass Dokumentaƒçn√≠ ≈ôetƒõzec k libovoln√© funkci si lze zobrazit pomoc√≠ zabudovan√© funkce help ... In [513]: help ( funkce ) Help on function funkce in module __main__: funkce() Toto je dokumentaƒçn√≠ ≈ôetƒõzec. Prvn√≠ ≈ô√°dek by mƒõl obsahovat struƒçn√Ω popis, k ƒçemu funkce slou≈æ√≠. Dal≈°√≠ mohou obsahovat detaily, charakteristiku parametr≈Ø funkce atp. ... nebo v prost≈ôed√≠ Jupyter pomoc√≠ ? : In [514]: funkce ? Signature: funkce ( ) Docstring: Toto je dokumentaƒçn√≠ ≈ôetƒõzec. Prvn√≠ ≈ô√°dek by mƒõl obsahovat struƒçn√Ω popis, k ƒçemu funkce slou≈æ√≠. Dal≈°√≠ mohou obsahovat detaily, charakteristiku parametr≈Ø funkce atp. File: ~/src/dlukes.github.io/content/notebooks/<ipython-input-512-8724e3419bb6> Type: function Inspiraci, jak ps√°t u≈æiteƒçn√© docstringy, doporuƒçuju hledat u funkc√≠, kter√© sami pou≈æ√≠v√°te :) In [515]: sorted ? Signature: sorted ( iterable , / , * , key = None , reverse = False ) Docstring: Return a new list containing all items from the iterable in ascending order. A custom key function can be supplied to customize the sort order, and the reverse flag can be set to request the result in descending order. Type: builtin_function_or_method","tags":"floss","url":"python-crash-course.html"},{"title":"Make the most of Python Jupyter notebooks","text":"Python-based Jupyter notebooks mostly consist of Python code, obviously, and some Markdown text. But they also offer some very handy functions and shortcuts which are not available in Python itself, and which are really helpful for interactive work. This is my personal best of / reference. The shortcuts fall into two groups: magics : special functions with special syntax whose names start with % (in which case they apply to the rest of the line ‚Üí \"line magics\") or %% (in which case they apply to the entire cell ‚Üí \"cell magics\") command line programs : if you know how to use command line programs, you can do so directly from the notebook by prefixing the command line invocation with ! If you want to follow along, the easiest way is to just click this link and have Binder launch a Jupyter environment with the notebook loaded for you. Or you can download this post in its original notebook format here and load it into your own Jupyter instance yourself. NB: Most of the following also applies to the IPython REPL . Magics The syntax of magic functions is modeled after the syntax of command line programs: to call them, just write their name and evaluate the cell, without any parentheses (unlike regular Python functions, which are called like this: function() ) arguments are separated just by whitespace (in Python, there are commas: function(arg1, arg) ) some have optional arguments ( options ) which tweak their behavior: these are formed by a hyphen and a letter, e.g. -r Getting help You can read more about the magic function system by calling the %magic magic: In [1]: % magic %quickref brings up a useful cheat sheet of special functionality: In [2]: % quickref If you want more information about an object, %pinfo and %pinfo2 are your friends: In [3]: def foo (): \"This foo function returns bar.\" return \"bar\" In [4]: # shows the object's docstring % pinfo foo In [5]: # shows the full source code % pinfo2 foo These are so handy that they have their own special syntax: ? and ?? , placed either before or after the object's name: In [6]: ? foo In [7]: foo ? In [8]: ?? foo In [9]: foo ?? Of course, this also works with magic functions: In [10]: ? %pylab You can also open a documentation popup by pressing Shift+Tab with your cursor placed in or after a variable name. Repeating the command cycles through different levels of detail. Manipulating objects The appeal of an interactive environment like Jupyter is that you can inspect any object you're working with by just evaluating it: In [11]: foo = 1 In [12]: foo Out[12]: 1 %who and %whos will show you all the objects you've defined: In [13]: % who foo In [14]: % whos Variable Type Data/Info ---------------------------- foo int 1 Sometimes though, these objects are large and you don't want to litter your notebook with tons of output you'll delete right afterwards. (Also, if you forget to delete it, your notebook might get too large to save .) That's when you need to use the Jupyter pager, which lets you inspect an object in a separate window. In [15]: foo = \"This is a line of text. \\n \" * 1000 In [16]: % page foo By default, the pager pretty-prints objects using the pprint() function from the pprint module. This is handy for collections, because it nicely shows the nesting hierarchy, but not so much for strings, because special characters like newlines \\n are shown as escape sequences. If you want the string to look like it would if it were a text file, pass the -r option (\"raw\") to page through the result of calling str() on the object instead: In [17]: % page -r foo If you want to inspect the source code of a module, use %pfile on the object representing that module, or an object imported from that module: In [18]: import os from random import choice In [19]: % pfile os In [20]: % pfile choice Sometimes, you create an object which you know you will want to reuse in a different session or maybe in a completely different notebook. A lightweight way to achieve this is using the %store magic: In [21]: % store foo Stored 'foo' (str) You can list the values stored in your database by invoking %store without arguments: In [22]: % store Stored variables and their in-db values: foo -> 'This is a line of text.\\nThis is a line of text.\\ To restore a variable from the database into your current Python process, use the -r option: In [23]: # restores only `foo` % store -r foo In [24]: # restores all variables in the database % store -r And this is how you clear no longer needed variables from storage: In [25]: # removes `foo` % store -d foo In [26]: # removes all variables % store -z In [27]: % store Stored variables and their in-db values: Working with the file system %ls lists files in the directory where your notebook is stored: In [28]: % ls 3foos.py command_line_intro.ipynb pos_tagging.ipynb zipf.ipynb classification.ipynb foo.py regex.ipynb cmudict.ipynb jupyter_magic.ipynb unicode.ipynb collocations.ipynb libraries.ipynb xcorr_vs_conv.ipynb If you provide a path as argument, it lists that directory instead: In [29]: % ls /etc/nginx conf.d / koi-utf nginx.conf sites-available / uwsgi_params fastcgi.conf koi-win proxy_params sites-enabled / win-utf fastcgi_params mime.types scgi_params snippets / If you provide a glob pattern , then only files that match it are listed: In [30]: % ls /etc/nginx/*.conf /etc/nginx/fastcgi.conf /etc/nginx/nginx.conf %ll (\"long listing\") formats the listing as one entry per line with columns providing additional information: In [31]: % ll ~/edu/ total 12805 drwxrwsrwt+ 3 lukes 2000147 Oct 4 18:52 exchange / drwxrwsr-x+ 21 lukes 2041640 Oct 24 14:44 lukes / drwxrwsr-x+ 9 lukes 2060567 Dec 18 21:54 mda / drwxrwsr-x+ 14 lukes 3001102 Oct 31 17:39 python / drwxrwsr-x+ 4 lukes 2004559 Feb 23 2017 r / drwxr-sr-x+ 4 lukes 2002420 Mar 9 2017 textlink / One of those columns indicates file size, which is great, but they're in bytes, which is less great (hard to read at a glance). The -h option makes the file sizes print in human-readable format: In [32]: % ll -h ~/edu/python/syn* -rw-rw-r--+ 1 lukes 1.5G Nov 1 2016 /home/lukes/edu/python/syn2015.gz -rw-rw-r--+ 1 lukes 112M Nov 2 2016 /home/lukes/edu/python/syn2015_sample %%writefile writes the contents of a cell to a file: In [33]: %% writefile foo.py def foo(): \"This foo function returns bar.\" return \"bar\" Writing foo.py %cat prints the contents of a file into the notebook: In [34]: % cat foo.py def foo(): \"This foo function returns bar.\" return \"bar\" %cat is called %cat because it can also con cat enate multiple files (or the same file, multiple times): In [35]: % cat foo.py foo.py def foo(): \"This foo function returns bar.\" return \"bar\" def foo(): \"This foo function returns bar.\" return \"bar\" The output of %cat can be saved into a file with > (if the file exists, it's overwritten): In [36]: % cat foo.py foo.py >3foos.py In [37]: % cat 3foos.py def foo(): \"This foo function returns bar.\" return \"bar\" def foo(): \"This foo function returns bar.\" return \"bar\" Hey! Our 3foos.py is one foo short. Let's add it by appending to the file with >> : In [38]: % cat foo.py >>3foos.py In [39]: % cat 3foos.py def foo(): \"This foo function returns bar.\" return \"bar\" def foo(): \"This foo function returns bar.\" return \"bar\" def foo(): \"This foo function returns bar.\" return \"bar\" There, much better. %less opens a file in the pager (with nice syntax highlighting if it's a Python source file): In [40]: % less foo.py %less is named after the program less , which is used to page through text files at the command line. Why is the original less called \"less\"? Because an earlier pager program was called more (as in \"show me more of this text file\"), and as the saying goes, \"less is more\". (Programmers are fond of dad jokes. I like how this one works on multiple levels -- the literal meaning that less -the-program is intended to replace more -the-program interacts with the figurative meaning that having less is better than having more, and both coalesce into \"use less because it's better than more \".) %cat and %ls are also named after corresponding command line programs. Finding out more about your code When developing, code often behaves differently from what you intended when you wrote it. The following tools might help you find out why. Timing the execution of a piece of code will help you determine if it's slowing you down. The %timeit magic has your back, it runs your code repeatedly and thus provides more reliable estimates. It comes in both line and cell variants. In [41]: % timeit sorted(range(1_000_000)) 61.4 ms ¬± 7.63 ms per loop (mean ¬± std. dev. of 7 runs, 10 loops each) In [42]: %% timeit lst = list(range(1_000_000)) sorted(lst) 68.8 ms ¬± 7.57 ms per loop (mean ¬± std. dev. of 7 runs, 10 loops each) The cell variant can include initialization code on the first line, which is run only once: In [43]: %% timeit lst = list(range(1_000_000)) sorted(lst) 28.8 ms ¬± 4.84 ms per loop (mean ¬± std. dev. of 7 runs, 10 loops each) If you have the memory_profiler library installed, you can load its magic extension and use %memit in the same way as %timeit to get a notion of how much memory your code is consuming. In [44]: % load_ext memory_profiler In [45]: % memit list(range(1_000_000)) peak memory: 82.43 MiB, increment: 35.71 MiB Peak memory is the highest total amount of memory the Python process used when your code ran. Increment is peak memory minus the amount of memory Python used before your code ran. In [46]: %% memit lst = list(range(1_000_000)) even = [i for i in lst if i % 2 == 0] peak memory: 127.82 MiB, increment: 73.29 MiB In [47]: %% memit lst = list(range(1_000_000)) even = [i for i in lst if i % 2 == 0] peak memory: 98.02 MiB, increment: -30.69 MiB If you have a more involved piece of code where multiple functions are called, you may need more granular information about running times than that provided by %timeit . In that case, you can resort to profiling using the %prun magic. Profiling tells you how fast different parts of your code run relative to each other, in other words, where your bottlenecks are. In [48]: import time def really_slow (): time . sleep ( 1 ) def fast (): pass def only_slow_because_it_calls_another_slow_function (): fast () really_slow () In [49]: % prun only_slow_because_it_calls_another_slow_function() The results show up in the pager, here's a copy: 7 function calls in 1.001 seconds Ordered by: internal time ncalls tottime percall cumtime percall filename:lineno(function) 1 1.001 1.001 1.001 1.001 {built-in method time.sleep} 1 0.000 0.000 1.001 1.001 {built-in method builtins.exec} 1 0.000 0.000 1.001 1.001 <ipython-input-81-8d3b1f67a0d9>:3(really_slow) 1 0.000 0.000 1.001 1.001 <ipython-input-81-8d3b1f67a0d9>:9(only_slow_because_it_calls_another_slow_function) 1 0.000 0.000 1.001 1.001 <string>:1(<module>) 1 0.000 0.000 0.000 0.000 <ipython-input-81-8d3b1f67a0d9>:6(fast) 1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects} %prun also has a cell variant: In [50]: %% prun really_slow() fast() 6 function calls in 1.001 seconds Ordered by: internal time ncalls tottime percall cumtime percall filename:lineno(function) 1 1.001 1.001 1.001 1.001 {built-in method time.sleep} 1 0.000 0.000 1.001 1.001 {built-in method builtins.exec} 1 0.000 0.000 1.001 1.001 <string>:2(<module>) 1 0.000 0.000 1.001 1.001 <ipython-input-81-8d3b1f67a0d9>:3(really_slow) 1 0.000 0.000 0.000 0.000 <ipython-input-81-8d3b1f67a0d9>:6(fast) 1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects} Perhaps the most useful magic for development is %debug , which allows you to pause the execution of a piece of code, examine the variables which are defined at that moment in time, resume execution fully or step-by-step etc. You can either pass a statement that you want to debug as argument: In [51]: def foo (): for i in range ( 10 ): print ( \"printing\" , i ) In [52]: % debug foo() NOTE: Enter 'c' at the ipdb> prompt to continue execution. > <string> (1) <module> () ipdb> help Documented commands (type help <topic>): ======================================== EOF cl disable interact next psource rv unt a clear display j p q s until alias commands down jump pdef quit source up args condition enable l pdoc r step w b cont exit list pfile restart tbreak whatis break continue h ll pinfo return u where bt d help longlist pinfo2 retval unalias c debug ignore n pp run undisplay Miscellaneous help topics: ========================== exec pdb ipdb> step --Call-- > <ipython-input-51-e20c0aea6cfb> (1) foo () ----> 1 def foo ( ) : 2 for i in range ( 10 ) : 3 print ( \"printing\" , i ) ipdb> next > <ipython-input-51-e20c0aea6cfb> (2) foo () 1 def foo ( ) : ----> 2 for i in range ( 10 ) : 3 print ( \"printing\" , i ) ipdb> next > <ipython-input-51-e20c0aea6cfb> (3) foo () 1 def foo ( ) : 2 for i in range ( 10 ) : ----> 3 print ( \"printing\" , i ) ipdb> i 0 ipdb> next printing 0 > <ipython-input-51-e20c0aea6cfb> (2) foo () 1 def foo ( ) : ----> 2 for i in range ( 10 ) : 3 print ( \"printing\" , i ) ipdb> next > <ipython-input-51-e20c0aea6cfb> (3) foo () 1 def foo ( ) : 2 for i in range ( 10 ) : ----> 3 print ( \"printing\" , i ) ipdb> i 1 ipdb> quit Or you can invoke plain %debug after an exception has been raised to jump directly to the place where the error occurred, so that you can figure out why things went wrong: In [53]: def foo (): dct = dict ( foo = 1 ) return dct [ \"bar\" ] In [54]: foo () --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-54-c19b6d9633cf> in <module> () ----> 1 foo ( ) <ipython-input-53-29ed6ce0c4f1> in foo () 1 def foo ( ) : 2 dct = dict ( foo = 1 ) ----> 3 return dct [ \"bar\" ] KeyError : 'bar' In [55]: % debug > <ipython-input-53-29ed6ce0c4f1> (3) foo () 1 def foo ( ) : 2 dct = dict ( foo = 1 ) ----> 3 return dct [ \"bar\" ] ipdb> \"bar\" in dct False ipdb> dct.keys() dict_keys(['foo']) ipdb> quit If you want to pause one of your functions and explore its state at a particular point, set a breakpoint using the set_trace() function from the IPython.core.debugger module. The debugger will be automatically invoked when the call to set_trace() is reached during execution: In [56]: from IPython.core.debugger import set_trace def foo (): for i in range ( 2 ): set_trace () print ( \"printing\" , i ) In [57]: foo () > <ipython-input-56-805417d84ad8> (6) foo () 2 3 def foo ( ) : 4 for i in range ( 2 ) : 5 set_trace ( ) ----> 6 print ( \"printing\" , i ) ipdb> i 0 ipdb> continue printing 0 > <ipython-input-56-805417d84ad8> (5) foo () 2 3 def foo ( ) : 4 for i in range ( 2 ) : ----> 5 set_trace ( ) 6 print ( \"printing\" , i ) ipdb> i 1 ipdb> continue printing 1 The Python debugger is called pdb and it has some special commands of its own which allow you to step through the execution. They can be listed by typing help at the debugger prompt (see above), or you can have a look at the documentation . The examples above also illustrate what a typical debugging session looks like (stepping through the program, inspecting variables). When you want to stop debugging, don't forget to quit the debugger with quit (or just q ) at the debugger prompt, or else your Python process will become unresponsive. Plotting Jupyter is tightly integrated with the matplotlib plotting library. Plotting is enabled by running the %matplotlib magic with an argument specifying how the notebook should handle graphical output. %matplotlib notebook will generate an interactive plot which you can resize, pan, zoom and more. A word of caution though: when using this variant, once you're done with the plot , don't forget to \"freeze\" it using the ‚èª symbol in the upper right corner, or else subsequent plotting commands from different cells will all draw into this same plot. In [58]: % matplotlib notebook In [59]: import matplotlib.pyplot as plt In [60]: plt . plot ( range ( 10 )) Out[60]: [<matplotlib.lines.Line2D at 0x7ff8e9b94518>] By contrast, %matplotlib inline will just show a basic plot with a default size: In [61]: % matplotlib inline In [62]: plt . plot ( range ( 10 )) Out[62]: [<matplotlib.lines.Line2D at 0x7ff8e9afa358>] For more information on plotting with matplotlib, see their usage guide . Command line programs The operations listed in the section on magics for working with the file system can of course also be achieved using the corresponding command line programs, so if you know those, no need to memorize the magics. In fact, the magics are often just thin wrappers around the command line programs, which is why they are named the same. In [63]: ! ls --color -hArtl /etc/nginx total 56K -rw-r--r-- 1 root root 3.0K Mar 30 2016 win-utf -rw-r--r-- 1 root root 664 Mar 30 2016 uwsgi_params -rw-r--r-- 1 root root 636 Mar 30 2016 scgi_params -rw-r--r-- 1 root root 180 Mar 30 2016 proxy_params -rw-r--r-- 1 root root 3.9K Mar 30 2016 mime.types -rw-r--r-- 1 root root 2.2K Mar 30 2016 koi-win -rw-r--r-- 1 root root 2.8K Mar 30 2016 koi-utf -rw-r--r-- 1 root root 1007 Mar 30 2016 fastcgi_params -rw-r--r-- 1 root root 1.1K Mar 30 2016 fastcgi.conf drwxr-xr-x 2 root root 4.0K Apr 26 2016 conf.d -rw-r--r-- 1 root root 1.5K Apr 27 2016 nginx.conf drwxr-xr-x 2 root root 4.0K Apr 27 2016 sites-enabled drwxr-xr-x 2 root root 4.0K Aug 15 09:21 sites-available drwxr-xr-x 2 root root 4.0K Aug 15 09:21 snippets The only functionality that I miss among the magics is the ability to take a quick look at part of a possibly very large text file. This can be done with the head command line program, which prints the beginning of a file: In [64]: ! head jupyter_magic.ipynb { \"cells\": [ { \"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [ \"# Make the most of Python Jupyter notebooks\\n\", \"\\n\", \"[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/dlukes/dlukes.github.io/source?filepath=content%2Fnotebooks%2Fjupyter_magic.ipynb)\\n\", \"\\n\", The -n option controls how many lines from the beginning of the file should be printed: In [65]: ! head -n3 jupyter_magic.ipynb regex.ipynb ==> jupyter_magic.ipynb <== { \"cells\": [ { ==> regex.ipynb <== { \"cells\": [ { Similarly, the tail program prints endings of files: In [66]: ! tail -n5 jupyter_magic.ipynb } }, \"nbformat\": 4, \"nbformat_minor\": 2 } Another useful feature of command line execution is that instead of printing the result, you can have it returned as a list of strings corresponding to lines of output. Either by prepending two exclamation marks instead of one: In [67]: !! tail -n5 jupyter_magic.ipynb Out[67]: [' }', ' },', ' \"nbformat\": 4,', ' \"nbformat_minor\": 2', '}'] Or by assigning the expression to a variable: In [68]: out = ! tail -n5 jupyter_magic.ipynb In [69]: out Out[69]: [' }', ' },', ' \"nbformat\": 4,', ' \"nbformat_minor\": 2', '}'] In summary These are just my favorite shortcuts, the ones I find most helpful. Obviously, there are many more, see %magic or %quickref . If you think I've missed a really neat one, let me know!","tags":"floss","url":"jupyter-magic.html"},{"title":"Text munching in R?","text":"R has been gaining traction as a language for data analysis. My feelings about the whole ecosystem are mixed -- it has some incredibly well-designed libraries and a top-of-the-game IDE , but the core language makes me cringe (it feels like \"Perl and Lisp: The Worse Parts\"). Be that as it may, it has undeniably become the go-to programming language for many people for whom programming is not their main breadwinner, many linguists among them. If you're one of these people and wondering whether it's worth undergoing the cognitive burden of learning another language and having to context-switch between them, read on! The main problem with R and large data is of course that R is fast as long as you can load everything into memory at once and use vectorized operations. The whole point of this post is that with the current size of a typical corpus, you often can't do that. You'll have to process the corpus line by line, which means using a for-loop, and these are notoriously slow in R. I'm not pretending this is some new discovery (it's not), I'm just trying to quantify how problematic this slowness is for processing large quantities of text (prohibitive, in my opinion), so that you don't have to figure it out for yourself and can get started learning Python 3 right away instead ;) (Another big problem is that R doesn't have an efficient and versatile hash table data structure.) tl;dr If you're planning to process corpora of hundreds of millions of tokens or more -- spoiler alert, you probably shouldn't do it in R. Task OK, I've specifically complained about R being bad at for-loops and hashes. Let's devise a task that'll show exactly how bad. At the same time, I don't mean to come up with anything particularly convoluted or far-fetched. So here goes: we'll be trying to build a per-part-of-speech frequency distribution of lemmas (dictionary headword forms) from a 120M-token corpus. If you've ever done any linguistic data analysis, I hope you'll agree it's a pretty basic and common task. The corpus data consists of lines with tab-separated word form, lemma and tag fields, plus some additional lines with metadata which do not contain tabs. We'll be skipping those. Here's a glimpse of the corpus format: <!-- WORD LEMMA TAG --> Hladina hladina NNFS1-----A----- jezera jezero NNNS2-----A----- , , Z:-------------- mrtv√° mrtv√Ω AAFS1----1A----- a a J&#94;-------------- ƒçern√° ƒçern√Ω AAFS1----1A----- , , Z:-------------- The first character of the tag indicates the part of speech. For each part of speech, we want to create a separate frequency distribution, i.e. we want to be able to say, for instance, the most frequent noun is X, followed by Y, whereas the most frequent adjective is Z etc. This should nicely exercise all of R's weak spots. Let's get to it! R: 1 day (!) / 15 hours (see EDIT below, but still !) After quite some exploration of various alternatives for the individual subtasks, this is the program that I came up with: library ( stringi ) library ( hash ) # read input from STDIN con <- file ( \"stdin\" , open = \"rt\" ) pos_sets <- hash () start <- Sys.time () while ( TRUE ) { line <- readLines ( con , n = 1 ) if ( length ( line ) == 0 ) { break } # lines with tokens (as opposed to metadata) contain tabs if ( stri_detect_fixed ( line , \"\\t\" )) { # individual token attributes are tab-separated attrs <- stri_split_fixed ( line , \"\\t\" ) # for each token, we're interested in the lemma (headword)... lemma <- attrs [[ 1 ]][ 2 ] # ... and the part-of-speech, which is the first character of the tag tag <- attrs [[ 1 ]][ 3 ] pos <- stri_split_boundaries ( tag , n = 2 , type = \"character\" )[[ 1 ]][ 1 ] # build a per-part-of-speech frequency distribution as a nested hash: # { # \"noun\": { # \"cat\": 5, # \"dog\": 3, # ... # }, # \"verb\": { # \"look\": 2, # ... # }, # ... # } if ( is.null ( pos_sets [[ pos ]])) { pos_sets [[ pos ]] <- hash () } if ( is.null ( pos_sets [[ pos ]][[ lemma ]])) { pos_sets [[ pos ]][[ lemma ]] <- 1 } else { pos_sets [[ pos ]][[ lemma ]] <- pos_sets [[ pos ]][[ lemma ]] + 1 } } } # report running time diff <- Sys.time () - start cat ( sprintf ( \"Done in %g %s.\\n\" , diff , units ( diff )), file = stderr ()) Given the input corpus mentioned above, this code takes 1.33 days to run. Compared to other languages one might conceivably use (see below), this is just ridiculous. Now, I'm certainly not an expert in R, so there may be better ways of doing some of this. But I doubt such improvements, if any, would be of any practical relevance, because even reducing the running time to a tenth of the original duration wouldn't be enough. And if there is a way to go even further, say to a hundredth, which would begin to make R competitive, then I would argue that a language which lets you shoot yourself so spectacularly in the foot performance-wise if you're not hip to some clever tricks should just be avoided for tasks where said performance matters. EDIT: Replacing stri_split_boundaries(tag, n=2, type=\"character\")[[1]][1] above with stri_sub(tag, from=1, to=1) , you can cut the running time down to 15 hours. That's still way too much in comparison with the competitors, and just reinforces one of the points made below: there's often no default and efficient way of doing some basic operations (like string manipulation) in R. This is in great part due to R's emphasis on vectorization, which leads to a proliferation of subtly different functions designed for doing subtly different kinds of vectorized passes over data. Good luck trying to remember them all. And if you pick the wrong one (cf. stri_split_boundaries() vs. stri_sub() ) -- because there are just too many similar ways of achieving the same result and too much documentation to read before you even begin to see what you should use -- you get penalized heavily. This is very programmer-unfriendly design. Contrast this with the Zen of Python : \"There should be one -- and preferably only one -- obvious way to do it.\" With these general considerations out of the way, let's look at some details of how to implement this task in R. In many cases, it's unclear how you should even approach the problem in R, due to missing or confusing built-in functionality . As a result, in addition to having a lousy running time on this task, R also puts a strain on the programmer's time. Reading the data This sounds so basic it should be obvious, right? Not so fast. First of all, the file(\"path/to/file\") function creates a file connection, which is however not open unless you also specify a mode in the open= argument, or alternatively, unless you call the open() function on the connection. Why you would want to create a connection that's not open is beyond me, but R adds insult to injury by allowing readLines() to work on a closed connection: it just opens the connection before doing the reading and closes it afterwards. This means that repeated calls to readLines(unopened_connection, n=1) will repeatedly read the first line of the file , which is most likely not what you want. This is API design level PHP. Second, the corpus is gzip compressed, so you'll need to uncompress it. There are basically two options: have an external program ( zcat ) do the decompression and pipe the data into R via STDIN handle the decompression within R itself As a general rule (for any language), it will always be faster to handle the decompression in a different process on a multi-core system, because the tasks can proceed in parallel. 1 On the other hand, it's more portable not to depend on external programs, and R does have a built-in function to open a connection to a gzipped file, namely gzfile() . Based on tests on shorter inputs, it's about 50% slower than external decompression, which is a somewhat worse performance deterioration than e.g. Python 3 (40% based on the full input). In light of the already dire running time, it's something we can't really afford. Third, having to do the line-by-line reading in a while (TRUE) loop, using a function called readLines() (note the plural) with an argument of 1 , checking the length of the resulting character vector in order to determine the end of the input -- that's just gross. String manipulation R has built-in functions for string matching ( grepl() et al.), not so much for string splitting. This is the point where I got suspicious of the performance of everything and started testing alternatives. I finally ended up using the stringi package, which is fast and has a fairly consistent API. stringr is a set of higher-level wrappers around it, which have however proven somewhat slower than the built-ins in my highly informal testing. O hash map, where art thou? Building a per-part-of-speech frequency distribution of headwords requires an appropriate data structure. As indicated in the comments in the R source, we want to build a nested collection that looks something like this: { \"noun\": { \"cat\": 5, \"dog\": 3, ... }, \"verb\": { \"look\": 2, ... }, ... } The requirements on the data structure we need are the following: it's a collection strings can be used as keys it can be arbitrarily nested key lookup is fast, i.e. constant time In other words, we need a hash (or a dict, in Python terminology). R doesn't have a hash (I'll qualify this statement in a bit). The workhorse data structure in R that satisfies points 1--3 is a list. Unfortunately, it has linear access time . That's not going to work. R also has environments, which it uses to store and access variables. Under the hood, environments are implemented as hashes, but using them as such is a massive pain, because their API isn't meant for it. Fortunately, there's a wrapper package which makes it more convenient. Unfortunately, environments weren't optimized with this use case in mind. They were designed to hold key--value (variable name--variable value) pairs explicitly defined by people as part of their programs, not millions of items extracted from data. As a result, they slow down dramatically once the number of items grows large. (The article in the previous link provides a survey of the state of the art of fast key lookup in R. The state of the art is... dismal. Your only option is basically indexing a data table, which is fine for a finalized data set, but useless when building the data set -- you can't afford to reindex after each new data point.) There's also the hashmap library , which is a wrapper around C++ Boost hashes. However, it doesn't do nesting, so it's of no use to us, and of very limited usefulness in general. Conclusion: technically, we have to concede that R has hashes, but for all practical intents and purposes, it doesn't . There's one last twist, though. Funnily enough, in our use case, it turns out it doesn't really matter anyway. Indeed, it seems the performance of for-loops in R is so egregiously bad that it dwarfs even the inefficiencies accrued by the linear lookup time of lists: if you reimplement the script with lists, it takes just a little longer than the version with hashes, about 1.36 days. (Or maybe it's just that the performance of environment-based hashes becomes so bad when they grow large as to be comparable with that of lists? Who knows, and frankly, I don't care enough to want to find out. If it's the for-loops though, then adding efficient hashes to R won't really solve anything.) EDIT: With stri_sub() substituted for stri_split_boundaries() as detailed above, the code using lists runs in about 1.28 days, which is a much smaller improvement than in the case of the code using hashes (1.33 days ‚Üí 15 hours). Summary If you like R and your reaction to this is, \"That's not fair! R was never meant to do any of this, that's why everything feels so backhanded.\" -- then good, that's basically the gist of this post: don't use R for something it wasn't meant to do . What are the alternatives, then? Task The following details an informal test comparing the speed of R, Python 3, Rust and Perl at processing a large corpus file (~120M tokens, 1.5GB gzipped) and creating a frequency distribution of headwords per part-of-speech. The idea is to see whether R is a viable alternative in this domain, or whether the slowing down caused by the inability to use vectorized computations (because we can't load the entire thing into memory at once) will just be too much. Python 3: 5 minutes Yes, that's right. It takes Python 3 5 minutes to do the same task that took R over a day . The code feels a lot simpler too: import sys import time def main (): pos_sets = {} start = time . time () for line in sys . stdin : if \" \\t \" in line : _ , lemma , tag , _ = line . split ( \" \\t \" , maxsplit = 3 ) pos = tag [ 0 ] # this is an intentionally naive implementation which mimicks # the R code and something an inexperienced coder might do; # a more concise and probably better performing solution could # be achieved using dict.setdefault() or collections.defaultdict # / collections.Counter if pos not in pos_sets : pos_sets [ pos ] = {} if lemma not in pos_sets [ pos ]: pos_sets [ pos ][ lemma ] = 1 else : pos_sets [ pos ][ lemma ] += 1 diff = time . time () - start print ( f \"Done in { diff : .0f } seconds.\" , file = sys . stderr ) if __name__ == \"__main__\" : main () FYI, this was run using Python 3.6. As a rule, use always the most recent version of Python 3 you can (at least 3.5, 3.4 in a pinch; with earlier releases, you may encounter performance issues). In any case, do not use Python 2 for new projects and let it end-of-life in peace. Perl: 13 minutes Perl used to be a popular alternative for text processing. Like R, it has its fair share of nauseating language design and weird quirks, but since it was actually meant for use in this domain, it won't spectacularly let you down. (Unless your data is silently corrupted because you handled text encoding wrong. Perl's behavior in this respect is a relict of a pre-UTF-8-everywhere past, and it's the single biggest reason for why the language should be put out of its misery already.) Here's the code: use strict ; use utf8 ; use open qw(:std :encoding(utf8)) ; my $start = time (); my %pos_sets = (); while ( <> ) { if ( /\\t/ ) { my @attrs = split /\\t/ ; my $lemma = @attrs [ 1 ]; my $tag = @attrs [ 2 ]; my $pos = substr $tag , 0 , 1 ; # auto-vivification: ergonomic, but also made possible by the whole # \"implicit defaults that have a potential of screwing stuff up # without you even knowing about it\" culture of Perl $pos_sets { $pos }{ $lemma } += 1 ; } } my $diff = time () - $start ; print STDERR \"Done in $diff seconds.\\n\" ; Bottom line though, being more than twice as slow as Python 3 (which came as a surprise to me, I must admit) and definitely the worse language, it has little to recommend itself if you're considering to learn a new language for this type of task. Except maybe if you want to continuously log what the program is doing to a terminal -- like output the number of lines processed after each line. Perl is clearly very efficient at writing to a terminal, the running time is basically the same with continuous logging incorporated. By contrast, Python 3 takes about three times longer (~ 15 minutes). (I guess maybe Python flushes output after each print() call, whereas Perl does some smart buffering which results in it not being slowed down by the latency of the terminal...? Who knows, at any rate, it's hardly a \"killer\" feature.) Rust: 1.25 minutes As a compiled, systems-level language, Rust is in a different league compared to the previous contestants: of course it's going to be faster. I included it because it provides a frame of reference. The important takeaway is that we're in the same ballpark with Python 3 (roughly units of minutes), so there's no pressing need to turn to a compiled language for this task. Here's the code, for completeness sake: use std :: io ; use std :: io :: prelude :: * ; use std :: collections :: HashMap ; use std :: time ; type LemmaCount = HashMap < String , i32 > ; type PosSet = HashMap < char , LemmaCount > ; fn main () { let start = time :: SystemTime :: now (); let mut pos_sets = PosSet :: new (); let stdin = io :: stdin (); for line in stdin . lock (). lines () { let line = line . unwrap (); if line . contains ( \" \\t \" ) { let mut attrs = line . split ( \" \\t \" ). skip ( 1 ). take ( 2 ); let lemma = attrs . next (). unwrap (); let tag = attrs . next (). unwrap (); let pos = tag . chars (). take ( 1 ). next (). unwrap (); let pos_set = pos_sets . entry ( pos ). or_insert ( LemmaCount :: new ()); let count_for_lemma = pos_set . entry ( String :: from ( lemma )). or_insert ( 0 ); * count_for_lemma += 1 ; } } let diff = start . elapsed (). unwrap (). as_secs (); println! ( \"Done in {:.0} seconds.\" , diff ); } Note in passing how nicely the Rust code reads for a compiled language. Of course, since it's a much stricter (and safer) language than Python, it's more ceremonious to write and the APIs are more complicated, because they have to adhere to the various memory management guarantees Rust gives you (among other things). But once the code is written, it's very readable and clear. And all necessary functions and data structures are (a) available in the standard library, and (b) plenty efficient. Conclusion Just to be clear: the ultimate purpose of this post is not bashing R (not for being slow at text munching, at any rate); it's to give a convincing account of why it's just not the right tool for the job. And not in a small way, either -- in a way that requires to learn a different tool, there's no way around it. Let me reiterate that my recommendation would hands down be Python 3 . Once the data is extracted, go back to R by all means. Although Python does have a fairly nice high-level data analysis library , it's not my intention to discourage anyone from using R for what it is good at, especially if this is a skill they are already proficient in. The internet is full of people asking advice on which programming language to learn, and the answers are invariably evasive -- it depends on your tastes, what fits your brain better, what your use case is. In the hopes that some people might find opinionated guidance useful for a change (I know I personally often do, when flirting with a new language): if you're looking to process large quantities of text data, the answer is a big, resounding NOT R ! A vectorized postscript Since I ran these on a server with 64 GB of RAM, I figured I might as well try loading everything into memory in R and doing it the proper, vectorized way, while I'm at it. Here's the code, using dplyr : library ( dplyr ) library ( stringi ) start <- Sys.time () con <- file ( \"stdin\" , open = \"rt\" ) corpus <- readLines ( con ) diff <- Sys.time () - start cat ( sprintf ( \"Corpus read in after %g %s.\\n\" , diff , units ( diff ))) corpus <- stri_subset_fixed ( corpus , \"\\t\" ) corpus <- stri_split_fixed ( corpus , \"\\t\" , simplify = TRUE ) freq_dist <- tibble ( POS = stri_sub ( corpus [, 3 ], from = 1 , to = 1 ), LEMMA = corpus [, 2 ] ) %>% group_by ( POS , LEMMA ) %>% summarize ( FREQ = n ()) diff <- Sys.time () - start cat ( sprintf ( \"Finished processing corpus after %g %s.\\n\" , diff , units ( diff ))) Let me say at the outset that this code looks much nicer -- it's clean, modern R, made possible in great part by Hadley Wickham's efforts to redesign the data manipulation vocabulary from the ground up. Note also that we've made a concession on our requirements: the resulting data structure is a tibble, not a hash, i.e. key lookup time is not constant but depends on the size of the data. Well, just loading the corpus into memory took ~18 minutes. The script then ran for several days , in the course of which I checked every now and then to see how much memory it was using: ~35 GB. I don't suppose anyone has that much RAM on their laptop. Then someone rebooted the server before the program could complete. I think you'll agree the experiment is conclusive even so. You could also offload the decompression to a different thread in the same process, but that complicates the implementation. Piping gives you parallelization basically for free. ‚Ü©","tags":"ling","url":"text-munching-in-r.html"},{"title":"Monkey-patching in R","text":"While building a Shiny application with R recently, I've come across the need to invert the filterRange() function in the DT package, which provides a convenient high-level way to add DataTables to your Shiny app. As indicated by its name, this function filters a numeric column in your datatable based on a range, so as it contains only values contained within that range . What I needed was the opposite: include values outside the specified range . The filtering is done server-side and unfortunately, no option is provided out-of-the-box to perform this inversion. One of the solutions is therefore to monkey-patch the filterRange() function in the DT R package, replacing it with a version that filters the outer range instead. Googling for \"monkey patching r\" (currently) yields this blog post , which provides a more complicated though arguably cleaner solution, which introduces a new environment in the search path. My position on this is that if you're worried about cleanliness, you shouldn't be monkey-patching in the first place. Conversely, if you decide monkey-patching is acceptable in your situation, the code required should be as quick and dirty as the thought. Of course, this is R, uncontested king of weird ways of doing anything but the most common data analysis tasks, and even some of those -- so it's never going to be as simple as Python, for instance: import sys sys . stdin = \"foo\" # Aaand done. But it doesn't have to be as complicated as the solution in the blog post above, either. The solution presented here is basically taken from this mailing list post , which has the disadvantage of not containing the key term \"monkey-patch\", which makes it hard to find on Google. It consists in the following steps: Get a handle on the relevant library's namespace with getNamespace() . Make the relevant binding modifiable with unlockBinding() . Define your custom version of the function. Store it in the namespace under the original name. Re-seal everything with lockBinding() . Here's the code for my specific use case with DT::filterRange() : # Monkey patch the filterRange() function in the DT package so that server-side filtering returns # values *outside* the range instead of inside. DT <- getNamespace ( \"DT\" ) unlockBinding ( \"filterRange\" , DT ) #################################################################################################### # This part of the code is deliberately kept as similar to the original as possible, in order to # make potential updates easier. See https://github.com/rstudio/DT/blob/v0.2/R/shiny.R#L474. # filter a numeric/date/time vector using the search string \"lower ... upper\" filterRange = function ( d , string ) { if ( ! grepl ( '[.]{3}' , string ) || length ( r <- strsplit ( string , '[.]{3}' )[[ 1 ]]) > 2 ) stop ( 'The range of a numeric / date / time column must be of length 2' ) if ( length ( r ) == 1 ) r = c ( r , '' ) # lower, r = gsub ( '&#94;\\\\s+|\\\\s+$' , '' , r ) r1 = r [ 1 ]; r2 = r [ 2 ] if ( is.numeric ( d )) { r1 = as.numeric ( r1 ); r2 = as.numeric ( r2 ) } else if ( inherits ( d , 'Date' )) { if ( r1 != '' ) r1 = as.Date ( r1 ) if ( r2 != '' ) r2 = as.Date ( r2 ) } else { if ( r1 != '' ) r1 = as.POSIXct ( r1 , tz = 'GMT' , '%Y-%m-%dT%H:%M:%S' ) if ( r2 != '' ) r2 = as.POSIXct ( r2 , tz = 'GMT' , '%Y-%m-%dT%H:%M:%S' ) } if ( r [ 1 ] == '' ) return ( d <= r2 ) if ( r [ 2 ] == '' ) return ( d >= r1 ) d <= r1 | d >= r2 } # End pastiche of original DT code. #################################################################################################### DT $ filterRange <- filterRange lockBinding ( \"filterRange\" , DT ) EDIT, 2021-04-13: As Vincent Wolowski points out in the comments, you can also call the higher-level function assignInNamespace , which does the unlock/relock dance for you behind the scenes: assignInNamespace('filterRange', filterRange, 'DT') . The last piece of the puzzle concerns UX: the user should understand that the filter applies to the outer range, not the inner one. Visually: This is easily achieved with a few lines of CSS: # datatable-id . noUi-background { background : #3FB8AF ; box-shadow : inset 0 0 3 px rgba ( 51 , 51 , 51 , .45 ); transition : background 450 ms ; } # datatable-id . noUi-connect { background : #FAFAFA ; box-shadow : inset 0 1 px 1 px #f0f0f0 ; } In conclusion, monkey-patching is rarely the most elegant, debuggable and maintainable solution to a problem you're having. More often, it's actually the least elegant (etc.) one. But every once in a while, it's the simplest one, the one with the best hassle/reward ratio (until it comes back to bite you once your codebase has grown or assumptions about the monkey-patched code have changed). At any rate, if you need to resort to it, it's nice to have a quick, googlable how-to, hence this post.","tags":"floss","url":"monkey-patching-in-r.html"},{"title":"\"Responsive\" iframes, e.g. for DokuWiki and Shiny","text":"Sometimes, the best way to embed an interactive element into a website is to use an iframe. Obviously, not when your website is a webapp and that element represents the main functionality it's supposed to provide -- that would be gross. But when your website is mostly textual / graphical content, typically authored within a wiki or blogging platform, and you just want to include this one element to liven it up, iframes are actually a decent (and perhaps the only?) solution. Trouble is, you probably want this Frankenstein monster to actually look good, i.e. seamless if at all possible. But iframes don't have automatic vertical resizing according to their content, which means you'll need to take care of that manually. How? By using the JavaScript messaging API for communication between parent and child frames to send information about window resize events (from parent to child) and height updates (from child to parent). Let's imagine you have a DokuWiki article in which you want to embed a small Shiny app. If you just embed it in your dokuwiki code using an iframe, taking care to remove the border and stretch it horizontally... < html > < iframe id = \"embedded-app\" src = \"https://your.shiny.app/url\" frameborder = \"0\" width = \"100%\" ></ iframe > </ html > ... this will happen: Eww, scrollbar. Messaging to the rescue! First of all, you need to teach your embedded web page to send information about its height to the parent at appropriate times. This can be achieved by adding this piece of JavaScript to it: ( function () { //////////////////////////////////////////// // CONFIGURE THESE TO MATCH YOUR USE CASE // //////////////////////////////////////////// // set this to a selector for the element that contains the entire UI // you want to access via the iframe -- for a Shiny app, it might be // a div with Bootstrap's container-fluid class var containerSelector = \".container-fluid\" ; // this should be the root URL of the parent frame (DokuWiki) which you want // to allow to send messages to the child var allowedOrigin = \"https://dokuwiki.example.com\" /////////////////////// // END CONFIGURATION // /////////////////////// function sendHeightOf ( querySelector ) { var container = document . querySelector ( querySelector ); if ( container . scrollHeight !== undefined ) { var h = container . scrollHeight ; parent . postMessage ( h , \"*\" ); } else { console . log ( \"No element corresponding to querySelector \" + querySelector + \" found, or element did not have property scrollHeight.\" ); } } // cross-browser compatible infrastructure var eventMethod = window . addEventListener ? \"addEventListener\" : \"attachEvent\" ; var eventer = window [ eventMethod ]; var messageEvent = eventMethod == \"attachEvent\" ? \"onmessage\" : \"message\" ; // listen for resize message from parent window (see point ‚ë° below) eventer ( messageEvent , function ( e ) { if ( e . origin == allowedOrigin ) { sendHeightOf ( containerSelector ); } else { console . log ( \"Was expecting a message from \" + allowedOrigin + \", got \" + e . origin + \" instead.\" ); } }); window . onload = function () { // inform parent at least once after load (see point ‚ë† below) sendHeightOf ( containerSelector ); // monitor self-initiated changes in size (see point ‚ë¢ below) var mo = new MutationObserver ( function () { sendHeightOf ( containerSelector ); }); mo . observe ( document , { subtree : true , childList : true , characterData : true }); }; })(); What are these \"appropriate times\" mentioned above? The code above implements the following ones, which should be generic enough to cover most situations: on initial page load on window resize (see below, the parent frame has to send a message to the child frame that it has been resized, to which the child responds with a size update message) on any kind of mutation of the DOM inside the child frame (not a full reload of the entire page, that would be handled by point ‚ë† above), which might affect the size of the rendered component On the parent (DokuWiki) side, you then need to handle the incoming size update messages from the child frame, and send resize messages when the window is resized. This can be achieved with the following DokuWiki markup: < html > < iframe id = \"embedded-app\" src = \"https://your.shiny.app/url\" frameborder = \"0\" width = \"100%\" ></ iframe > < script > ( function () { //////////////////////////////////////////// // CONFIGURE THESE TO MATCH YOUR USE CASE // //////////////////////////////////////////// // this should be the root URL of the child frame (Shiny app) which you want // to allow to send messages to the parent var allowedOrigin = \"https://your.shiny.app\" /////////////////////// // END CONFIGURATION // /////////////////////// var embeddedApp = document . getElementById ( \"embedded-app\" ); function resizeIframe ( pixels ) { embeddedApp . style . height = pixels + \"px\" ; } // cross-browser compatible infrastructure var eventMethod = window . addEventListener ? \"addEventListener\" : \"attachEvent\" ; var eventer = window [ eventMethod ]; var messageEvent = eventMethod == \"attachEvent\" ? \"onmessage\" : \"message\" ; // listen to message from iframe eventer ( messageEvent , function ( e ) { if ( e . origin === allowedOrigin ) { var key = e . message ? \"message\" : \"data\" ; var data = e [ key ]; resizeIframe ( data ); } else { console . log ( \"Was expecting a message from \" + allowedOrigin + \", got \" + e . origin + \" instead.\" ); } }, false ); // send message to iframe on window resize window . onresize = function () { embeddedApp . contentWindow . postMessage ( \"parentWindowResized\" , \"*\" ); }; })(); </ script > </ html > And the result? Yay! And of course, the iframe gets resized as needed when display conditions change: Cue bittersweet feeling after having figured out a workaround for such a specific use case that you're not quite sure it was worth putting all that effort into it in the first place...","tags":"floss","url":"responsive-iframe.html"},{"title":"The Cathedral and the Bazaar: What is a Useful Notion of \"Language\"?","text":"If you like the essay, then you'll definitely want to take a look at Luc Steels's The Talking Heads Experiment: Origins of Words and Meanings . It's published as an open-access book by Language Science Press, so go grab the free download ! Abstract The essay analyzes why Noam Chomsky's notion of language (both its essence ‚Äî language as a set of grammatical sentences ‚Äî and genesis) leads neither to interesting discoveries nor even to useful questions from the point of view of linguistics as a science. A much more fruitful approach to language is to view it as a complex, dynamic, distributed system with emergent properties stemming from its functions, as advocated e.g. by Luc Steels. The argument will be developed against the backdrop of the evolution of Ludwig Wittgenstein's thought, from the Tractatus to the concept of language games, i.e. from an approach to language based on thorough formal analysis but also misconceptions about its functions, to a much keener though less formal grasp of its praxis and purpose. Introduction At least since Thomas Kuhn's The Structure of Scientific Revolutions , it has been a fairly commonplace notion that working within the confines of a particular scientific paradigm conditions to a certain extent the questions one is likely to ask and therefore also the answers that ensue. This effectively limits the range of possible discoveries, because some are not answers to meaningful questions within a given framework while other observations still are taken as given axioms, which means they cannot be the target of further scientific investigation. In contemporary linguistics, one very prominent such paradigm is that of generative grammar , single-handedly established in 1957 by Noam Chomsky in his seminal work Syntactic Structures . While serious criticism has been leveled over time against this initial exposition as well as Chomsky's subsequent elaborations on it (see Pullum 2011; Sampson 2015; and Sampson 2005 for a book-length treatment), the book undeniably attracted significant numbers of brilliant young minds under the wings of its research program, which went from aspiring challenger in the domain of linguistics to established heavyweight in a comparatively short period of time (the transition had been achieved by the mid-1970s at the latest). In the process, it co-opted or spawned various other sub-fields of linguistics, and even rebranded itself, such that Cartesian linguistics , cognitive linguistics and most recently biolinguistics are all labels which suggest a strong generativist presence. One serious competitor to the Chomskyan account of language that has emerged over the years is the field of evolutionary linguistics . It might seem strange at first glance why biolinguistics and evolutionary linguistics should be at odds. As their names indicate, they both aspire to a close relationship with biology, which seems to indicate their research agendas and outlooks should largely overlap. Yet their fundamental assumptions about what constitutes language are so irreconcilable that they might as well be considered to deal with different objects of study. Of the two, it is evolutionary linguistics which leads to questions and investigations which can be conceived of as scientific (in the Popperian sense of involving falsifiable hypotheses instead of being merely speculative), consequently yielding the most useful insights ‚Äì in the fairly pedestrian sense that these can be intersubjectively replicated without resorting to an argument from authority, which makes them a better foundation to build upon, because the superadded structures are less likely to crumble should said authority ever change their mind, as Chomsky has done several times already. Wittgenstein on language: From logical calculus to language games Let us now take a short d√©tour through the development of Ludwig Wittgenstein's thoughts on language, so that we may couch our later discussion of the differences between generative grammar / biolinguistics and evolutionary linguistics in terms of a contrast that is perhaps more familiar. The imagery in the title of the present essay was borrowed from Eric S. Raymond's book The Cathedral & the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary (Raymond 1999). In it, Raymond describes two models of collaborative software development, one of them very rigid, restrictive and hostile to newcomers (the \"cathedral\"), the other overwhelmingly inclusive, open to outside contributions and organic change, an effervescent hive of activity (the \"bazaar\"), whose unexpected but empirically demonstrable virtues he has come to embrace. This architectural metaphor also happens to be very apt when characterizing Wittgenstein's view of language in the two major stages of his thought, as represented by his two books Tractatus Logico-Philosophicus and Philosophical Investigations . In the Tractatus , Wittgenstein has a \"preconceived idea of language as an exact calculus operated according to precise rules\" (McGinn 2006, 12) and formalizing this system of rules leads him to the following dogmatic conclusion: \"what can be said at all can be said clearly, and what we cannot talk about we must pass over in silence\" (Wittgenstein 2001, 3). The deontic force of the final injunction should be taken with a grain of salt; it could perhaps be rephrased in the following less epigraph-worthy manner: there is a sharp logical boundary to be drawn between meaningful and nonsensical propositions, and the purpose of language is to construct meaningful ones, therefore it is futile (rather than strictly forbidden) to engage in nonsensical ones. There have been attempts to read the Tractatus in an ironic mode, as a consciously doomed, self-defeating attempt to circumscribe the limits to the expression of thought, which prefigures the much more subtle attitude towards language that Wittgenstein later exhibits in the Philosophical Investigations (see McGinn 2006, 5‚Äì6 and elsewhere for an overview of this so-called \"resolute\" reading). In my opinion, such a stance exhibits a blatant, possibly wilful disregard of his almost penitent tone in the preface to Philosophical Investigations : \"I could not but recognize grave mistakes in what I set out in that first book\" (Wittgenstein 2009, 4e). Where does Wittgenstein think he went wrong then? Arguably, the most serious misconception was conferring a privileged ontological status to language, seeing it as \"the unique correlate, picture, of the world\" (Wittgenstein 2009, 49e), whereas in fact, these referential properties are highly dependent on communicative context. It signifies only insofar as it has an effect on the addressee (another human being, or even myself) which to all practical intents and purposes the speaker can identify as somehow related to what she was trying to achieve with her utterance in the first place. But there is little meaning, in any practical sense, outside these highly particular, localized (in both physical and cultural space and time), embodied, grounded interactions. Wittgenstein calls these interactions \"language-games\" to \"emphasize the fact that the speaking of language is part of an activity, or of a form of life\" (Wittgenstein 2009, 15e, original emphasis). Of course, the notion of game still involves some kind of rules, but by focusing on the activity rather than its regulations, it is much easier to account for \"the case where we play, and make up the rules as we go along [‚Ä¶] and even where we alter them ‚Äì as we go along\" (Wittgenstein 2009, 44e). This is not to say that we cannot construct abstractions, although Wittgenstein himself is clearly in favor of systematically examining particular cases: \"In order to see more clearly, here as in countless similar cases, we must look at what really happens in detail , as it were from close up\" (Wittgenstein 2009, 30e). However, once we do abstract away, it is crucial to approach the resulting theory from a pragmatic standpoint: \"We want to establish an order in our knowledge of language: an order for a particular purpose, one out of many possible orders, not the order\" (Wittgenstein 2009, 56e, original emphasis). Generative grammar In many ways, Chomsky conceives of language as the early Wittgenstein did, i.e. under the cathedral metaphor. This may come across as a surprise because unlike Wittgenstein, he is not concerned with issues of meaningfulness, philosophical or otherwise. Indeed, it is one of his fundamental precepts that grammar can and should be dissociated from meaning, as demonstrated by his famous example sentence \"Colorless green ideas sleep furiously\", which he claims is perfectly grammatical yet meaningless 1 (Chomsky 2002, Chap. 2). Nevertheless, the goal for both is to describe language per se , in the abstract, without regard to its context-grounded use in actual communication. Both strive to give a highly formal definition of the system they think they are uncovering: while Wittgenstein attempts to establish a logical calculus of how propositions can be said to carry meaning in terms of their referential relationship to external reality, Chomsky tries to hint at a calculus which would determine which candidate sentences belong to the language encoded by this calculus, i.e. separate those that are grammatical from those that are not. Another way to put this is that the descriptive part of linguistics can be equated with formal language theory: a language is viewed as a (potentially infinite) set of symbol strings (sentences), and the linguist's task is to find the simplest and most elegant set of rules that would constitute the basis for a procedure to generate (hence generative grammar) all of them and only those, whether observed or potential. Chomsky himself gives the following definition: \"by a generative grammar I mean simply a system of rules that in some explicit and well-defined way assigns structural descriptions to sentences\" (Chomsky 1965, 8). Furthermore, \"Linguistic theory is concerned primarily with an ideal speaker-listener, in a completely homogeneous speech-community, who knows its language perfectly\" (Chomsky 1965, 3). Specifically, it is concerned with his \" competence (the speaker-hearer's [intrinsic] knowledge of his language)\", not his \" performance (the actual use of language in concrete situations)\" (Chomsky 1965, 4). Additionally, no claim is made as to the cognitive or neurophysiological accuracy of the mechanisms described, although to hedge his bets both ways, Chomsky adds that \"No doubt, a reasonable model of language use will incorporate, as a basic component, the generative grammar that expresses the speaker-hearer's knowledge of the language\" (Chomsky 1965, 9). In short, Chomsky consciously sets up the playing field for a thoroughly mentalistic, speculative discipline. At first, it might seem like reasonable approximation and a small concession to make, especially in the face of the sheer daunting complexity of all the intricate mechanisms that conspire to yield the phenomenon we call language, but only until one fully realizes the consequences of such a move. Observe for instance the carefully crafted loophole claiming that linguistics is primarily concerned with an ideal speaker-hearer's competence and that actual usage data is just circumstantial evidence. This effectively allows linguists to dismiss inconvenient edge cases or counterexamples to their theories purely on grounds of their being noisy data or slips of the tongue, which is something they are allowed to determine based on introspection. Mind you, this is not just a theoretical loophole; Chomsky himself has repeatedly relied on it, especially with respect to so-called linguistic universals (see e.g. Sampson 2005, 139 or 160). The net result is rampant, unchecked theorizing. One such example is the postulation of a two-layer linguistic analysis, the observed language data corresponding to a surface structure which provides hints as to an underlying, more regular deep structure to be uncovered. The deep structure is purportedly closer to the universal properties of language; both layers are linked by a system of transformations: We can greatly simplify the description of English and gain new and important insight into its formal structure if we limit the direct description in terms of phrase structure to a kernel of basic sentences (simple, declarative, active, with no complex verb or noun phrases), deriving all other sentences from these (more properly, from the strings that underlie them) by transformation, possibly repeated. (Chomsky 2002, 106‚Äì7) Constructing a formal framework for grammar modeling with cognitively unmotivated levels of abstraction might have been a valid goal (though arguably not within linguistics) if the result was indeed, as Chomsky claims it to be, maximally elegant, as simple as can be but no simpler. I was not able to track down a formal definition of this criterion, but simplicity is clearly discursively construed as a desirable quality: \"simple and revealing\" (Chomsky 2002, 11) or \"effective and illuminating\" (Chomsky 2002, 13) are Chomsky's choice epithets for what to look for in a grammar. But that is not true either: transformations are an unnecessary addition, singling them out as a separate category of operations adds nothing to the generative power of his system (Pullum 2011, 290). They are therefore a wart under any reasonable definition of \"simplicity\" and Chomsky thus manages to fall short of even the self-defined, theory-internal standards that are the only ones he allows his enterprise to be held to. Ontogeny and phylogeny As we have seen, generative grammar concerns itself with an ideal speaker-hearer's competence in a perfectly homogeneous community. The trouble is that such an impoverished model eschews any possibility of dynamism. The very dichotomy between grammatical and ungrammatical is intuitively problematic if we consider that judgments are bound to diverge when made in reference to different dialects, sociolects and idiolects, 2 not to mention that binary classification might be too reductive in some cases (how would you categorize, on first encounter, a construction which you passively understand but would never produce actively?). Though Chomsky sometimes mentions in passing the possibility of levels of grammaticalness which would allow a finer-grained analysis (e.g. Chomsky 2002, 16; Chomsky 1965, 11), it seems to be just another instance of hedging his bets, he never makes it a fundamental component of his theory. This would seem to indicate that Chomsky's theory of language has a serious problem in that it is unable to account for any phenomena that involve fluctuations in linguistic ability, including language emergence / diachronic change (phylogeny) and acquisition (ontogeny). Chomsky's response to this is that our language faculty is largely innate: we are genetically endowed with a language-acquisition device in our brains (Chomsky 1965, 31‚Äì33) which can supposedly infer the correct grammatical rules even given incomplete, limited and noisy input, which is what Chomsky argues children get (the \"poverty of stimulus\" argument), thanks to strong universal constraints on what a human language can be like. Under this account, the capacity for language, initially \"a language of thought, later externalized and used in many ways\" (Chomsky 2007, 24), appeared as a random mutation in a single individual and progressively spread through the population because it offered a considerable competitive advantage: \"capacities for complex thought, planning, interpretation\" (Chomsky 2007, 22). In his later career, Chomsky increasingly focused on exploring this purported shared genetic basis for human language, hence the aforementioned label \"biolinguistics\". Make no mistake, this in no way entails a turn from mentalism towards empirical neurophysiological or genetic investigation. Quite to the contrary, liberated from the constraints of having to account for individual existing languages in detail, he soars to new heights of abstractness in postulating the formal language underpinnings of human language. The principles and parameters model of Universal Grammar (Chomsky 1986) expands upon the notion of linguistic universals by splitting them up into two sets: principles, which are hardwired and immutable, and parameters, which are hardwired too but can be flipped on or off based on linguistic behavior observed by the child in her particular language community. Since the choices are heavily constrained, the learner can infer correct parameter settings in spite of deficient input. This line of research culminates in the so-called minimalist program (Chomsky 1995), where Chomsky identifies the \"core principle of language, [the operation of] unbounded Merge\" (Chomsky 2007, 22). Under the \"strong\" minimalist hypothesis, this would be the only principle necessary to account for human-like languages (Chomsky 2007, 20), which would paradoxically essentially discard all work (or should I say speculation?) previously done on the parameters side of the Universal Grammar project. All other universal characteristics of language could then be explained by newly introduced \"interface\" conditions, 3 i.e. constraints on how language inter-operates with other systems, including thought and physical language production (Chomsky 2007, 14); 4 all empirically documented variation between the world's languages would be chalked up to lexical differences (Chomsky 2007, 25). The poverty of stimulus and language universals arguments for innateness have been thoroughly debunked, especially in Geoffrey Sampson's book-length diatribe The ‚Äò Language Instinct ' Debate . In short, it turns out that some of the grammatical constructions which were assumed to be absent from a language learner's input yet acquired nonetheless have since been empirically proven to occur fairly commonly (Sampson 2005, 72‚Äì79). Moreover, there is no qualitative difference between a statement like \"the stimulus is too poor to allow language learning without a genetic basis\" and \"the stimulus is just rich enough etc.\", both are unverifiable unless we have already independently proven that language learning occurs one way or the other, so to adduce either of the statements as proof for the hypothesis at stake is misguided (Sampson 2005, 47‚Äì48). Finally, the alleged language universals turn out to be either false when checked against additional languages (Sampson 2005, 138‚Äì9) or so general as to be meaningless (Sampson 2005, Chap. 5). Irrespective of this, let us suppose for a moment that genetic mutation and subsequent inheritance do play a role in the emergence of language, and work out an account of language emergence consistent with this hypothesis. If language started out through mutation in a single individual as a purely internal advanced conceptualization faculty, then once it started to spread, what was the motivation for the genetically-endowed humans to externalize their thoughts? How did they know to which of their peers they could speak (which had inherited the mutation) and which not? And most importantly, how did they know which parameters of Universal Grammar to flip on and which off, if there was no prior language based on which to decide? Universal Grammar would have had to be fairly detailed in order for intersubjective agreement on the norms for the first ever human language to be reached on the basis of it alone. Yet as we have seen, Chomsky has been moving away from this notion ‚Äì at the limit, the minimalist program posits only one very general mechanism required for language. The poverty of stimulus argument is turned against its creator as the argument from poverty of the machinery supposed to make up for the poverty of said stimulus. Alternatively, if we fully subscribe to the minimalist program and the notion that all the surface variety exhibited by language comes from the lexicon, then how are individual words created, how do they propagate? One might be tempted to say \"people just invented them\", but consider for a while that in the current setup, there is absolutely no mechanism that would explain how a community of speakers reaches agreement on their lexicon ‚Äì this theory offers no incentive whatsoever for consensus to be reached; from its point of view, a solution where each speaker ends up with their own private lexicon is equally valid because indistinguishable on the basis of the theory's conceptual apparatus. Chomsky's ideas on phylogenesis appear thoroughly ridiculous when fully carried out to their logical consequences, and this can all be blamed on his sterile, idealized and static view of language which dismisses actual communication as a secondary purpose and therefore a peripheral issue. On a side note, it is hard to say which aspect of Chomsky's theory of language came first ‚Äì whether innateness accommodated the mentalism and the concomitant quest for formal purity (botched as it may be) of generative grammar, whether it was the other way round, or whether they perhaps co-evolved in his mind. The facts are that Chomsky's initial publications on generative grammar concentrate on the formal language theory part (Chomsky 1956; Chomsky 2002), but he added the innateness argument fairly early on, even tacking a seemingly respectable philosophical lineage onto it in Cartesian Linguistics (Chomsky 2009), which pretends to trace back both innateness and mentalism to Descartes and the Port-Royal grammarians, binding them as two sides of the same coin. It is worth noting that in both formal language theory and history of linguistics / philosophy, Chomsky is more of a dabbler than an expert: he has provably borrowed most of his ideas in the former field from others, sometimes mangling them or extending them in unfortunate ways (Pullum 2011; Sampson 2015), and has thoroughly underresearched (or wilfully twisted?) his understanding of the latter, which has resulted in serious misrepresentations of the history of ideas (Miel 1969; Aarsleff 1970). Evolutionary linguistics There are various sub-fields of linguistics which are in discord with generative grammar, especially over the notion that performance data should be used only as evidence for guiding the speculation and detailed usage and frequency patterns should be disregarded; the primacy of syntax (as advocated by Chomsky) is also disputed. One of these sub-fields is obviously corpus linguistics, which takes a decidedly empiricist stance and starts by assembling a large body of language data (a corpus) from which patterns of language use are inferred. Nevertheless, not all of these compete with generative grammar at the fundamental explanatory level of how language came about phylogenetically and how it is transmitted by ontogenetic acquisition. We have repeatedly encountered Chomsky's emphasis on how communication, actual interactions between speakers, are just an afterthought in the system of language: evolutionary biologist Salvador Luria was the most forceful advocate of the view that communicative needs would not have provided \"any great selective pressure to produce a system such as language,\" with its crucial relation to \"development of abstract or productive thinking.\" His fellow Nobel laureate Fran√ßois Jacob (1977) added later that \"the role of language as a communication system between individuals would have come about only secondarily, as many linguists believe,\" (Chomsky 2007, 23) Part of this vehemence dovetails with the single individual mutation hypothesis of the origins of language ‚Äì it helps if the significance of communication is downplayed in an account where communication is initially impossible, simply because there is no other language-endowed being to communicate with. If communication were language's killer feature, then the selective pressure for the incriminated gene to propagate would not kick in. The other part can reasonably be attributed to Chomsky's intent to make a clean break from a prior popular theory on language acquisition, epitomized by B. F. Skinner's 1957 monograph Verbal Behavior , which offered a heavily empiricist, behaviorist account of language learning in terms of a stimulus-response cycle. Characteristically, Chomsky's strategy is to trivialize the function of the stimulus, casually implying both that it might not be needed at all, and if it is, then details of the role it plays are of little interest: it would not be at all surprising to find that normal language learning requires use of language in real-life situations, in some way. But this, if true [sic!], would not be sufficient to show that information regarding situational context (in particular, a pairing of signals with structural descriptions that is at least in part prior to assumptions about syntactic structure) plays any role in determining how language is acquired, once the mechanism is put to work and the task of language learning is undertaken by the child. (Chomsky 1965, 33) In retrospect, Skinner's account may be simplistic in many ways, but the basic notion that one has to pay attention to stimuli and responses in the course of particular linguistic interactions is sound. In particular, a theory of language built on this foundation successfully copes with all of the impasses we have explored above regarding Chomsky's approach. One such framework is that of evolutionary linguistics. Evolutionary linguistics views language as a complex adaptive system with emergent properties (Steels 2015, 8‚Äì9). A complex adaptive system is a system which is not centrally organized, coordinated or designed: its \"macroscopic\" characteristics are said to \"emerge\" as the result of localized interactions between individual entities (agents) with similar \"microscopic\" characteristics (be they physical, behavioral or motivational). The whole is more than the sum of its parts, and none of the agents can be properly said to have designed the system, nor can they deliberately change it in an arbitrary way; but all are continuously shaping it by taking part in the interactions that constitute its fabric. Examples of complex adaptive systems include the dynamics of insect societies (beehives, ant nests) or patterns of collective motion in large animal groups (flocks of birds or shoals of fish). These and more are discussed in much greater depth in the first chapter of Pierre-Yves Oudeyer's book Self-Organization in the Evolution of Speech . Adaptiveness is a property that these systems acquire by virtue of not being hardwired on the macro level: they are defined functionally instead of structurally. If the conditions in the environment change, the system will adapt to keep fulfilling its function, because the agents are forced to modify their behavior in order to achieve their individual goals. Of course, they may fail to do so, in which case the system breaks down and ceases to exist. If we revert to the metaphor from the title of the present essay, according to Chomsky, language is a cathedral erected by a single unwitting architect, the random genetic mutation that endowed us with the language faculty. Conversely, Luc Steels and fellow evolutionary linguists argue that the apparent macroscopic orderliness of language is the result of a myriad interactions of multiple individual agents, as suggested by the the bazaar image. One form that linguistic research can take under this paradigm is formulating and running computational models which simulate the behavior of agent populations and study the microscopic conditions, i.e. the cognitive and physical abilities, motivations etc. of each agent, necessary for a system like language to emerge within the population and stabilize. By direct inspiration from Wittgenstein's Philosophical Investigations , the interactions between agents are termed \"language games\" (Steels 2015, 167‚Äì8); depending on the topic being investigated, the agents can play different types of language games with different rules. It is openly acknowledged that such simulations represent only a limited approximation of a well-defined subspace of the actual uses of language. In the research to date, rules are generally definite and set for the entire experiment, but simulating language games with fuzzy rules remains a perfectly valid research topic within this framework, in the Wittgensteinian spirit of allowing rules to be made up and modified \"as we go along\" (Wittgenstein 2009, 44e). A relatively simple game that agents can play is the so-called Guessing Game (see Chap. 2 of Steels 2015 for more details). In this scenario, a population of agents, embodied in physical robots, tries to establish a shared lexicon and coupled ontology for a simple world consisting of geometrical shapes. Each game is an interaction of two agents picked at random, in the context of a scene consisting of said geometrical shapes. One agent (the speaker) takes the initiative, selects a topic from the scene and names it; the other (the hearer) tries to guess which object the first one had in mind and points to it; the speaker decodes the pointing gesture and the game succeeds if he interprets it as referencing his original topic. If so, he acknowledges the match; otherwise, he points at the intended topic as a repair strategy. At the outset, neither the lexicon nor the ontology are given, only a set of sensors and actuators (which allow the agents to interact with the environment by taking in streams of raw perceptual data or producing sound and pointing gestures) and very general cognitive principles. These include an associative memory and feedback mechanisms to propagate failures and successes in conceptualization and communication to all components of the system and act on them. 5 New distinctions along the perceptual dimensions are introduced in a random fashion, 6 but those that lead to a successful unambiguous selection of a topic and communicative success are strengthened over the course of many interactions, while useless ones are dampened by lateral inhibition and eventually pruned. At the same time, speakers create new words for concepts that are as of yet missing from their lexicon, and hearers may adopt them into theirs for their conceptualization of the topic the speaker points at in case of failure. A similar feedback mechanism then ensures that highly successful words are preferred and come to dominate within the speech community. It is important to realize that at no time do the individual agents share the same ontology or lexicon: newly introduced distinctions and words are random and unique for each agent, agents simply gradually learn which of these are useful in achieving communicative success, which means that they naturally settle on ontologies and lexicons that are close enough to those of others in the population. This barely scratches the surface of how all these notions must be orchestrated for a working computer implementation of this model, not to mention the even more elaborate agent-based language game modeling experiments that are already being conducted, investigating for instance the emergence of grammar (see Part III of Steels 2015 for an overview of recent scholarship). We see that even a seemingly simple task like establishing a shared conceptualization of reality and agreeing on names for these concepts is a complex endeavor which relies on a highly sophisticated (though also highly general) machinery. Another key observation is that while agent-based models can be fully virtual, grounding them in physical reality (cf. the use of robots with sensors and actuators) brings additional challenges that enable researchers to reach vital insights which would otherwise be impossible. In particular, grounding introduces fuzziness on the sensory input channels (by virtue of different points of view for the two robots and analog-to-digital conversion) which the agents must cope with, or else the mechanisms they were endowed with cannot be considered as constituting a plausible, sufficient model of the dynamics of human language. Unlike in generative grammar, anything that is transient, imperfect, is eminently included in the purview of linguistic inquiry. Failures are very much part of the dynamics that steer the evolution of language. How could it adapt to the speakers' changing requirements if it did not include appropriate repair strategies? Indeed, how could it be bootstrapped at all? The reward is a model that successfully simulates not only language emergence, but also transmission: if virgin agents are added into an existing population, they gradually acquire its language (see Fig. 1). Figure 1. Communicative success in a population of agents with a steady influx of virgin agents and outflux of old ones (overall population size remains the same). The game starts in phase 1 with 20 virgin agents; phases 2 and 4 show the behavior of the model at an agent renewal rate of 1/1000 games, whereas phase 3 corresponds to a heavier rate of 1/100. (Figure from Steels 2015, 121.) Crucially, the drive to communicate, to interact, is built into the agents, they just keep playing games as long as they can. But if it was not, the simulation would have to be more complex and somehow elicit this drive by introducing appropriate ecological constraints, e.g. by requiring co-operation as a survival strategy (Steels 2015, 106). Otherwise, the agents would have no motivation to strive for communicative success in their mutual encounters, they would fail to reach intersubjective alignment of their conceptual spaces and lexicons, and language would not emerge. In other words, far from being an afterthought, successful communication with a partner, grounded in an external context, turns out to be a fundamental requirement to establish the kind of dynamics which allow languages to appear. Paradoxically, since it concerns itself with simulations and computational models, this branch of evolutionary linguistics is, like generative grammar, also highly speculative. However, unlike generative grammar, it is a kind of speculation which considers guidance by empirical observations a necessity, not a nuisance. Furthermore, simulations are meant to be tested: if an agent-based model of language emergence fails to converge on the result stipulated for that particular experiment, the model is plain wrong and the dynamics it is trying to put into place (cognitive strategies, feedback propagation etc.) need to be revised. There is thus a clear-cut criterion for validity. Lastly, even if a simulation works, an accompanying debate as to whether the mechanisms involved are actually plausible approximations of reality is considered an integral part of hypothesis evaluation, with evidence from strongly empirically grounded disciplines like biology and neurophysiology a vital element in the process. Conclusion Taking a cue from Wittgenstein's Philosophical Investigations , this essay should not be construed as an attempt to replace one doctrine with another, but to advocate a \"change of attitude\" (cf. McGinn 2013, 33) which allows asking more meaningful questions about language. This being said, on the evidence presented above, it is hard not to conclude that Noam Chomsky is fundamentally mistaken about the corrective that is necessary for language learning to take place. According to Chomsky, the criterion for evaluating linguistic rules lies within a dedicated language organ we are genetically endowed with; the innate structures themselves embody the metric by which conjectures pertaining to linguistic rules will be judged. By contrast, in the evolutionary linguistics perspective, genetics provide innate structures which are capable of random growth, but the feedback (reinforcement and pruning) which results in steering this growth in a particular direction comes from interactions with the environment. This theory presupposes much less specificity in the hardware infrastructure which makes this possible and so should be preferred both on grounds of simplicity and flexibility of the model, not to mention that it is biologically plausible and has been empirically verified to work. In the context of science, Chomsky's rhetorical strategy in and of itself is dishonest: he preaches formal rigor while practicing sleight of hand, and casually retreats to increasingly abstract ground on reaching an impasse. He thus carves out a region in discursive space which has no corresponding equivalent in a logically consistent conceptual space, without which a piece of discourse can hardly constitute a scientific theory. In other words, much like his famous example sentence \"Colorless green ideas sleep furiously\", his discourse is grammatical but largely nonsensical under the requirements on a system of thought which aspires to mirror reality in a coherent fashion. Requirements on scientific discourse notwithstanding, we as linguists should keep in mind that language in general is much more than a system for encoding logical propositions. Even Wittgenstein had to resign himself to the fact ‚Äì or perhaps knew all along ‚Äì that the Tractatus , which he framed as the ultimate solution to all metaphysical controversies, could only fan the flames of philosophical debate. It was after all addressed to a diverse community of people bound perhaps exclusively by their penchant for elaborate language games. References Aarsleff, Hans. 1970. \"The History of Linguistics and Professor Chomsky.\" Language 46 (3): 570‚Äì85. Chomsky, Noam. 1956. \"Three Models for the Description of Language.\" ‚Äî‚Äî‚Äî. 1965. Aspects of the Theory of Syntax . Cambridge, MA: The M.I.T. Press. ‚Äî‚Äî‚Äî. 1986. Knowledge of Language: Its Nature, Origin, and Use . Convergence. New York, Westport, London: Praeger. ‚Äî‚Äî‚Äî. 1995. The Minimalist Program . Cambridge, MA: The MIT Press. ‚Äî‚Äî‚Äî. 2002. Syntactic Structures . 2nd ed. Berlin, New York: Mouton de Gruyter. ‚Äî‚Äî‚Äî. 2007. \"Of Minds and Language.\" Biolinguistics , no. 1: 9‚Äì27. ‚Äî‚Äî‚Äî. 2009. Cartesian Linguistics: A Chapter in the History of Rationalist Thought . 3rd ed. Cambridge: Cambridge University Press. McGinn, Marie. 2006. Elucidating the Tractatus: Wittgenstein's Early Philosophy of Logic and Language . Oxford: Oxford University Press. ‚Äî‚Äî‚Äî. 2013. The Routledge Guidebook to Wittgenstein's Philosophical Investigations . The Routledge Guides to Great Books. Routledge. Miel, Jan. 1969. \"Pascal, Port-Royal, and Cartesian Linguistics.\" Journal of the History of Ideas 30 (2): 261‚Äì71. Oudeyer, Pierre-Yves. 2006. Self-Organization in the Evolution of Speech . Translated by James R. Hurford. Oxford, New York: OUP. Pullum, Geoffrey K. 2011. \"On the Mathematical Foundations of Syntactic Structures .\" Journal of Logic, Language and Information 20: 277‚Äì96. Raymond, Eric S. 1999. The Cathedral & the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary . O'Reilly Media. Sampson, Geoffrey. 2005. The \"Language Instinct\" Debate . 3rd ed. London, New York: Continuum. ‚Äî‚Äî‚Äî. 2015. \"Rigid Strings and Flaky Snowflakes.\" Language and Cognition 10: 1‚Äì17. Skinner, B. F. 1957. Verbal Behavior . The Century Psychology Series. New York: Appleton ‚Äì Century ‚Äì Crofts. Steels, Luc. 2015. The Talking Heads Experiment: Origins of Words and Meanings . Computational Models of Language Evolution 1. Berlin: Language Science Press. Wittgenstein, Ludwig. 2001. Tractatus Logico-Philosophicus . Routledge Classics. London, New York: Routledge. ‚Äî‚Äî‚Äî. 2009. Philosophical Investigations . Chichester, United Kingdom: Blackwell Publishing. Let us pretend for a moment that language games like \"poetry\" or the surrealist pastime of cadavre exquis do not exist; in these, the quoted sentence could appear as perfectly valid and meaningful, though perhaps not in the sense that Chomsky intended. \"Meaningful\" in a late-Wittgensteinian perspective could be paraphrased as \"accepted by at least one involved party as a valid turn within the context of a particular language game\". ‚Ü© Arguing that this does not matter because we should be concerned with the ideal speaker-hearer's competence just takes us further down the impasse, because now we have to determine how to delimit the purported \"ideal\". ‚Ü© This is the beauty of building empirically unmotivated, purely speculative theories: at any moment, one can freely accommodate a new element into the existing framework, substituting novelty and amalgamation for critical evaluation. ‚Ü© Cf. also Sampson's riposte : \"If complex properties of some aspect of human behaviour have to be as they are as a matter of conceptual necessity, then there is no reason to postulate complex genetically inherited cognitive machinery determining those behaviour patterns\" (Sampson 2015, 9). ‚Ü© This interconnected architecture stands in stark contrast to Chomsky's deliberately isolationist approach: \"the relation between semantics and syntax [‚Ä¶] can only be studied after the syntactic structure has been determined on independent grounds\" (Chomsky 2002, 17). ‚Ü© Wittgenstein only hints at the problem of conceptualization, but he is prescient in realizing it is not a given: \"The primary elements [of the objects which constitute the world in this particular language game] are the coloured squares. ‚ÄòBut are these simple?' ‚Äì I wouldn't know what I could more naturally call a ‚Äòsimple' in this language-game. But under other circumstances, I'd call a monochrome square, consisting perhaps of two rectangles or of the elements colour and shape, ‚Äòcomposite'\" (Wittgenstein 2009, 27e). ‚Ü©","tags":"ling","url":"cathedral-and-bazaar.html"},{"title":"Configuring Emacs Daemon on Mac OS X","text":"I know I promised this article a loooong time ago (June 2014, when I first got a Mac, to judge by the previous timestamp in the header of this file), but since the historically attested readership of this blog is 2 + a bunch of my facebook friends who I nagged to read my attempt at explaining character encodings to non-technical people , I don't suppose it's as if a legion of fans have been restlessly looking forward to this one ;) Nevertheless, the distinct advantage is that my OS X Emacs setup has had the opportunity to grow more mature and also much simpler in the meantime, which means that if a third reader accidentally stumbles over this note (exploding my ratings...), they might actually find something genuinely useful here. tl;dr This article presents a way to start Emacs Daemon (a persistent Emacs session) from the GUI and subsequently connect to it (creating frames on demand) using an Automator script . The benefit is that you incur startup time lag only once (when you start the daemon) while still being able to close all frames when you're not using Emacs, keeping a clean workspace . This is especially useful if your Emacs is heavily customized and loading it takes a while . Another benefit is that whenever you open a frame connected to an Emacs daemon, all your previously open buffers are still there as you left them (as opposed to opening a fresh instance of Emacs). Skim over the code blocks to get the important gist without the verbose sauce. Tested on OS X 10.11 El Capitan, with Homebrew Emacs and Spacemacs config. Why Emacs Daemon, why this post Installing Emacs on a Mac in and of itself is not that much of a problem -- there are several options, ranging from Homebrew and Macports to Emacs for Mac OS X , Emacs Mac Port and Aquamacs . The last two in this list have some OS X specific tweaks (smooth scrolling, tabs, adapted keyboard shortcuts), which makes them perhaps more appealing out of the box but also less extensible, as some of the information out there about generic Emacs might not apply to them as straightforwardly or indeed at all. With that in mind, if you want to tinker with your Emacs config, it's a good idea to stick with Homebrew's fairly conservative version of Emacs: $ brew update $ brew install emacs --with-cocoa # this step gets you a standard OS X launcher icon $ brew linkapps emacs But now that you've got Emacs, and especially if you're transferring some heavy customization over from say Linux, you might be unhappy that each time you start it from cold, it takes a while, typically a few seconds. That's what emacs --daemon and emacsclient are for: Emacs is run as a daemon in the backround and you connect to it with client frames that spawn almost instantly . This also means that you can close all existing frames to keep your workspace clean if you won't be using Emacs for a while (hard to imagine, right, since you can even read xkcd from inside Emacs ) and then whip up a frame at the speed of a thought when need arises. Now this is all easy to achieve when using the terminal , but since you probably bought that Mac in great part for its shiny pretty elegant ergonomic GUI, you might want Emacs to use GUI frames instead of terminal ones and connect to the Emacs daemon (or start it if it's not running) by just clicking on an app icon in the launcher or finding it from Spotlight. That's where Automator comes in. An Automator script Automator is a built-in OS X app for creating custom automated user workflows for just about any installed app you might have or even OS functionality. Among other things, this means that it allows you to wrap the daemon auto-start functionality available from the terminal (as described in the previous paragraph) into an app launchable from the GUI. Let's get down to business: Launch Automator and create a new document. Select Application as its type. Search the Actions palette on the left for the Run Shell Script action and add it to your Automator document. In the Run Shell Script building block, change the following: set Shell to the shell you're using and whose init files have thus the PATH correctly set to the emacs and emacsclient executables (if you're using Homebrew, it probably told you how to properly set up your PATH as a post-install step) set Pass input to \"as arguments\" (if you then set this Automator app as the default for opening a given type of file , you'll be able to use emacsclient to open files by double-clicking on them in Finder) Finally, paste in the following code snippet and save the app e.g. as EmacsClient.app , preferably in your Applications folder so that it is easily accessible from the launcher. emacsclient --no-wait -c -a emacs \" $@ \" >/dev/null 2 > & 1 & EDIT : An earlier version of this article had nohup prepended to the command above; as pointed out in the comments by MaTres (thanks!), this is unnecessary . At the end of the day, your Automator EmacsClient.app should look something like this: The core of the command that you might want to tweak based on your particular Emacs setup is emacsclient --no-wait -c -a emacs ; mine is optimized to work with mostly stock Spacemacs config (see below). If it doesn't work, you might also want to try a simple emacsclient -c -a \"\" and variations; a good debugging technique is to try these out in the terminal: as soon as you get the line working there, it'll start working in the Automator task as well. \"$@\" is just the list of files (if any) passed to Emacs to open (the aforementioned double-click in Finder use case). The rest is some black magic to ensure that the shell which spawns the Emacs process (because this Automator app is after all, at heart, only a shell script) totally and utterly disowns it, so that the shell script is allowed to return and the Automator task completes as soon as Emacs has started (or the client has spawned a new frame). Otherwise, you'd end up with an irritating spinning cog wheel in your notification area which would stay there until you completely quit Emacs. Which is probably not what you want, since you're undergoing all this hassle in the first place to get a zen, distraction-free Emacs experience. The details of the various incantations are discussed in this Apple forum thread , but let's have a whirlwind tour for the moderately interested (my knowledge of Unix processes is far from perfect, so feel free to correct me on these points!): >/dev/null redirects standard output to oblivion and 2>&1 redirects standard error to standard output (i.e. also to oblivion), which persuades Automator that you're really not expecting to hear from the process via these standard streams ever again, so there's no point in keeping the shell script running. These can be shortened to &>/dev/null . the final & runs the command in the background, which ensures control of the shell is returned to the user as soon as the process is spawned; since there are no additional commands in the shell script and all remaining ties have been severed, Automator finally agrees that the task has probably done all it was expected to do and exits it. Wrapping up Whew! That's it. It's really not that complicated, it's just that my prose is verbose, so it makes it look like there's lots and lots to do. Trust me, there isn't. My first go at solving this usability problem -- the one I originally wanted to post way back in 2014 -- was a lengthy, godawful Applescript prone to subtle breakage. This is much better. And the ability to just use a single GUI app for transparently launching and connecting to the Emacs daemon is pure bliss. While you're at it, for an even better Emacs experience, go fetch the excellent Spacemacs Emacs config distribution , which pulls this venerable piece of software screaming into the 21st century. The best editor is neither Vim nor Emacs, its Vim + Emacs! The addictive icing of Vim modal editing on the outside, a creamy Elisp core -- what more could you want from life? ;) Oh and if, like me, you love Spacemacs' snappy icon with the Evil spaceship over planet Emacs -- or if, like me, you have OCD -- you'll definitely want to switch your Emacs logo to the Spacemacs one !","tags":"macOS","url":"emacs-daemon-osx.html"},{"title":"How computers handle text: a gentle but thorough introduction to Unicode","text":"Or, the absolute minimum every software developer linguist absolutely, positively must know about Unicode and character sets (no excuses!) Note : This text uses the Python programming language to give some hands-on experience of the concepts discussed. If you're not familiar with programming at all, much less with Python, my advice is: either: ignore the code, focus on the comments around it, they should be enough to follow the thread of the explanation; or, if you've got a little more time: Python is pretty intuitive, so you might want to have a look at the code examples anyway. In any case, the code might make more sense to you if you tinker with it in an interactive Python session: And now, without further ado... Much like any other piece of data inside a digital computer, text is represented as a series of binary digits (bits), i.e. 0's and 1's. A mapping between sequences of bits and characters is called an encoding. How many different characters your encoding can handle depends on how many bits you allow per character: with 1 bit you can have 2&#94;1 = 2 characters (one is mapped to 0, the other to 1) with 2 bits you can have 2&#94;2 = 2*2 = 4 characters (mapped to 00, 01, 10 and 11) with 3 bits you can have 2&#94;3 = 2*2*2 = 8 characters etc. Now, a short digression on representing numbers, to make sure we're all on the same page: in the context of computers, you often see the same number represented in three different ways: as a decimal number, using 10 digits 0--9 (e.g. 12) as a binary number, using only 2 digits, 0 and 1 (e.g. 1100, which equals decimal 12) as a hexadecimal number, using 16 digits: 0--9 and a--f (e.g. c, which also equals decimal 12) As a consequence, the need often arises to convert between these different numeral systems . If you don't want to do so by hand, Python has your back! The bin() function gives you a string representation of the binary form of a number: In [1]: bin ( 65 ) Out[1]: '0b1000001' As you can see, in Python, binary numbers are given a 0b prefix to distinguish them from regular (decimal) numbers. The number itself is what follows after (i.e. 1000001). Similarly, hexadecimal numbers are given an 0x prefix: In [2]: hex ( 65 ) Out[2]: '0x41' To convert in the opposite direction, i.e. to decimal, just evaluate the binary or hexadecimal representation of a number: In [3]: 0b1000001 Out[3]: 65 In [4]: 0x41 Out[4]: 65 Numbers using these different bases (base 2 = binary, base 10 = decimal, base 16 = hexadecimal) are mutually compatible, e.g. for comparison purposes: In [5]: 0b1000001 == 0x41 == 65 Out[5]: True Why are hexadecimals useful? They're primarily a more convenient, condensed way of representing sequences of bits: each hexadecimal digit can represent 16 different values, and therefore it can stand in for a sequence of 4 bits (2&#94;4 = 16). In [6]: 0xa == 0b1010 Out[6]: True In [7]: 0xb == 0b1011 Out[7]: True In [8]: # if we paste together hexadecimal a and b, it's the # same as pasting together binary 1010 and 1011 0xab == 0b10101011 Out[8]: True In other words, instead of binary 10101011, we can just write hexadecimal ab and save ourselves some space. Of course, this only works if shorter binary numbers are padded to a 4-bit width : In [9]: 0x2 == 0b10 Out[9]: True In [10]: 0x3 == 0b11 Out[10]: True In [11]: # if we paste together hexadecimal 2 and 3, we have to # paste together binary 0010 and 0011... 0x23 == 0b00100011 Out[11]: True In [12]: # ... not just 10 and 11 0x23 == 0b1011 Out[12]: False The padding has no effect on the value, much like decimal 42 and 00000042 are effectively the same numbers. Now back to text encodings. The oldest encoding still in widespread use is ASCII , which is a 7-bit encoding. What's the number of different sequences of seven 1's and 0's? In [13]: # this is how Python spells 2&#94;7, i.e. 2*2*2*2*2*2*2 2 ** 7 Out[13]: 128 This means ASCII can represent 128 different characters , which comfortably fits the basic Latin alphabet (both lowercase and uppercase), Arabic numerals, punctuation and some \"control characters\" which were primarily useful on the old teletype terminals for which ASCII was designed. For instance, the letter \"A\" corresponds to the number 65 ( 1000001 in binary, see above). \"ASCII\" stands for \" American Standard Code for Information Interchange\" -- which explains why there are no accented characters, for instance. Nowadays, ASCII is represented using 8 bits (= 1 byte), because that's the unit of computer memory which has become ubiquitous (in terms of both hardware and software assumptions), but still uses only 7 bits' worth of information. That extra bit means that there's room for another 128 characters in addition to the 128 ASCII ones , coming up to a total of 256. In [14]: 2 ** ( 7 + 1 ) Out[14]: 256 What happens in the range [128; 256) is not covered by the ASCII standard. In the 1990s, many encodings were standardized which used this range for their own purposes, usually representing additional accented characters used in a particular region. E.g. Czech (and Slovak, Polish...) alphabets can be represented using the ISO latin-2 encoding, or Microsoft's cp-1250 . Encodings which stick to the same character mappings as ASCII in the range [0; 128) and represent them physically in the same way (as 1 byte) , while potentially adding more character mappings beyond that, are called ASCII -compatible . ASCII compatibility is a good thing‚Ñ¢, because when you start reading a character stream in a computer, there's no way to know in advance what encoding it is in (unless it's a file you've encoded yourself). So in practice, a heuristic has been established to start reading the stream assuming it is ASCII by default, and switch to a different encoding if evidence becomes available that motivates it. For instance, HTML files should all start something like this: <!DOCTYPE html> < html > < head > < meta charset = \"utf-8\" /> ... This way, whenever a program wants to read a file like this, it can start off with ASCII , waiting to see if it reaches the charset (i.e. encoding) attribute, and once it does, it can switch from ASCII to that encoding ( UTF-8 here) and restart reading the file, now fairly sure that it's using the correct encoding. This trick works only if we can assume that whatever encoding the rest of the file is in, the first few lines can be considered as ASCII for all practical intents and purposes. Without the charset attribute, the only way to know if the encoding is right would be for you to look at the rendered text and see if it makes sense; if it did not, you'd have to resort to trial and error, manually switching the encodings and looking for the one in which the numbers behind the characters stop coming out as gibberish and are actually translated into intelligible text. Let's take a look at printable characters in the Latin-2 character set . The character set consists of mappings between positive integers (whole numbers) and characters; each one of these is called a codepoint . The Latin-2 encoding then defines how to encode each of these integers as a series of bits (1's and 0's) in the computer's memory. In [15]: latin2_printable_characters = [] # the Latin-2 character set has 256 codepoints, corresponding to # integers from 0 to 255 for codepoint in range ( 256 ): # the Latin-2 encoding is simple: each codepoint is encoded # as the byte corresponding to that integer in binary byte = bytes ([ codepoint ]) character = byte . decode ( encoding = \"latin2\" ) if character . isprintable (): latin2_printable_characters . append (( codepoint , character )) latin2_printable_characters Out[15]: [(32, ' '), (33, '!'), (34, '\"'), (35, '#'), (36, '$'), (37, '%'), (38, '&'), (39, \"'\"), (40, '('), (41, ')'), (42, '*'), (43, '+'), (44, ','), (45, '-'), (46, '.'), (47, '/'), (48, '0'), (49, '1'), (50, '2'), (51, '3'), (52, '4'), (53, '5'), (54, '6'), (55, '7'), (56, '8'), (57, '9'), (58, ':'), (59, ';'), (60, '<'), (61, '='), (62, '>'), (63, '?'), (64, '@'), (65, 'A'), (66, 'B'), (67, 'C'), (68, 'D'), (69, 'E'), (70, 'F'), (71, 'G'), (72, 'H'), (73, 'I'), (74, 'J'), (75, 'K'), (76, 'L'), (77, 'M'), (78, 'N'), (79, 'O'), (80, 'P'), (81, 'Q'), (82, 'R'), (83, 'S'), (84, 'T'), (85, 'U'), (86, 'V'), (87, 'W'), (88, 'X'), (89, 'Y'), (90, 'Z'), (91, '['), (92, '\\\\'), (93, ']'), (94, '&#94;'), (95, '_'), (96, '`'), (97, 'a'), (98, 'b'), (99, 'c'), (100, 'd'), (101, 'e'), (102, 'f'), (103, 'g'), (104, 'h'), (105, 'i'), (106, 'j'), (107, 'k'), (108, 'l'), (109, 'm'), (110, 'n'), (111, 'o'), (112, 'p'), (113, 'q'), (114, 'r'), (115, 's'), (116, 't'), (117, 'u'), (118, 'v'), (119, 'w'), (120, 'x'), (121, 'y'), (122, 'z'), (123, '{'), (124, '|'), (125, '}'), (126, '~'), (161, 'ƒÑ'), (162, 'Àò'), (163, '≈Å'), (164, '¬§'), (165, 'ƒΩ'), (166, '≈ö'), (167, '¬ß'), (168, '¬®'), (169, '≈†'), (170, '≈û'), (171, '≈§'), (172, '≈π'), (174, '≈Ω'), (175, '≈ª'), (176, '¬∞'), (177, 'ƒÖ'), (178, 'Àõ'), (179, '≈Ç'), (180, '¬¥'), (181, 'ƒæ'), (182, '≈õ'), (183, 'Àá'), (184, '¬∏'), (185, '≈°'), (186, '≈ü'), (187, '≈•'), (188, '≈∫'), (189, 'Àù'), (190, '≈æ'), (191, '≈º'), (192, '≈î'), (193, '√Å'), (194, '√Ç'), (195, 'ƒÇ'), (196, '√Ñ'), (197, 'ƒπ'), (198, 'ƒÜ'), (199, '√á'), (200, 'ƒå'), (201, '√â'), (202, 'ƒò'), (203, '√ã'), (204, 'ƒö'), (205, '√ç'), (206, '√é'), (207, 'ƒé'), (208, 'ƒê'), (209, '≈É'), (210, '≈á'), (211, '√ì'), (212, '√î'), (213, '≈ê'), (214, '√ñ'), (215, '√ó'), (216, '≈ò'), (217, '≈Æ'), (218, '√ö'), (219, '≈∞'), (220, '√ú'), (221, '√ù'), (222, '≈¢'), (223, '√ü'), (224, '≈ï'), (225, '√°'), (226, '√¢'), (227, 'ƒÉ'), (228, '√§'), (229, 'ƒ∫'), (230, 'ƒá'), (231, '√ß'), (232, 'ƒç'), (233, '√©'), (234, 'ƒô'), (235, '√´'), (236, 'ƒõ'), (237, '√≠'), (238, '√Æ'), (239, 'ƒè'), (240, 'ƒë'), (241, '≈Ñ'), (242, '≈à'), (243, '√≥'), (244, '√¥'), (245, '≈ë'), (246, '√∂'), (247, '√∑'), (248, '≈ô'), (249, '≈Ø'), (250, '√∫'), (251, '≈±'), (252, '√º'), (253, '√Ω'), (254, '≈£'), (255, 'Àô')] Using the 8th bit (and thus the codepoint range [128; 256)) solves the problem of handling languages with character sets different than that of American English, but introduces a lot of complexity -- whenever you come across a text file with an unknown encoding, it might be in one of literally dozens of encodings. Additional drawbacks include: how to handle multilingual text with characters from many different alphabets, which are not part of the same 8-bit encoding? how to handle writing systems which have way more than 256 \"characters\", e.g. Chinese, Japanese and Korean (CJK) ideograms? For these purposes, a standard character set known as Unicode was developed which strives for universal coverage of (ultimately) all characters ever used in the history of writing, even adding new ones like emojis . Unicode is much bigger than the character sets we've seen so far -- its most frequently used subset, the Basic Multilingual Plane , has 2&#94;16 codepoints, but overall the number of codepoints is past 1M and there's room to accommodate many more. In [16]: 2 ** 16 Out[16]: 65536 Now, the most straightforward representation for 2&#94;16 codepoints is what? Well, it's simply using 16 bits per character, i.e. 2 bytes. That encoding exists, it's called UTF-16 (\"UTF\" stands for \"Unicode Transformation Format\"), but consider the drawbacks: we've lost ASCII compatibility by the simple fact of using 2 bytes per character instead of 1 (encoding \"a\" as 01100001 or 00000000|01100001 , with the | indicating an imaginary boundary between bytes, is not the same thing) encoding a string in a language which is mostly written down using basic letters of the Latin alphabet now takes up twice as much space (which is probably not a good idea, given the general dominance of English in electronic communication) Looks like we'll have to think outside the box. The box in question here is called fixed-width encodings -- all of the encoding schemes we've encountered so far were fixed-width, meaning that each character was represented by either 7, 8 or 16 bits. In other word, you could jump around the string in multiples of 7, 8 or 16 and always land at the beginning of a character. (Not exactly true for UTF-16 , because it is something more than just a \"16-bit ASCII \": it has ways of handling characters beyond 2&#94;16 using so-called surrogate sequences -- but you get the gist.) The smart idea that some bright people have come up with was to use a variable-width encoding . The most ubiquitous one currently is UTF-8 , which we've already met in the HTML example above. UTF-8 is ASCII -compatible, i.e. the 1's and 0's used to encode text containing only ASCII characters are the same regardless of whether you use ASCII or UTF-8 : it's a sequence of 8-bit bytes. But UTF-8 can also handle many more additional characters, as defined by the Unicode standard, by using progressively longer and longer sequences of bits. In [17]: def print_utf8_bytes ( char ): \"\"\"Prints binary representation of character as encoded by UTF-8. \"\"\" # encode the string as UTF-8 and iterate over the bytes; # iterating over a sequence of bytes yields integers in the # range [0; 256); the formatting directive \"{:08b}\" does two # things: # - \"b\" prints the integer in its binary representation # - \"08\" left-pads the binary representation with 0's to a total # width of 8, which is the width of a byte binary_bytes = [ f \" { byte : 08b } \" for byte in char . encode ( \"utf8\" )] print ( f \" { char !r} encoded in UTF-8 is: { binary_bytes } \" ) print_utf8_bytes ( \"A\" ) # the representations... print_utf8_bytes ( \"ƒç\" ) # ... keep... print_utf8_bytes ( \"Â≠ó\" ) # ... getting longer. 'A' encoded in UTF-8 is: ['01000001'] 'ƒç' encoded in UTF-8 is: ['11000100', '10001101'] 'Â≠ó' encoded in UTF-8 is: ['11100101', '10101101', '10010111'] How does that even work? The obvious problem here is that with a fixed-width encoding, you just chop up the string at regular intervals (7, 8, 16 bits) and you know that each interval represents one character. So how do you know where to chop up a variable width-encoded string, if each character can take up a different number of bits? Essentially, the trick is to use some of the bits in the representation of a codepoint to store information not about which character it is (whether it's an \"A\" or a \"Â≠ó\"), but how many bits it occupies . In other words, if you want to skip ahead 10 characters in a string encoded with a variable width-encoding, you can't just skip 10 * 7 or 8 or 16 bits; you have to read all the intervening characters to figure out how much space they take up. Take the following example: In [18]: for char in \"B√°sn√≠k ÊùéÁôΩ\" : print_utf8_bytes ( char ) 'B' encoded in UTF-8 is: ['01000010'] '√°' encoded in UTF-8 is: ['11000011', '10100001'] 's' encoded in UTF-8 is: ['01110011'] 'n' encoded in UTF-8 is: ['01101110'] '√≠' encoded in UTF-8 is: ['11000011', '10101101'] 'k' encoded in UTF-8 is: ['01101011'] ' ' encoded in UTF-8 is: ['00100000'] 'Êùé' encoded in UTF-8 is: ['11100110', '10011101', '10001110'] 'ÁôΩ' encoded in UTF-8 is: ['11100111', '10011001', '10111101'] Notice the initial bits in each byte of a character follow a pattern depending on how many bytes in total that character has: if it's a 1-byte character, that byte starts with 0 if it's a 2-byte character, the first byte starts with 11 and the following one with 10 if it's a 3-byte character, the first byte starts with 111 and the following ones with 10 This makes it possible to find out which bytes belong to which characters, and also to spot invalid strings, as the leading byte in a multi-byte sequence always \"announces\" how many continuation bytes (= starting with 10) should follow. So much for a quick introduction to UTF-8 (= the encoding), but there's much more to Unicode (= the character set). While UTF-8 defines only how integer numbers corresponding to codepoints are to be represented as 1's and 0's in a computer's memory, Unicode specifies how those numbers are to be interpreted as characters, what their properties and mutual relationships are, what conversions (i.e. mappings between (sequences of) codepoints) they can undergo, etc. Consider for instance the various ways diacritics are handled: \"ƒç\" can be represented either as a single codepoint ( LATIN SMALL LETTER C WITH CARON -- all Unicode codepoints have cute names like this) or a sequence of two codepoints, the character \"c\" and a combining diacritic mark ( COMBINING CARON ). You can search for the codepoints corresponding to Unicode characters e.g. here and play with them in Python using the chr(0xXXXX) built-in function or with the special string escape sequence \\uXXXX (where XXXX is the hexadecimal representation of the codepoint) -- both are ways to get the character corresponding to the given codepoint: In [19]: # \"ƒç\" as LATIN SMALL LETTER C WITH CARON, codepoint 010d print ( chr ( 0x010d )) print ( \" \\u010d \" ) ƒç ƒç In [20]: # \"ƒç\" as a sequence of LATIN SMALL LETTER C, codepoint 0063, and # COMBINING CARON, codepoint 030c print ( chr ( 0x0063 ) + chr ( 0x030c )) print ( \" \\u0063\\u030c \" ) cÃå cÃå In [21]: # of course, chr() also works with decimal numbers chr ( 269 ) Out[21]: 'ƒç' This means you have to be careful when working with languages that use accents, because to a computer, the two possible representations are of course different strings , even though to you, they're conceptually the same: In [22]: s1 = \" \\u010d \" s2 = \" \\u0063\\u030c \" # s1 and s2 look the same to the naked eye... print ( s1 , s2 ) ƒç cÃå In [23]: # ... but they're not s1 == s2 Out[23]: False Watch out, they even have different lengths ! This might come to bite you if you're trying to compute the length of a word in letters. In [24]: print ( \"s1 is\" , len ( s1 ), \"character(s) long.\" ) print ( \"s2 is\" , len ( s2 ), \"character(s) long.\" ) s1 is 1 character(s) long. s2 is 2 character(s) long. For this reason, even though we've been informally calling these Unicode entities \"characters\", it is more accurate and less confusing to use the technical term \"codepoints\". Generally, most text out there will use the first, single-codepoint approach whenever possible, and pre-packaged linguistic corpora will try to be consistent about this (unless they come from the web, which always warrants being suspicious and defensive about your material). If you're worried about inconsistencies in your data, you can perform a normalization : In [25]: from unicodedata import normalize # NFC stands for Normal Form C; this normalization applies a canonical # decomposition (into a multi-codepoint representation) followed by a # canonical composition (into a single-codepoint representation) s1 = normalize ( \"NFC\" , s1 ) s2 = normalize ( \"NFC\" , s2 ) s1 == s2 Out[25]: True Let's wrap things up by saying that Python itself uses Unicode internally, but the encoding it defaults to when opening an external file depends on the locale of the system (broadly speaking, the set of region, language and character-encoding related settings of the operating system). On most modern Linux and macOS systems, this will probably be a UTF-8 locale and Python will therefore assume UTF-8 as the encoding by default. Unfortunately, Windows is different. To be on the safe side, whenever opening files in Python, you can specify the encoding explicitly: In [26]: with open ( \"unicode.ipynb\" , encoding = \"utf-8\" ) as file : pass In fact, it's always a good idea to specify the encoding explicitly, using UTF-8 as a default if you don't know, for at least two reasons -- it makes your code more: portable -- it will work the same across different operating systems which assume different default encodings; and resistant to data corruption -- UTF-8 is more restrictive than fixed-width encodings, in the sense that not all sequences of bytes are valid UTF-8 . E.g. if one byte starts with 11, then the following one must start with 10 (see above). If it starts with anything else, it's an error. By contrast, in a fixed-width encoding, any sequence of bytes is valid. Decoding will always succeed, but if you use the wrong fixed-width encoding, the result will be garbage, which you might not notice. Therefore, it makes sense to default to UTF-8 : if it works, then there's a good chance that the file actually was encoded in UTF-8 and you've read the data in correctly; if it fails, you get an explicit error which prompts you to investigate further. Another good idea, when dealing with Unicode text from an unknown and unreliable source, is to look at the set of codepoints contained in it and eliminate or replace those that look suspicious. Here's a function to help with that: In [27]: import unicodedata as ud from collections import Counter import pandas as pd def inspect_codepoints ( string ): \"\"\"Create a frequency distribution of the codepoints in a string. \"\"\" char_frequencies = Counter ( string ) df = pd . DataFrame . from_records ( ( freq , char , f \"U+ { ord ( char ) : 04x } \" , ud . name ( char ), ud . category ( char ) ) for char , freq in char_frequencies . most_common () ) df . columns = ( \"freq\" , \"char\" , \"codepoint\" , \"name\" , \"category\" ) return df Depending on your font configuration, it may be very hard to spot the two intruders in the sentence below. The frequency table shows the string contains regular LATIN SMALL LETTER T and LATIN SMALL LETTER G , but also their specialized but visually similar variants MATHEMATICAL SANS-SERIF SMALL T and LATIN SMALL LETTER SCRIPT G . You might want to replace such codepoints before doing further text processing... In [28]: inspect_codepoints ( \"Intruders here, good ùóçhin…° I checked.\" ) Out[28]: freq char codepoint name category 0 5 e U+0065 LATIN SMALL LETTER E Ll 1 5 U+0020 SPACE Zs 2 3 r U+0072 LATIN SMALL LETTER R Ll 3 3 d U+0064 LATIN SMALL LETTER D Ll 4 3 h U+0068 LATIN SMALL LETTER H Ll 5 2 I U+0049 LATIN CAPITAL LETTER I Lu 6 2 n U+006e LATIN SMALL LETTER N Ll 7 2 o U+006f LATIN SMALL LETTER O Ll 8 2 c U+0063 LATIN SMALL LETTER C Ll 9 1 t U+0074 LATIN SMALL LETTER T Ll 10 1 u U+0075 LATIN SMALL LETTER U Ll 11 1 s U+0073 LATIN SMALL LETTER S Ll 12 1 , U+002c COMMA Po 13 1 g U+0067 LATIN SMALL LETTER G Ll 14 1 ùóç U+1d5cd MATHEMATICAL SANS-SERIF SMALL T Ll 15 1 i U+0069 LATIN SMALL LETTER I Ll 16 1 …° U+0261 LATIN SMALL LETTER SCRIPT G Ll 17 1 k U+006b LATIN SMALL LETTER K Ll 18 1 . U+002e FULL STOP Po ... because of course, for a computer, the word \"thing\" written with two different variants of \"g\" is really just two different words, which is probably not what you want: In [29]: \"thing\" == \"thin…°\" Out[29]: False Finally, to put things into perspective, here's a diagram what happens when processing text with Python (\"Unicode\" in the central box stands for Python's internal representation of Unicode, which is not UTF-8 nor UTF-16 ): (Image shamelessly hotlinked from / courtesy of the NLTK Book . Go check it out, it's an awesome intro to Python programming for linguists!) A terminological postscript: we've been using some terms a bit informally, but now that we have a practical intuition for what they mean, it's good to get the definitions straight in one's head. So, a character set is a mapping between codepoints (integers) and characters . We may for instance say that in our character set, the integer 99 corresponds to the character \"c\". On the other hand, an encoding is a mapping between a codepoint (an integer) and a physical sequence of 1's and 0's that represent it in memory . With fixed-width encodings, this mapping is generally straightforward -- the 1's and 0's directly represent the given integer, only in binary and padded with zeros to fit the desired width. With variable-width encodings, which have to explicitly encode information about how many bits are spanned by each codepoint, this straightforward correspondence breaks down. A comparison might be helpful here: as encodings, UTF-8 and UTF-16 both use the same character set -- the same integers corresponding to the same characters. But since they're different encodings , when the time comes to turn these integers into sequences of bits to store in a computer's memory, each of them generates a different one. For more on Unicode, a great read already hinted at above is Joel Spolsky's The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) . Another great piece of material is the Characters, Symbols and the Unicode Miracle video by the Computerphile channel on YouTube. To make the discussion digestible for newcomers, I sometimes slightly distorted facts about how things are \"really really\" done. And some inaccuracies may be genuine mistakes. In any case, please let me know in the comments! I'm grateful for feedback and looking to improve this material; I'll fix the mistakes and consider ditching some of the simplifications if they prove untenable :)","tags":"ling","url":"unicode.html"},{"title":"√öprava rozhran√≠ konkordanceru KonText -- vylep≈°en√° verze","text":"P≈ôed nƒõjakou dobou jsem zde vyvƒõsil skript , jeho≈æ pomoc√≠ lze lehce \"p≈ôeskl√°dat\" a upravit rozhran√≠ korpusov√©ho konkordanceru KonText : menu je um√≠stƒõn√© po stranƒõ m√≠sto naho≈ôe a permanentnƒõ rozbalen√© nad vyhledanou konkordanc√≠ je um√≠stƒõn rychl√Ω hledac√≠ box, v nƒõm≈æ lze p≈ôedchoz√≠ dotaz pohodlnƒõ upravit V√≠c o motivaci tƒõchto √∫prav se doƒçtete v p≈Øvodn√≠m ƒçl√°nku . St√°le plat√≠, ≈æe ƒåNK nem√° v pl√°nu tyto zmƒõny zaƒçlenit p≈ô√≠mo do ofici√°ln√≠ verze KonTextu, zejm√©na proto, ≈æe rychl√Ω hledac√≠ box sice v jist√Ωch situac√≠ch m≈Ø≈æe b√Ωt u≈æiteƒçn√Ω, nicm√©nƒõ oproti standardn√≠mu formul√°≈ôi Nov√Ω dotaz v√Ωraznƒõ omezuje mo≈ænosti pro zad√°n√≠ dotazu. Vylep≈°en√° verze, kter√° je k dispozici n√≠≈æe, odstra≈àuje nƒõkter√© p≈ôedchoz√≠ nedostatky skriptu: rychl√Ω hledac√≠ box nad konkordanc√≠ je vƒõt≈°√≠, ukazuje v≈ædy CQL podobu posledn√≠ho zadan√©ho dotazu 1 , a p≈ôedev≈°√≠m z≈Øst√°v√° zobrazen√Ω i bƒõhem listov√°n√≠ konkordanc√≠ (tj. nen√≠ k dispozici jen na jej√≠ prvn√≠ str√°nce). Dotaz lze nyn√≠ nav√≠c pro vƒõt≈°√≠ p≈ôehlednost rozdƒõlit do v√≠ce ≈ô√°dk≈Ø, tak≈æe opƒõtovn√© vyhled√°v√°n√≠ se novƒõ spou≈°t√≠ stiskem kombinace kl√°ves Ctrl+Enter (m√≠sto jen Enteru). V√Ωsledn√© upraven√© rozhran√≠ KonText vypad√° st√°le podobnƒõ: Postup instalace skriptu Nov√° verze skriptu je k dispozici zde: Kroky k jeho zprovoznƒõn√≠ z≈Øst√°vaj√≠ stejn√©: Nainstalovat si do sv√©ho prohl√≠≈æeƒçe plugin Tampermonkey , pokud pou≈æ√≠v√°te Chrome, nebo Greasemonkey , pokud pou≈æ√≠v√°te Firefox. (Pokud pou≈æ√≠v√°te Internet Explorer, budete muset doƒçasnƒõ p≈ôesedlat na Chrome nebo Firefox.) Testovan√Ω je skript zat√≠m jen na Chromu. Zalo≈æit v dan√©m pluginu nov√Ω skript (pro Chrome je tutorial zde , pro Firefox zde ). Smazat kostru nov√©ho skriptu a nahradit ji skriptem, kter√Ω si zkop√≠rujete v√Ω≈°e. Skript ulo≈æit. Pou≈æ√≠vat KonText jako norm√°lnƒõ -- skript u≈æ by podle adresy mƒõl s√°m poznat, ≈æe se m√° spustit. Pokud se tak nestane, nejsp√≠≈° to znamen√°, ≈æe je prohl√≠≈æeƒçov√Ω plugin (Tampermonkey nebo Greasemonkey) deaktivovan√Ω a je pot≈ôeba jej znovu aktivovat. V p≈ôedchoz√≠ verzi se po aplikaci libovoln√©ho filtru zmƒõnil obsah hledac√≠ho boxu na parametry filtrov√°n√≠. ‚Ü©","tags":"ling","url":"kontext-interface-tweak-update.html"},{"title":"√öprava rozhran√≠ konkordanceru KonText","text":"!POZOR! K dispozici je nyn√≠ vylep≈°en√° verze n√≠≈æe popsan√©ho skriptu . Hled√°n√≠ v korpusech ƒåNK ƒåesk√Ω n√°rodn√≠ korpus je sb√≠rka jazykov√Ωch korpus≈Ø ƒç√°steƒçnƒõ vytv√°≈ôen√Ωch √östavem ƒåesk√©ho n√°rodn√≠ho korpusu a ƒç√°steƒçnƒõ jin√Ωmi institucemi. V≈°echny jsou hostovan√© na jednom serveru a dostupn√© skrz r≈Øzn√° vyhled√°vac√≠ rozhran√≠ (tzv. konkordancery ), nap≈ô. NoSke , Bonito ƒçi nejnovƒõji KonText . Koncem b≈ôezna 2015 ov≈°em bude podpora star≈°√≠ch rozhran√≠ ukonƒçena a nad√°le p≈Øjde k dat≈Øm v ƒåNK p≈ôistupovat prim√°rnƒõ pouze p≈ôes KonText. (Pokud v√°m odstavec v√Ω≈°e ned√°v√° p≈ô√≠li≈° smysl, s jazykov√Ωmi korpusy se setk√°v√°te poprv√©, ale chcete se dozvƒõdƒõt v√≠c, radƒõji si m√≠sto tohoto postu p≈ôeƒçtƒõte, k ƒçemu je takov√Ω korpus dobr√Ω , a zkuste si v nƒõm nƒõco pro zaj√≠mavost vyhledat . Pokud se v√°m p≈ôi vzpom√≠nce na Bonito ƒçi NoSke naopak zaskvƒõla slza v oku, ƒçtƒõte d√°l!) KonText vs. Bonito / NoSke KonText m√° oproti star≈°√≠m rozhran√≠m ≈ôadu v√Ωhod -- bohat≈°√≠ funkcionalitu, mnoh√© pom≈Øcky, kter√© v√°m pomohou se zad√°n√≠m slo≈æitƒõj≈°√≠ch dotaz≈Ø (sestaven√≠ morfologick√©ho tagu ƒçi podm√≠nky within ), a v neposledn√≠ ≈ôadƒõ mnohem l√©pe vypad√°, co≈æ kup≈ô√≠kladu mnƒõ p≈ôi pr√°ci p≈Øsob√≠ jako balz√°m na du≈°i. Nicm√©nƒõ dlouholet√≠ u≈æivatel√© ƒåNK byli jednodu≈°e zvykl√≠ na nƒõkter√© aspekty Bonita a NoSke, kter√© jim teƒè v KonTextu chyb√≠. Onehdy p≈ôi rozhovoru s jedn√≠m z nich vyplavaly na povrch jako hodnƒõ d≈Øle≈æit√© dvƒõ st√≠≈ænosti: Vrchn√≠ menu v KonTextu je z√°ke≈ôn√©, schov√°v√° se, ƒçlovƒõk nem√° p≈ôehled nad dostupn√Ωmi funkcemi. Oproti tomu NoSke m√° menu po stranƒõ a je permanentnƒõ rozvinut√©, tak≈æe u≈æivatel m√° v≈°echny mo≈ænosti interakce s konkordanc√≠ soustavnƒõ jako na dlani. Po zad√°n√≠ dotazu ƒçlovƒõk ƒçasto na z√°kladƒõ konkordance zjist√≠, ≈æe jej pot≈ôebuje je≈°tƒõ trochu upravit / zjemnit. KonText si sice p≈ôedchoz√≠ dotazy pamatuje, je ale pot≈ôeba se k nim doklikat; ≈°ikovnƒõj≈°√≠ by bylo, kdyby tato mo≈ænost byla dostupn√° p≈ô√≠mo ze str√°nky konkordance v podobƒõ nƒõjak√©ho zjednodu≈°en√©ho hledac√≠ho boxu. (NoSke tohle vlastnƒõ taky neum√≠, v Bonitu je to jednodu≈°≈°√≠.) V obou p≈ô√≠padech jde o smyslupln√© po≈æadavky, jen≈æe KonText je pomƒõrnƒõ velk√° a slo≈æit√° aplikace, tak≈æe i pokud se ƒåNK rozhodne do n√≠ tyto podnƒõty v nƒõjak√© podobƒõ zapracovat (nap≈ô. jako mo≈ænost p≈ôepnut√≠ zobrazen√≠ menu), bude nƒõjakou chv√≠li trvat, ne≈æ se implementace navrhne, vytvo≈ô√≠, ≈ô√°dnƒõ otestuje a koneƒçnƒõ dostane k u≈æivatel≈Øm. Nicm√©nƒõ aby bylo mo≈æn√© alespo≈à vyzkou≈°et, jak by zm√≠nƒõn√© zmƒõny vypadaly v praxi, dal jsem dohromady kr√°tk√Ω skript, kter√Ω ji≈æ v prohl√≠≈æeƒçi nahran√Ω KonText trochu \"p≈ôestav√≠\" a uprav√≠. V√Ωsledek vypad√° n√°sledovnƒõ: Rovnou p≈ôedes√≠l√°m: ten skript je nevzhledn√Ω bastl p≈ôilepen√Ω na KonText zvnƒõj≈°ku; proto taky bylo mo≈æn√© jej d√°t dohromady pomƒõrnƒõ rychle, proto≈æe si neklade n√°rok na spolehlivost, kter√° se vy≈æaduje od ofici√°ln√≠ verze KonTextu. Je to sp√≠≈° prototyp, jeho≈æ √∫ƒçelem je otestovat v√Ω≈°e popsan√© zmƒõny v praxi a z√≠skat p≈ôedstavu o tom, zda a do jak√© m√≠ry jsou p≈ô√≠nosn√©. (Vlastn√≠ zku≈°enost: po chv√≠li pou≈æ√≠v√°n√≠ mi p≈ôijde p≈ô√≠datn√Ω hledac√≠ box nad konkordanc√≠ hodnƒõ ≈°ikovn√Ω a u≈æiteƒçn√Ω.) Teƒè k j√°dru pudla: pokud m√°te z√°jem, m≈Ø≈æete si KonText takto k obrazu sv√©mu (resp. k obr√°zku o odstavec v√Ω≈°) upravit tak√© a vyzkou≈°et, jak v√°m takov√© nastaven√≠ vyhovuje. Kdy≈æ se v√°m jedna z √∫prav bude l√≠bit (nebo v√°s u toho napadne jin√°, kterou by si KonText zaslou≈æil), m≈Ø≈æete pak zadat po≈æadavek na nov√Ω feature . N√°vod, jak si KonText upravit, n√°sleduje n√≠≈æe. Postup instalace skriptu Skript samotn√Ω je k dispozici zde: K jeho zprovoznƒõn√≠ jsou pot≈ôeba n√°sleduj√≠c√≠ kroky: Nainstalovat si do sv√©ho prohl√≠≈æeƒçe plugin Tampermonkey , pokud pou≈æ√≠v√°te Chrome, nebo Greasemonkey , pokud pou≈æ√≠v√°te Firefox. (Pokud pou≈æ√≠v√°te Internet Explorer, budete muset doƒçasnƒõ p≈ôesedlat na Chrome nebo Firefox.) Testovan√Ω je skript zat√≠m jen na Chromu. Zalo≈æit v dan√©m pluginu nov√Ω skript (pro Chrome je tutorial zde , pro Firefox zde ). Smazat kostru nov√©ho skriptu a nahradit ji skriptem, kter√Ω si zkop√≠rujete v√Ω≈°e. Skript ulo≈æit. Pou≈æ√≠vat KonText jako norm√°lnƒõ -- skript u≈æ by podle adresy mƒõl s√°m poznat, ≈æe se m√° spustit. Pokud se tak nestane, nejsp√≠≈° to znamen√°, ≈æe je prohl√≠≈æeƒçov√Ω plugin (Tampermonkey nebo Greasemonkey) deaktivovan√Ω a je pot≈ôeba jej znovu aktivovat. Omezen√≠ Skript m√° pravdƒõpodobnƒõ hromadu drobn√Ωch much, na kter√© se mi zat√≠m nepoda≈ôilo p≈ôij√≠t -- budu se je sna≈æit pr≈Øbƒõ≈ænƒõ opravovat, kdy≈æ na nƒõ padnu, nebo kdy≈æ mi o nich d√°te vƒõdƒõt . Krom toho m√° i nƒõkter√© mouchy, o nich≈æ u≈æ v√≠m, ale bohu≈æel toho s nimi nejde moc dƒõlat. Asi nejn√°padnƒõj≈°√≠ je, ≈æe p≈ôidan√Ω hledac√≠ box funguje jen na tƒõch str√°nk√°ch, kde je p≈Øvodn√≠ dotaz i souƒç√°st√≠ adresy URL (co≈æ nejsou v≈°echny -- t≈ôeba kdy≈æ zaƒçnete listovat konkordanc√≠ na druhou str√°nku a d√°l, dotaz je z adresy vyjmut a pomocn√Ω hledac√≠ box tedy zmiz√≠ ). Ale vzhledem k tomu, ≈æe jeho hlavn√≠ √∫ƒçel m√° b√Ωt mo≈ænost lehce upravit dotaz po prvn√≠m rychl√©m nahl√©dnut√≠ do konkordance, snad to nebude takov√Ω probl√©m. Pokud nƒõkdy bude podobn√Ω box ≈ô√°dnƒõ p≈ôid√°n p≈ô√≠mo do KonTextu, takov√Ωmi nedostatky samoz≈ôejmƒõ trpƒõt nebude. A je≈°tƒõ k pou≈æ√≠v√°n√≠ p≈ôidan√©ho hledac√≠ho boxu : Typ dotazu, kter√Ω je do nƒõj pot≈ôeba zadat, je stejn√Ω jako ten, kter√Ω jste p≈ôi prvotn√≠m vyhled√°n√≠ konkordance zadali na str√°nce Nov√Ω dotaz . Pokud tento prvotn√≠ dotaz byl Z√°kladn√≠ dotaz, m≈Ø≈æete pomoc√≠ rychl√©ho boxu zadat jin√Ω Z√°kladn√≠ dotaz; pokud to byl CQL dotaz, m≈Ø≈æete ho upravit zas jen na dal≈°√≠ CQL dotaz. D≈Øvodem je, ≈æe smyslem tohoto pomocn√©ho boxu nen√≠ nahradit plnohodnotn√Ω formul√°≈ô pro zad√°n√≠ dotazu, jen poskytnout rychlou mo≈ænost, jak ji≈æ zadan√Ω dotaz upravit . Pomocn√Ω hledac√≠ box se objev√≠ i pot√©, co na konkordanci provedete filtrov√°n√≠. V takov√© situaci se d√° pou≈æ√≠t k tomu, abyste pozmƒõnili zad√°n√≠ aktu√°ln√≠ho filtru , tj. filtrov√°n√≠ se provede znovu na p≈Øvodn√≠ konkordanci, ne na t√©to ji≈æ filtrovan√©. Pokud chcete opakovanƒõ filtrovat tu samou konkordanci a postupnƒõ podle dan√Ωch krit√©ri√≠ vy≈ôazovat / p≈ôid√°vat ≈ô√°dky, je pot≈ôeba m√≠sto hledac√≠ho boxu opakovanƒõ pou≈æ√≠t menu Filtr . Komu si stƒõ≈æovat, kdy≈æ to nebude fungovat Skript je volnƒõ ≈°i≈ôiteln√Ω pod licenc√≠ GNU GPL v3 , tak≈æe se na nƒõj nev√°≈æe ≈æ√°dn√° z√°ruka. Kdy≈æ se v√°m ale nebude da≈ôit jej zprovoznit, r√°d se pokus√≠m pomoct! Staƒç√≠ se ozvat na adresu uvedenou zde .","tags":"ling","url":"kontext-interface-tweak.html"},{"title":"Beyond semantic versioning? (cross-post)","text":"Background Ever since I first read about semantic versioning , I've thought of it as a neat idea. But only recently did it occur to me that what I liked about the idea was its goal, much less its execution (more on that below). What made it obvious was this lengthy discussion about breaking changes introduced in v1.7 of underscore.js without an accompanying major version bump. Even though I still think sticking to semver is the right thing to do if your community of users expects you to (even if you don't personally like the system), I am convinced there are fundamentally better ways of dealing with the problem of safely and consistently updating dependencies. It made me want to add my two cents to the discussion , as someone who's more of a dabbler in programming and not really part of the community, so feel free to ignore me :) I attach my commentary below for reference (it's virtually the same text as in the link above). tl;dr semver is trying to do the right thing, but doing it wrong -- instead of implicitly encoding severity of change information in version numbers , explicit keywords like :patch, :potentially-breaking or :major-api-change would make much more sense. More verbosely I've always found the goals of semver worthy, but this thread has made me realize that while its aims are commendable, its methods are kind of broken: semver tries to take an existing semiotic system (= version numbers), which has developed informally and is therefore a loose convention rather than an exact spec, and reinterpret it in terms of an exact spec (or impose that spec on it). trouble is, the prior informal meaning won't go away so easily (why should it?), especially for projects that have been around longer than semver. the problem then is, since the two systems (the informal one and semver) look the same in terms of their symbolic representation, it's hard to guess which one you're dealing with by just eyeballing the version number of a library (or project in general). it's like if someone decided that \"f*ck\" should mean \"orchid\" from now on, because it's nicer -- on hearing the word, you'd never know if it's being used as the original profanity, or in its new meaning. homonymy is a pain to deal with when it's accidental (cf. NLP), so why introduce it on purpose? the job that semver set out to do should be fulfilled by a new formal means which is instantly recognizable, not by hijacking an existing one and overlaying additional interpretation on it and thus making it ambiguous . even if version numbers hadn't existed before semver, they're terribly inadequate for the purpose of conveying information about the severity of changes introduced by an update (though I understand their appeal to mathematically-minded people). they're inadequate because they're implicit -- it's a bit like if someone decided they don't need hash maps because they can make do with arrays by remembering the order in which they're adding in the key-val pairs. if I remember the order, then I know which key the given index implicitly refers to, and the result is as good as a hash map, isn't it? except it isn't. keys are useful because they have explicit semantics , making it instantly clear what kind of value you're retrieving. in the same way, encoding the information about the severity of changes into version numbers makes it implicit (in addition to being ambiguous, as stated previously). why not use explicit keyword tags along with the version number (which can be romantic, semantic -- whichever floats the dev team's boat and best reflects the progress of the project) to give a heads up as to the nature of the update? e.g. :patch, :potentially-breaking, :major-api-change etc. granted, even language is a code which needs to be learned, like semver (gross oversimplification here, but let's not get into the details of language acquisition), but since it's widely established and conventionalized for conveying the kinds of meanings semver is trying to convey, why not just use it when it's available ? why use a system (version numbers) which is less well-suited to the purpose and ambiguous to boot? (on the other hand, numbers are eminently well-suited for keeping track of which version is newer than which and how much so -- the original purpose of version numbering -- because they are designed to have orderings defined on them. by contrast, words would do a terrible job at this. if you care to indicate the evolution of your codebase, you might introduce your own disciplined romantic or sentimental versioning scheme, which ironically is a more meaningful and ergo semantic way of doing versioning than semver, because it sticks to the conventional semantics of numbers (the closer the numbers, the more similar the versions). if you don't care about this, which is perfectly fine, you might as well use dates for version numbers.) keyword tags have the advantage that they're instantly human-readable by anyone who has a basic command of English. if there is sufficient will in the community, a useful subset can be frozen in a binding spec, so that they are machine-readable as well. I'm not sure whether these keywords should be an appendix to the version number (like v2.3.4-:potentially-breaking), or whether the information they provide should be more extensive and included in a formalized preamble to the changelog (finally forcing people to at least take a glance at it ;) ). using the latter approach, the information provided could be (optionally) even more targeted, e.g. detailing explicitly which parts of the API are affected in a non-backwards compatible manner by the update. anyways, just a few ideas :) I am not primarily a coder, so there may be obvious drawbacks to this scheme that I can't see or which have already been discussed by the community on multiple occasions which have escaped my attention. in which case, please bear with me and excuse my lack of sophistication.","tags":"floss","url":"beyond-semver.html"},{"title":"Filling (hardwrapping) paragraphs in Airmail with `par`","text":"tl;dr Jump directly to the proposed solution . Tested on OS X 10.9 (Mavericks). Back story Airmail is a great application -- being very happy with Gmail's in-browser UI, it's honestly the first e-mail desktop client that I ever felt even remotely tempted to use. It has: a sleek, functional design almost flawless integration with Gmail (except for categories -- but there's a not-too-hackish way to deal with those) a Markdown compose mode (yay!) -- and tons of other good stuff. Especially that last feature almost got me sold -- you see, I like my e-mail hardwrapped (what Emacs calls \"filling paragraphs\"), because most of the time, I view it on monitors that are too wide for soft line wrapping to achieve a comfortable text width. (By the way, Airmail's layout deals with this issue very elegantly, but I know I won't be using only Airmail. Plus there are the obvious netiquette issues -- lines \"should be\" wrapped at 72 characters etc.) In Gmail, I therefore use plain-text compose, which is fine for the purposes described above, but frustrating whenever you want to apply formatting (obviously, you can't -- it's plain text). I tried using the usual replacements for formatting like stars & co., and I don't know about your grandma, but mine certainly doesn't take *...* to mean emphasis. I thought the Markdown compose mode in Airmail would solve my problems -- I could apply formatting if and when I wanted (using the frankly more streamlined process of typing it in rather than fumbling around for the right button in the GUI) and fill my paragraphs, because I somehow automatically assumed there'd by a hard-wrap feature like in any decent editor (read: emacs or vi). Markdown is plain text after all, isn't it? Long story short, as of yet, there isn't . There isn't even one for the plain-text compose mode, as far as I'm aware. So I added my two cents to this feature request thread and went back to the Gmail in-browser UI. Solution But then I realized (it took me a while, I'm still very much an OS X newbie): in OS X, you can define custom actions with shortcuts 1 for any application using Automator Services these actions can be easily set to receive text selected in the application as input these actions can also involve shell scripts there already is a great (command line) program for filling paragraphs -- it's called par , and as much as I admire what Airmail's developers have achieved, it's unlikely that they'd come up with a more sophisticated hard-wrapping algorithm than par 's simply as a side project for Airmail (see the EXAMPLES section in man par ) With that in mind, you can have hard-wrapping in Markdown or plain-text Airmail compose at your fingertips in no time flat. If you don't have homebrew , start by installing that (or any other ports manager that will allow you to install par ; I'll assume homebrew below) by pasting ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" at a Terminal prompt. Then: install par with brew install par at a Terminal prompt open Automator (e.g. by typing \"Automator\" into Spotlight) and create a new Service select the applications for which you want the service to be active (for me, that's just Airmail) and tick the \"Output replaces selected text\" box drag the \"Run Shell Script\" action onto the workflow canvas, and as the shell script, paste in PARINIT = \"rTbgqR B=.,?_A_a Q=_s>|\" /usr/local/bin/par 79 the $PARINIT environment variable contains the default recommended settings for par (if you want to customize its behavior, you can -- good luck wrapping your head around par 's manpage, though) you should set the full path to the par executable, the shell spawned by the Service might not inherit your $PATH -- for par installed via homebrew , it's /usr/local/bin/par the parameter at the end is the max number of characters per line -- mailing list etiquette stipulates 72, I personally prefer the pythonesque 79, but it's your choice At this point, your service should look something like in the screenshot below: Save it, open Keyboard preferences (type \"Keyboard\" into Spotlight), navigate to Shortcuts ‚Üí Services ‚Üí Text and set a keyboard shortcut for your newly created Service, e.g. Cmd+Opt+P. Next time you compose an e-mail in Airmail, just select the entire text when you're done (Cmd+A), press Cmd+Opt+P, and voil√†! Your lines have been hardwrapped, your paragraphs filled :) (Same thing, I know.) If the shortcut doesn't appear to work 1 , try fiddling around with it, resetting it (maybe the one you've chosen conflicts with a pre-existing one?), restarting Airmail, logging out and back in, rebooting... The custom shortcut part is unfortunately the least reliable aspect of this whole setup. Automator is a great idea, I was pleasantly surprised by it when I started using OS X a few days back, but it could seriously use some bug-squashing. If you fail miserably at getting the shortcut to work, you can still access your fill paragraph service via the menu (select the text you want to hard-wrap, then navigate to Airmail ‚Üí Services ‚Üí <name of your fill paragraph service>). Clicking around in a GUI is tedious (though hey -- it's the Apple way after all, isn't it?), but it shouldn't be too much of a bother since you need to do it only once per e-mail. Bottom line : I am now officially completely sold on Airmail (even bought the released version instead of using the free beta) and look forward to the joy of using it! EDIT: In order to have the least trouble possible getting the shell script up and running as a Service , two rules of thumb: Leave it completely up to OS X where it stores the Service (.workflow) file. This will probably be in ~/Library/Services , and I learnt the hard way not to tinker with it -- if Services is a symlink instead of a real directory, the OS won't discover new Service files (though old ones will still be accessible). If the Service doesn't show up in the keyboard shortcuts menu after creation, try refreshing the service list with /System/Library/CoreServices/pbs -update . Those shortcuts are in fact quite buggy, especially those that you want to be global (not specific to a concrete app) -- at least on Mavericks (OS X 10.9). They tend to get disabled on a whim, especially if you tinker with them, and are a pain to get working again (login, logout, reboot -- anything goes). If anyone knows why, please let me know! ‚Ü© ‚Ü©","tags":"macOS","url":"fill-par-in-airmail.html"}]}