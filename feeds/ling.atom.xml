<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Little Umbrellas - ling</title><link href="https://dlukes.github.io/" rel="alternate"></link><link href="https://dlukes.github.io/feeds/ling.atom.xml" rel="self"></link><id>https://dlukes.github.io/</id><updated>2017-10-28T00:00:00+02:00</updated><entry><title>Text munching in R?</title><link href="https://dlukes.github.io/text-munching-in-r.html" rel="alternate"></link><published>2017-10-28T00:00:00+02:00</published><updated>2017-10-28T00:00:00+02:00</updated><author><name>dlukes</name></author><id>tag:dlukes.github.io,2017-10-28:/text-munching-in-r.html</id><summary type="html">&lt;p&gt;Is R suited for processing large quantities of text data?&lt;/p&gt;</summary><content type="html">&lt;p&gt;R has been &lt;a href="https://stackoverflow.blog/2017/10/10/impressive-growth-r/"&gt;gaining traction&lt;/a&gt; as a language for data analysis. My
feelings about the whole ecosystem are mixed -- it has some &lt;a href="https://www.tidyverse.org/"&gt;incredibly
well-designed libraries&lt;/a&gt; and a &lt;a href="https://www.rstudio.com/"&gt;top-of-the-game IDE&lt;/a&gt;, but
the core language makes me cringe (it feels like "Perl and Lisp: The Worse
Parts"). Be that as it may, it has undeniably become the go-to programming
language for many people for whom programming is not their main breadwinner,
many linguists among them. If you're one of these people and wondering whether
it's worth undergoing the cognitive burden of learning another language and
having to context-switch between them, read on!&lt;/p&gt;
&lt;p&gt;The main problem with R and large data is of course that R is fast as long as
you can load everything into memory at once and use vectorized operations. The
whole point of this post is that with the current size of a typical corpus, you
often can't do that. You'll have to process the corpus line by line, which
means using a for-loop, and these are notoriously slow in R. I'm not pretending
this is some new discovery (it's not), I'm just trying to quantify how
problematic this slowness is for processing large quantities of text
(prohibitive, in my opinion), so that you don't have to figure it out for
yourself and can get started &lt;a href="http://www.nltk.org/book/"&gt;learning Python 3&lt;/a&gt; right away instead
;)&lt;/p&gt;
&lt;p&gt;(Another big problem is that R doesn't have an efficient and versatile hash
table data structure.)&lt;/p&gt;
&lt;h1&gt;tl;dr&lt;/h1&gt;
&lt;p&gt;If you're planning to process corpora of hundreds of millions of tokens or more
-- spoiler alert, you probably shouldn't do it in R.&lt;/p&gt;
&lt;h1&gt;Task&lt;/h1&gt;
&lt;p&gt;OK, I've specifically complained about R being bad at for-loops and hashes.
Let's devise a task that'll show exactly how bad. At the same time, I don't
mean to come up with anything particularly convoluted or far-fetched. So here
goes: we'll be trying to build a per-part-of-speech frequency distribution of
lemmas (dictionary headword forms) from a 120M-token corpus. If you've ever
done any linguistic data analysis, I hope you'll agree it's a pretty basic and
common task.&lt;/p&gt;
&lt;p&gt;The corpus data consists of lines with tab-separated word form, lemma and tag
fields, plus some additional lines with metadata which do not contain tabs.
We'll be skipping those. Here's a glimpse of the corpus format:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;&amp;lt;!--&lt;/span&gt;
&lt;span class="c"&gt;WORD     LEMMA    TAG --&amp;gt;&lt;/span&gt;
Hladina  hladina  NNFS1-----A-----
jezera   jezero   NNNS2-----A-----
,        ,        Z:--------------
mrtvá    mrtvý    AAFS1----1A-----
a        a        J^--------------
černá    černý    AAFS1----1A-----
,        ,        Z:--------------
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The first character of the tag indicates the part of speech. For each part of
speech, we want to create a separate frequency distribution, i.e. we want to be
able to say, for instance, the most frequent noun is X, followed by Y, whereas
the most frequent adjective is Z etc. This should nicely exercise all of R's
weak spots. Let's get to it!&lt;/p&gt;
&lt;h1&gt;R: 1 day (!) / 15 hours (see EDIT below, but still !)&lt;/h1&gt;
&lt;p&gt;After quite some exploration of various alternatives for the individual
subtasks, this is the program that I came up with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;stringi&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;hash&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# read input from STDIN&lt;/span&gt;
con &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;file&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;stdin&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; open&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;rt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

pos_sets &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; hash&lt;span class="p"&gt;()&lt;/span&gt;
start &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;Sys.time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="kr"&gt;while&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  line &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;readLines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;con&lt;span class="p"&gt;,&lt;/span&gt; n&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="kr"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;line&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="kr"&gt;break&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="c1"&gt;# lines with tokens (as opposed to metadata) contain tabs&lt;/span&gt;
  &lt;span class="kr"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;stri_detect_fixed&lt;span class="p"&gt;(&lt;/span&gt;line&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;\t&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="c1"&gt;# individual token attributes are tab-separated&lt;/span&gt;
    attrs &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; stri_split_fixed&lt;span class="p"&gt;(&lt;/span&gt;line&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;\t&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# for each token, we&amp;#39;re interested in the lemma (headword)...&lt;/span&gt;
    lemma &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; attrs&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]][&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="c1"&gt;# ... and the part-of-speech, which is the first character of the tag&lt;/span&gt;
    tag &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; attrs&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]][&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    pos &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; stri_split_boundaries&lt;span class="p"&gt;(&lt;/span&gt;tag&lt;span class="p"&gt;,&lt;/span&gt; n&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; type&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;character&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)[[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]][&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="c1"&gt;# build a per-part-of-speech frequency distribution as a nested hash:&lt;/span&gt;
    &lt;span class="c1"&gt;# {&lt;/span&gt;
    &lt;span class="c1"&gt;#   &amp;quot;noun&amp;quot;: {&lt;/span&gt;
    &lt;span class="c1"&gt;#     &amp;quot;cat&amp;quot;: 5,&lt;/span&gt;
    &lt;span class="c1"&gt;#     &amp;quot;dog&amp;quot;: 3,&lt;/span&gt;
    &lt;span class="c1"&gt;#     ...&lt;/span&gt;
    &lt;span class="c1"&gt;#   },&lt;/span&gt;
    &lt;span class="c1"&gt;#   &amp;quot;verb&amp;quot;: {&lt;/span&gt;
    &lt;span class="c1"&gt;#     &amp;quot;look&amp;quot;: 2,&lt;/span&gt;
    &lt;span class="c1"&gt;#     ...&lt;/span&gt;
    &lt;span class="c1"&gt;#   },&lt;/span&gt;
    &lt;span class="c1"&gt;#   ...&lt;/span&gt;
    &lt;span class="c1"&gt;# }&lt;/span&gt;
    &lt;span class="kr"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;is.null&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;pos_sets&lt;span class="p"&gt;[[&lt;/span&gt;pos&lt;span class="p"&gt;]]))&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      pos_sets&lt;span class="p"&gt;[[&lt;/span&gt;pos&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; hash&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="kr"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;is.null&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;pos_sets&lt;span class="p"&gt;[[&lt;/span&gt;pos&lt;span class="p"&gt;]][[&lt;/span&gt;lemma&lt;span class="p"&gt;]]))&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      pos_sets&lt;span class="p"&gt;[[&lt;/span&gt;pos&lt;span class="p"&gt;]][[&lt;/span&gt;lemma&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="kr"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      pos_sets&lt;span class="p"&gt;[[&lt;/span&gt;pos&lt;span class="p"&gt;]][[&lt;/span&gt;lemma&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; pos_sets&lt;span class="p"&gt;[[&lt;/span&gt;pos&lt;span class="p"&gt;]][[&lt;/span&gt;lemma&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;# report running time&lt;/span&gt;
diff &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;Sys.time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; start
&lt;span class="kp"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;sprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Done in %g %s.\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;diff&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;units&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;diff&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; file&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kp"&gt;stderr&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Given the input corpus mentioned above, this code takes &lt;strong&gt;1.33&lt;/strong&gt; days to run.
Compared to other languages one might conceivably use (see below), this is just
ridiculous.&lt;/p&gt;
&lt;p&gt;Now, I'm certainly not an expert in R, so there may be better ways of doing
some of this. But I doubt such improvements, if any, would be of any practical
relevance, because even reducing the running time to &lt;em&gt;a tenth&lt;/em&gt; of the original
duration wouldn't be enough. And if there &lt;em&gt;is&lt;/em&gt; a way to go even further, say to
a hundredth, which would begin to make R competitive, then I would argue that a
language which lets you shoot yourself so spectacularly in the foot
performance-wise if you're not hip to some clever tricks should just be avoided
for tasks where said performance matters.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; Replacing &lt;code&gt;stri_split_boundaries(tag, n=2, type="character")[[1]][1]&lt;/code&gt;
above with &lt;code&gt;stri_sub(tag, from=1, to=1)&lt;/code&gt;, you can cut the running time down to
15 hours. That's still way too much in comparison with the competitors, and
just reinforces one of the points made below: there's often no default and
efficient way of doing some basic operations (like string manipulation) in R.&lt;/p&gt;
&lt;p&gt;This is in great part due to R's emphasis on vectorization, which leads to a
proliferation of subtly different functions designed for doing subtly different
kinds of vectorized passes over data. Good luck trying to remember them all.
And if you pick the wrong one (cf. &lt;code&gt;stri_split_boundaries()&lt;/code&gt; vs. &lt;code&gt;stri_sub()&lt;/code&gt;)
-- because there are just too many similar ways of achieving the same result
and too much documentation to read before you even begin to see what you should
use -- you get penalized heavily. This is very programmer-unfriendly design.&lt;/p&gt;
&lt;p&gt;Contrast this with the &lt;a href="https://www.python.org/dev/peps/pep-0020/#the-zen-of-python"&gt;Zen of Python&lt;/a&gt;: "There should be one -- and
preferably only one -- obvious way to do it."&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;With these general considerations out of the way, let's look at some details of
how to implement this task in R. In many cases, it's unclear how you should
even approach the problem in R, due to &lt;strong&gt;missing or confusing built-in
functionality&lt;/strong&gt;. As a result, in addition to having a lousy running time on
this task, R also puts a strain on the programmer's time.&lt;/p&gt;
&lt;h2&gt;Reading the data&lt;/h2&gt;
&lt;p&gt;This sounds so basic it should be obvious, right? Not so fast.&lt;/p&gt;
&lt;p&gt;First of all, the &lt;code&gt;file("path/to/file")&lt;/code&gt; function creates a file connection,
which is however not open unless you also specify a mode in the &lt;code&gt;open=&lt;/code&gt;
argument, or alternatively, unless you call the &lt;code&gt;open()&lt;/code&gt; function on the
connection. Why you would want to create a connection that's not open is beyond
me, but R adds insult to injury by allowing &lt;code&gt;readLines()&lt;/code&gt; to work on a closed
connection: it just opens the connection before doing the reading and closes it
afterwards. This means that repeated calls to &lt;code&gt;readLines(unopened_connection,
n=1)&lt;/code&gt; will &lt;strong&gt;repeatedly read the first line of the file&lt;/strong&gt;, which is most likely
not what you want. This is API design level PHP.&lt;/p&gt;
&lt;p&gt;Second, the corpus is gzip compressed, so you'll need to uncompress it. There
are basically two options:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;have an external program (&lt;code&gt;zcat&lt;/code&gt;) do the decompression and pipe the data
   into R via &lt;code&gt;STDIN&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;handle the decompression within R itself&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As a general rule (for any language), it will always be faster to handle the
decompression in a different process on a multi-core system, because the tasks
can proceed in parallel.&lt;sup id="fnref-1"&gt;&lt;a class="footnote-ref" href="#fn-1"&gt;1&lt;/a&gt;&lt;/sup&gt; On the other hand, it's more portable not to
depend on external programs, and R does have a built-in function to open a
connection to a gzipped file, namely &lt;code&gt;gzfile()&lt;/code&gt;. Based on tests on shorter
inputs, it's about 50% slower than external decompression, which is a somewhat
worse performance deterioration than e.g. Python 3 (40% based on the full
input). In light of the already dire running time, it's something we can't
really afford.&lt;/p&gt;
&lt;p&gt;Third, having to do the line-by-line reading in a &lt;code&gt;while (TRUE)&lt;/code&gt; loop, using a
function called &lt;code&gt;readLines()&lt;/code&gt; (note the plural) with an argument of &lt;code&gt;1&lt;/code&gt;,
checking the length of the resulting character vector in order to determine the
end of the input -- that's just gross.&lt;/p&gt;
&lt;h2&gt;String manipulation&lt;/h2&gt;
&lt;p&gt;R has built-in functions for string matching (&lt;code&gt;grepl()&lt;/code&gt; et al.), not so much
for string splitting. This is the point where I got suspicious of the
performance of everything and started testing alternatives. I finally ended up
using the &lt;a href="http://www.gagolewski.com/software/stringi/"&gt;stringi&lt;/a&gt; package, which is fast and has a fairly consistent
API. &lt;a href="https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html"&gt;stringr&lt;/a&gt; is a set of higher-level wrappers around it, which have
however proven somewhat slower than the built-ins in my highly informal
testing.&lt;/p&gt;
&lt;h2&gt;O hash map, where art thou?&lt;/h2&gt;
&lt;p&gt;Building a per-part-of-speech frequency distribution of headwords requires an
appropriate data structure. As indicated in the comments in the R source, we
want to build a nested collection that looks something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{
  &amp;quot;noun&amp;quot;: {
    &amp;quot;cat&amp;quot;: 5,
    &amp;quot;dog&amp;quot;: 3,
    ...
  },
  &amp;quot;verb&amp;quot;: {
    &amp;quot;look&amp;quot;: 2,
    ...
  },
  ...
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The requirements on the data structure we need are the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;it's a collection&lt;/li&gt;
&lt;li&gt;strings can be used as keys&lt;/li&gt;
&lt;li&gt;it can be arbitrarily nested&lt;/li&gt;
&lt;li&gt;key lookup is fast, i.e. constant time&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In other words, we need a hash (or a dict, in Python terminology). R doesn't
&lt;em&gt;have&lt;/em&gt; a hash (I'll qualify this statement in a bit).&lt;/p&gt;
&lt;p&gt;The workhorse data structure in R that satisfies points 1--3 is a list.
Unfortunately, it has &lt;a href="https://stackoverflow.com/questions/41353298/what-is-the-time-complexity-of-name-look-up-in-an-r-list"&gt;linear access time&lt;/a&gt;. That's not going to work.&lt;/p&gt;
&lt;p&gt;R also has environments, which it uses to store and access variables. Under the
hood, environments are implemented as hashes, but using them as such is a
massive pain, because their API isn't meant for it. Fortunately, there's &lt;a href="https://cran.r-project.org/web/packages/hash/index.html"&gt;a
wrapper package&lt;/a&gt; which makes it more convenient. Unfortunately,
environments weren't optimized with this use case in mind. They were designed
to hold key--value (variable name--variable value) pairs explicitly defined by
people as part of their programs, not millions of items extracted from data. As
a result, they &lt;a href="http://appsilondatascience.com/blog/rstats/2017/03/02/r-fast-lookup.html"&gt;slow down dramatically&lt;/a&gt; once the number of items
grows large.&lt;/p&gt;
&lt;p&gt;(The article in the previous link provides a survey of the state of the art of
fast key lookup in R. The state of the art is... dismal. Your only option is
basically indexing a data table, which is fine for a finalized data set, but
useless when &lt;em&gt;building the data set&lt;/em&gt; -- you can't afford to reindex after each
new data point.)&lt;/p&gt;
&lt;p&gt;There's also the &lt;a href="https://cran.r-project.org/web/packages/hashmap/index.html"&gt;hashmap library&lt;/a&gt;, which is a wrapper around C++
Boost hashes. However, it doesn't do nesting, so it's of no use to us, and of
very limited usefulness in general.&lt;/p&gt;
&lt;p&gt;Conclusion: technically, we have to concede that R has hashes, but &lt;strong&gt;for all
practical intents and purposes, it doesn't&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;There's one last twist, though. Funnily enough, in our use case, it turns out
it doesn't really matter anyway. Indeed, it seems the performance of for-loops
in R is &lt;em&gt;so egregiously bad&lt;/em&gt; that it dwarfs even the inefficiencies accrued by
the linear lookup time of lists: if you reimplement the script with lists, it
takes just a little longer than the version with hashes, about 1.36 days.&lt;/p&gt;
&lt;p&gt;(Or maybe it's just that the performance of environment-based hashes becomes so
bad when they grow large as to be comparable with that of lists? Who knows, and
frankly, I don't care enough to want to find out. If it's the for-loops though,
then adding efficient hashes to R won't really solve anything.)&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; With &lt;code&gt;stri_sub()&lt;/code&gt; substituted for &lt;code&gt;stri_split_boundaries()&lt;/code&gt; as
detailed above, the code using lists runs in about 1.28 days, which is a much
smaller improvement than in the case of the code using hashes (1.33 days → 15
hours).&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;If you like R and your reaction to this is, "That's not fair! R was never meant
to do any of this, that's why everything feels so backhanded." -- then good,
that's basically the gist of this post: &lt;strong&gt;don't use R for something it wasn't
meant to do&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;What are the alternatives, then?&lt;/p&gt;
&lt;h1&gt;Task&lt;/h1&gt;
&lt;p&gt;The following details an informal test comparing the speed of R, Python 3, Rust
and Perl at processing a large corpus file (~120M tokens, 1.5GB gzipped) and
creating a frequency distribution of headwords per part-of-speech. The idea is
to see whether R is a viable alternative in this domain, or whether the slowing
down caused by the inability to use vectorized computations (because we can't
load the entire thing into memory at once) will just be too much.&lt;/p&gt;
&lt;h1&gt;Python 3: 5 minutes&lt;/h1&gt;
&lt;p&gt;Yes, that's right. It takes Python 3 &lt;strong&gt;5 minutes&lt;/strong&gt; to do the same task that
took R &lt;strong&gt;over a day&lt;/strong&gt;. The code feels a lot simpler too:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;pos_sets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stdin&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lemma&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tag&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxsplit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;pos&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tag&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="c1"&gt;# this is an intentionally naive implementation which mimicks&lt;/span&gt;
            &lt;span class="c1"&gt;# the R code and something an inexperienced coder might do;&lt;/span&gt;
            &lt;span class="c1"&gt;# a more concise and probably better performing solution could&lt;/span&gt;
            &lt;span class="c1"&gt;# be achieved using dict.setdefault() or collections.defaultdict&lt;/span&gt;
            &lt;span class="c1"&gt;# / collections.Counter&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;pos&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;pos_sets&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;pos_sets&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;lemma&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;pos_sets&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
                &lt;span class="n"&gt;pos_sets&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;lemma&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;pos_sets&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;lemma&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

    &lt;span class="n"&gt;diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Done in {diff:.0f} seconds.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;FYI, this was run using Python 3.6. As a rule, use always the most recent
version of Python 3 you can (at least 3.5, 3.4 in a pinch; with earlier
releases, you may encounter performance issues). In any case, &lt;strong&gt;do not use
Python 2&lt;/strong&gt; for new projects and let it end-of-life in peace.&lt;/p&gt;
&lt;h1&gt;Perl: 13 minutes&lt;/h1&gt;
&lt;p&gt;Perl used to be a popular alternative for text processing. Like R, it has its
fair share of nauseating language design and weird quirks, but since it was
actually meant for use in this domain, it won't spectacularly let you down.&lt;/p&gt;
&lt;p&gt;(Unless your data is silently corrupted because you handled text encoding
wrong. Perl's behavior in this respect is a relict of a pre-UTF-8-everywhere
past, and it's the single biggest reason for why the language should be put out
of its misery already.)&lt;/p&gt;
&lt;p&gt;Here's the code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;use&lt;/span&gt; &lt;span class="nn"&gt;strict&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;use&lt;/span&gt; &lt;span class="nn"&gt;utf8&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;use&lt;/span&gt; &lt;span class="nn"&gt;open&lt;/span&gt; &lt;span class="sx"&gt;qw(:std :encoding(utf8))&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;my&lt;/span&gt; &lt;span class="nv"&gt;$start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;time&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="k"&gt;my&lt;/span&gt; &lt;span class="nv"&gt;%pos_sets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sr"&gt;/\t/&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;my&lt;/span&gt; &lt;span class="nv"&gt;@attrs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;split&lt;/span&gt; &lt;span class="sr"&gt;/\t/&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;my&lt;/span&gt; &lt;span class="nv"&gt;$lemma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;@attrs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
    &lt;span class="k"&gt;my&lt;/span&gt; &lt;span class="nv"&gt;$tag&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;@attrs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
    &lt;span class="k"&gt;my&lt;/span&gt; &lt;span class="nv"&gt;$pos&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;substr&lt;/span&gt; &lt;span class="nv"&gt;$tag&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="c1"&gt;# auto-vivification: ergonomic, but also made possible by the whole&lt;/span&gt;
    &lt;span class="c1"&gt;# &amp;quot;implicit defaults that have a potential of screwing stuff up&lt;/span&gt;
    &lt;span class="c1"&gt;# without you even knowing about it&amp;quot; culture of Perl&lt;/span&gt;
    &lt;span class="nv"&gt;$pos_sets&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nv"&gt;$pos&lt;/span&gt;&lt;span class="p"&gt;}{&lt;/span&gt;&lt;span class="nv"&gt;$lemma&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;my&lt;/span&gt; &lt;span class="nv"&gt;$diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nv"&gt;$start&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="bp"&gt;STDERR&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Done in $diff seconds.\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Bottom line though, being more than twice as slow as Python 3 (which came as a
surprise to me, I must admit) and definitely the worse language, it has little
to recommend itself if you're considering to learn a new language for this type
of task.&lt;/p&gt;
&lt;p&gt;Except maybe if you want to continuously log what the program is doing to a
terminal -- like output the number of lines processed after each line. Perl is
clearly very efficient at writing to a terminal, the running time is basically
the same with continuous logging incorporated. By contrast, Python 3 takes
about three times longer (~ 15 minutes).&lt;/p&gt;
&lt;p&gt;(I guess maybe Python flushes output after each &lt;code&gt;print()&lt;/code&gt; call, whereas Perl
does some smart buffering which results in it not being slowed down by the
latency of the terminal...? Who knows, at any rate, it's hardly a "killer"
feature.)&lt;/p&gt;
&lt;h1&gt;Rust: 1.25 minutes&lt;/h1&gt;
&lt;p&gt;As a compiled, systems-level language, Rust is in a different league compared
to the previous contestants: of course it's going to be faster. I included it
because it provides a frame of reference. The important takeaway is that we're
in the same ballpark with Python 3 (roughly units of minutes), so there's no
pressing need to turn to a compiled language for this task.&lt;/p&gt;
&lt;p&gt;Here's the code, for completeness sake:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;use&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;::&lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;use&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;::&lt;span class="n"&gt;io&lt;/span&gt;::&lt;span class="n"&gt;prelude&lt;/span&gt;::&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;use&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;::&lt;span class="n"&gt;collections&lt;/span&gt;::&lt;span class="n"&gt;HashMap&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;use&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;::&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="k"&gt;type&lt;/span&gt; &lt;span class="nc"&gt;LemmaCount&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;HashMap&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nb"&gt;String&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;i32&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;type&lt;/span&gt; &lt;span class="nc"&gt;PosSet&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;HashMap&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;char&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LemmaCount&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="k"&gt;fn&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;::&lt;span class="n"&gt;SystemTime&lt;/span&gt;::&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;mut&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;pos_sets&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;PosSet&lt;/span&gt;::&lt;span class="n"&gt;new&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;stdin&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;io&lt;/span&gt;::&lt;span class="n"&gt;stdin&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;stdin&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lock&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;lines&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unwrap&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contains&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;mut&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;attrs&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;skip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;take&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;lemma&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;attrs&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;unwrap&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;tag&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;attrs&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;unwrap&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;tag&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;chars&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;take&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;unwrap&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;pos_set&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;pos_sets&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;or_insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;LemmaCount&lt;/span&gt;::&lt;span class="n"&gt;new&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;count_for_lemma&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;pos_set&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;String&lt;/span&gt;::&lt;span class="n"&gt;from&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lemma&lt;/span&gt;&lt;span class="p"&gt;)).&lt;/span&gt;&lt;span class="n"&gt;or_insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;count_for_lemma&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;diff&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;elapsed&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;unwrap&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;as_secs&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Done in {:.0} seconds.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;diff&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note in passing how nicely the Rust code reads for a compiled language. Of
course, since it's a much stricter (and safer) language than Python, it's more
ceremonious to write and the APIs are more complicated, because they have to
adhere to the various memory management guarantees Rust gives you (among other
things). But once the code is written, it's very readable and clear. And all
necessary functions and data structures are (a) available in the standard
library, and (b) plenty efficient.&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Just to be clear: the ultimate purpose of this post is &lt;strong&gt;not&lt;/strong&gt; bashing R (not
for being slow at text munching, at any rate); it's to give a convincing
account of why it's just not the right tool for the job. And not in a small
way, either -- in a way that requires to learn a different tool, there's no way
around it. Let me reiterate that my recommendation would hands down be &lt;a href="http://www.nltk.org/book/"&gt;Python
3&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Once the data is extracted, go back to R by all means. Although
Python does have a fairly nice high-level &lt;a href="http://pandas.pydata.org/"&gt;data analysis library&lt;/a&gt;, it's
not my intention to discourage anyone from using R for what it &lt;em&gt;is&lt;/em&gt; good at,
especially if this is a skill they are already proficient in.&lt;/p&gt;
&lt;p&gt;The internet is full of people asking advice on which programming language to
learn, and the answers are invariably evasive -- it depends on your tastes,
what fits your brain better, what your use case is. In the hopes that some
people might find opinionated guidance useful for a change (I know I personally
often do, when flirting with a new language): if you're looking to process
large quantities of text data, the answer is a big, resounding &lt;strong&gt;NOT R&lt;/strong&gt;!&lt;/p&gt;
&lt;h1&gt;A vectorized postscript&lt;/h1&gt;
&lt;p&gt;Since I ran these on a server with 64 GB of RAM, I figured I might as well try
loading everything into memory in R and doing it the proper, vectorized way,
while I'm at it. Here's the code, using &lt;code&gt;dplyr&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;dplyr&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;stringi&lt;span class="p"&gt;)&lt;/span&gt;

start &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;Sys.time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

con &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;file&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;stdin&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; open&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;rt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
corpus &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;readLines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;con&lt;span class="p"&gt;)&lt;/span&gt;

diff &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;Sys.time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; start
&lt;span class="kp"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;sprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Corpus read in after %g %s.\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;diff&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;units&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;diff&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

corpus &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; stri_subset_fixed&lt;span class="p"&gt;(&lt;/span&gt;corpus&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;\t&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
corpus &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; stri_split_fixed&lt;span class="p"&gt;(&lt;/span&gt;corpus&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;\t&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; simplify&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
freq_dist &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; tibble&lt;span class="p"&gt;(&lt;/span&gt;
  POS&lt;span class="o"&gt;=&lt;/span&gt;stri_sub&lt;span class="p"&gt;(&lt;/span&gt;corpus&lt;span class="p"&gt;[,&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; from&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; to&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
  LEMMA&lt;span class="o"&gt;=&lt;/span&gt;corpus&lt;span class="p"&gt;[,&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  group_by&lt;span class="p"&gt;(&lt;/span&gt;POS&lt;span class="p"&gt;,&lt;/span&gt; LEMMA&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  summarize&lt;span class="p"&gt;(&lt;/span&gt;FREQ&lt;span class="o"&gt;=&lt;/span&gt;n&lt;span class="p"&gt;())&lt;/span&gt;

diff &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;Sys.time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; start
&lt;span class="kp"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;sprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Finished processing corpus after %g %s.\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;diff&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;units&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;diff&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let me say at the outset that this code looks much nicer -- it's clean, modern
R, made possible in great part by Hadley Wickham's efforts to redesign the data
manipulation vocabulary from the ground up. Note also that we've made a
concession on our requirements: the resulting data structure is a tibble, not a
hash, i.e. key lookup time is not constant but depends on the size of the data.&lt;/p&gt;
&lt;p&gt;Well, just loading the corpus into memory took ~18 minutes. The script then ran
for &lt;strong&gt;several days&lt;/strong&gt;, in the course of which I checked every now and then to
see how much memory it was using: ~35 GB. I don't suppose anyone has that much
RAM on their laptop. Then someone rebooted the server before the program could
complete. I think you'll agree the experiment is conclusive even so.&lt;/p&gt;
&lt;!----------------------------- LINKS AND NOTES -----------------------------&gt;

&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn-1"&gt;
&lt;p&gt;You could also offload the decompression to a different thread in the
same process, but that complicates the implementation. Piping gives you
parallelization basically for free.&amp;#160;&lt;a class="footnote-backref" href="#fnref-1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="python"></category><category term="r"></category><category term="rust"></category><category term="perl"></category><category term="text processing"></category></entry><entry><title>The Cathedral and the Bazaar: What is a Useful Notion of “Language”?</title><link href="https://dlukes.github.io/cathedral-and-bazaar.html" rel="alternate"></link><published>2016-07-21T00:00:00+02:00</published><updated>2016-07-21T00:00:00+02:00</updated><author><name>dlukes</name></author><id>tag:dlukes.github.io,2016-07-21:/cathedral-and-bazaar.html</id><summary type="html">&lt;p&gt;Why Noam Chomsky's approach to linguistics is broken, and why you should take a look at agent-based models of language evolution.&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you like the essay, then you'll definitely want to take a look at Luc
Steels's &lt;em&gt;The Talking Heads Experiment: Origins of Words and Meanings&lt;/em&gt;. It's
published as an open-access book by Language Science Press, so
&lt;a href="http://langsci-press.org//catalog/book/49"&gt;go grab the free download&lt;/a&gt;!&lt;/p&gt;
&lt;h1&gt;Abstract&lt;/h1&gt;
&lt;p&gt;The essay analyzes why Noam Chomsky’s notion of language (both its
essence — language as a set of grammatical sentences — and genesis)
leads neither to interesting discoveries nor even to useful questions
from the point of view of linguistics as a science. A much more fruitful
approach to language is to view it as a complex, dynamic, distributed
system with emergent properties stemming from its functions, as
advocated e.g. by Luc Steels. The argument will be developed against the
backdrop of the evolution of Ludwig Wittgenstein’s thought, from the
&lt;em&gt;Tractatus&lt;/em&gt; to the concept of language games, i.e. from an approach to
language based on thorough formal analysis but also misconceptions about
its functions, to a much keener though less formal grasp of its praxis
and purpose.&lt;/p&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;At least since Thomas Kuhn’s &lt;em&gt;The Structure of Scientific Revolutions&lt;/em&gt;,
it has been a fairly commonplace notion that working within the confines
of a particular scientific paradigm conditions to a certain extent the
questions one is likely to ask and therefore also the answers that
ensue. This effectively limits the range of possible discoveries,
because some are not answers to meaningful questions within a given
framework while other observations still are taken as given axioms,
which means they cannot be the target of further scientific
investigation.&lt;/p&gt;
&lt;p&gt;In contemporary linguistics, one very prominent such paradigm is that of
&lt;em&gt;generative grammar&lt;/em&gt;, single-handedly established in 1957 by Noam
Chomsky in his seminal work &lt;em&gt;Syntactic Structures&lt;/em&gt;. While serious
criticism has been leveled over time against this initial exposition as
well as Chomsky’s subsequent elaborations on it (see Pullum 2011;
Sampson 2015; and Sampson 2005 for a book-length treatment), the book
undeniably attracted significant numbers of brilliant young minds under
the wings of its research program, which went from aspiring challenger
in the domain of linguistics to established heavyweight in a
comparatively short period of time (the transition had been achieved by
the mid-1970s at the latest). In the process, it co-opted or spawned
various other sub-fields of linguistics, and even rebranded itself, such
that &lt;em&gt;Cartesian linguistics&lt;/em&gt;, &lt;em&gt;cognitive linguistics&lt;/em&gt; and most recently
&lt;em&gt;biolinguistics&lt;/em&gt; are all labels which suggest a strong generativist
presence.&lt;/p&gt;
&lt;p&gt;One serious competitor to the Chomskyan account of language that has
emerged over the years is the field of &lt;em&gt;evolutionary linguistics&lt;/em&gt;. It
might seem strange at first glance why biolinguistics and evolutionary
linguistics should be at odds. As their names indicate, they both aspire
to a close relationship with biology, which seems to indicate their
research agendas and outlooks should largely overlap. Yet their
fundamental assumptions about what constitutes language are so
irreconcilable that they might as well be considered to deal with
different objects of study. Of the two, it is evolutionary linguistics
which leads to questions and investigations which can be conceived of as
scientific (in the Popperian sense of involving falsifiable hypotheses
instead of being merely speculative), consequently yielding the most
useful insights – in the fairly pedestrian sense that these can be
intersubjectively replicated without resorting to an argument from
authority, which makes them a better foundation to build upon, because
the superadded structures are less likely to crumble should said
authority ever change their mind, as Chomsky has done several times
already.&lt;/p&gt;
&lt;h1&gt;Wittgenstein on language: From logical calculus to language games&lt;/h1&gt;
&lt;p&gt;Let us now take a short détour through the development of Ludwig
Wittgenstein’s thoughts on language, so that we may couch our later
discussion of the differences between generative grammar /
biolinguistics and evolutionary linguistics in terms of a contrast that
is perhaps more familiar. The imagery in the title of the present essay
was borrowed from Eric S. Raymond’s book &lt;em&gt;The Cathedral &amp;amp; the Bazaar:
Musings on Linux and Open Source by an Accidental Revolutionary&lt;/em&gt;
(Raymond 1999). In it, Raymond describes two models of collaborative
software development, one of them very rigid, restrictive and hostile to
newcomers (the “cathedral”), the other overwhelmingly inclusive, open to
outside contributions and organic change, an effervescent hive of
activity (the “bazaar”), whose unexpected but empirically demonstrable
virtues he has come to embrace.&lt;/p&gt;
&lt;p&gt;This architectural metaphor also happens to be very apt when
characterizing Wittgenstein’s view of language in the two major stages
of his thought, as represented by his two books &lt;em&gt;Tractatus
Logico-Philosophicus&lt;/em&gt; and &lt;em&gt;Philosophical Investigations&lt;/em&gt;. In the
&lt;em&gt;Tractatus&lt;/em&gt;, Wittgenstein has a “preconceived idea of language as an
exact calculus operated according to precise rules” (McGinn 2006, 12)
and formalizing this system of rules leads him to the following dogmatic
conclusion: “what can be said at all can be said clearly, and what we
cannot talk about we must pass over in silence” (Wittgenstein 2001, 3).
The deontic force of the final injunction should be taken with a grain
of salt; it could perhaps be rephrased in the following less
epigraph-worthy manner: there is a sharp logical boundary to be drawn
between meaningful and nonsensical propositions, and the purpose of
language is to construct meaningful ones, therefore it is futile (rather
than strictly forbidden) to engage in nonsensical ones.&lt;/p&gt;
&lt;p&gt;There have been attempts to read the &lt;em&gt;Tractatus&lt;/em&gt; in an ironic mode, as a
consciously doomed, self-defeating attempt to circumscribe the limits to
the expression of thought, which prefigures the much more subtle
attitude towards language that Wittgenstein later exhibits in the
&lt;em&gt;Philosophical Investigations&lt;/em&gt; (see McGinn 2006, 5–6 and elsewhere for
an overview of this so-called “resolute” reading). In my opinion, such a
stance exhibits a blatant, possibly wilful disregard of his almost
penitent tone in the preface to &lt;em&gt;Philosophical Investigations&lt;/em&gt;: “I could
not but recognize grave mistakes in what I set out in that first book”
(Wittgenstein 2009, 4e).&lt;/p&gt;
&lt;p&gt;Where does Wittgenstein think he went wrong then? Arguably, the most
serious misconception was conferring a privileged ontological status to
language, seeing it as “the unique correlate, picture, of the world”
(Wittgenstein 2009, 49e), whereas in fact, these referential properties
are highly dependent on communicative context. It signifies only insofar
as it has an effect on the addressee (another human being, or even
myself) which to all practical intents and purposes the speaker can
identify as somehow related to what she was trying to achieve with her
utterance in the first place. But there is little meaning, in any
practical sense, outside these highly particular, localized (in both
physical and cultural space and time), embodied, grounded interactions.
Wittgenstein calls these interactions “language-games” to “emphasize the
fact that the &lt;em&gt;speaking&lt;/em&gt; of language is part of an activity, or of a
form of life” (Wittgenstein 2009, 15e, original emphasis). Of course,
the notion of game still involves some kind of rules, but by focusing on
the activity rather than its regulations, it is much easier to account
for “the case where we play, and make up the rules as we go along […]
and even where we alter them – as we go along” (Wittgenstein 2009, 44e).&lt;/p&gt;
&lt;p&gt;This is not to say that we cannot construct abstractions, although
Wittgenstein himself is clearly in favor of systematically examining
particular cases: “In order to see more clearly, here as in countless
similar cases, we must look at what really happens &lt;em&gt;in detail&lt;/em&gt;, as it
were from close up” (Wittgenstein 2009, 30e). However, once we do
abstract away, it is crucial to approach the resulting theory from a
pragmatic standpoint: “We want to establish an order in our knowledge of
language: an order for a particular purpose, one out of many possible
orders, not &lt;em&gt;the&lt;/em&gt; order” (Wittgenstein 2009, 56e, original emphasis).&lt;/p&gt;
&lt;h1&gt;Generative grammar&lt;/h1&gt;
&lt;p&gt;In many ways, Chomsky conceives of language as the early Wittgenstein
did, i.e. under the cathedral metaphor. This may come across as a
surprise because unlike Wittgenstein, he is not concerned with issues of
meaningfulness, philosophical or otherwise. Indeed, it is one of his
fundamental precepts that grammar can and should be dissociated from
meaning, as demonstrated by his famous example sentence “Colorless green
ideas sleep furiously”, which he claims is perfectly grammatical yet
meaningless&lt;sup id="fnref-1"&gt;&lt;a class="footnote-ref" href="#fn-1"&gt;1&lt;/a&gt;&lt;/sup&gt; (Chomsky 2002, Chap. 2).&lt;/p&gt;
&lt;p&gt;Nevertheless, the goal for both is to describe language &lt;em&gt;per se&lt;/em&gt;, in the
abstract, without regard to its context-grounded use in actual
communication. Both strive to give a highly formal definition of the
system they think they are uncovering: while Wittgenstein attempts to
establish a logical calculus of how propositions can be said to carry
meaning in terms of their referential relationship to external reality,
Chomsky tries to hint at a calculus which would determine which
candidate sentences belong to the language encoded by this calculus,
i.e. separate those that are grammatical from those that are not.&lt;/p&gt;
&lt;p&gt;Another way to put this is that the descriptive part of linguistics can
be equated with formal language theory: a language is viewed as a
(potentially infinite) set of symbol strings (sentences), and the
linguist’s task is to find the simplest and most elegant set of rules
that would constitute the basis for a procedure to generate (hence
&lt;em&gt;generative&lt;/em&gt; grammar) all of them and only those, whether observed or
potential. Chomsky himself gives the following definition: “by a
generative grammar I mean simply a system of rules that in some explicit
and well-defined way assigns structural descriptions to sentences”
(Chomsky 1965, 8).&lt;/p&gt;
&lt;p&gt;Furthermore, “Linguistic theory is concerned primarily with an ideal
speaker-listener, in a completely homogeneous speech-community, who
knows its language perfectly” (Chomsky 1965, 3). Specifically, it is
concerned with his “&lt;em&gt;competence&lt;/em&gt; (the speaker-hearer’s [intrinsic]
knowledge of his language)”, not his “&lt;em&gt;performance&lt;/em&gt; (the actual use of
language in concrete situations)” (Chomsky 1965, 4). Additionally, no
claim is made as to the cognitive or neurophysiological accuracy of the
mechanisms described, although to hedge his bets both ways, Chomsky adds
that “No doubt, a reasonable model of language use will incorporate, as
a basic component, the generative grammar that expresses the
speaker-hearer’s knowledge of the language” (Chomsky 1965, 9).&lt;/p&gt;
&lt;p&gt;In short, Chomsky consciously sets up the playing field for a thoroughly
mentalistic, speculative discipline. At first, it might seem like
reasonable approximation and a small concession to make, especially in
the face of the sheer daunting complexity of all the intricate
mechanisms that conspire to yield the phenomenon we call language, but
only until one fully realizes the consequences of such a move. Observe
for instance the carefully crafted loophole claiming that linguistics is
primarily concerned with an ideal speaker-hearer’s competence and that
actual usage data is just circumstantial evidence. This effectively
allows linguists to dismiss inconvenient edge cases or counterexamples
to their theories purely on grounds of their being noisy data or slips
of the tongue, which is something they are allowed to determine based on
introspection. Mind you, this is not just a theoretical loophole;
Chomsky himself has repeatedly relied on it, especially with respect to
so-called linguistic universals (see e.g. Sampson 2005, 139 or 160).&lt;/p&gt;
&lt;p&gt;The net result is rampant, unchecked theorizing. One such example is the
postulation of a two-layer linguistic analysis, the observed language
data corresponding to a surface structure which provides hints as to an
underlying, more regular deep structure to be uncovered. The deep
structure is purportedly closer to the universal properties of language;
both layers are linked by a system of transformations:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We can greatly simplify the description of English and gain new and important
insight into its formal structure if we limit the direct description in terms
of phrase structure to a kernel of basic sentences (simple, declarative,
active, with no complex verb or noun phrases), deriving all other sentences
from these (more properly, from the strings that underlie them) by
transformation, possibly repeated. (Chomsky 2002, 106–7)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Constructing a formal framework for grammar modeling with cognitively
unmotivated levels of abstraction might have been a valid goal (though
arguably not within linguistics) if the result was indeed, as Chomsky
claims it to be, maximally elegant, as simple as can be but no simpler.
I was not able to track down a formal definition of this criterion, but
simplicity is clearly discursively construed as a desirable quality:
“simple and revealing” (Chomsky 2002, 11) or “effective and
illuminating” (Chomsky 2002, 13) are Chomsky’s choice epithets for what
to look for in a grammar. But that is not true either: transformations
are an unnecessary addition, singling them out as a separate category of
operations adds nothing to the generative power of his system (Pullum
2011, 290). They are therefore a wart under any reasonable definition of
“simplicity” and Chomsky thus manages to fall short of even the
self-defined, theory-internal standards that are the only ones he allows
his enterprise to be held to.&lt;/p&gt;
&lt;h1&gt;Ontogeny and phylogeny&lt;/h1&gt;
&lt;p&gt;As we have seen, generative grammar concerns itself with an ideal
speaker-hearer’s competence in a perfectly homogeneous community. The
trouble is that such an impoverished model eschews any possibility of
dynamism. The very dichotomy between grammatical and ungrammatical is
intuitively problematic if we consider that judgments are bound to
diverge when made in reference to different dialects, sociolects and
idiolects,&lt;sup id="fnref-2"&gt;&lt;a class="footnote-ref" href="#fn-2"&gt;2&lt;/a&gt;&lt;/sup&gt; not to mention that binary classification might be too
reductive in some cases (how would you categorize, on first encounter, a
construction which you passively understand but would never produce
actively?). Though Chomsky sometimes mentions in passing the possibility
of levels of grammaticalness which would allow a finer-grained analysis
(e.g. Chomsky 2002, 16; Chomsky 1965, 11), it seems to be just another
instance of hedging his bets, he never makes it a fundamental component
of his theory. This would seem to indicate that Chomsky’s theory of
language has a serious problem in that it is unable to account for any
phenomena that involve fluctuations in linguistic ability, including
language emergence / diachronic change (phylogeny) and acquisition
(ontogeny).&lt;/p&gt;
&lt;p&gt;Chomsky’s response to this is that our language faculty is largely
innate: we are genetically endowed with a &lt;em&gt;language-acquisition device&lt;/em&gt;
in our brains (Chomsky 1965, 31–33) which can supposedly infer the
correct grammatical rules even given incomplete, limited and noisy
input, which is what Chomsky argues children get (the “poverty of
stimulus” argument), thanks to strong universal constraints on what a
human language can be like. Under this account, the capacity for
language, initially “a language of thought, later externalized and used
in many ways” (Chomsky 2007, 24), appeared as a random mutation in a
single individual and progressively spread through the population
because it offered a considerable competitive advantage: “capacities for
complex thought, planning, interpretation” (Chomsky 2007, 22).&lt;/p&gt;
&lt;p&gt;In his later career, Chomsky increasingly focused on exploring this
purported shared genetic basis for human language, hence the
aforementioned label “biolinguistics”. Make no mistake, this in no way
entails a turn from mentalism towards empirical neurophysiological or
genetic investigation. Quite to the contrary, liberated from the
constraints of having to account for individual existing languages in
detail, he soars to new heights of abstractness in postulating the
formal language underpinnings of human language. The &lt;em&gt;principles and
parameters&lt;/em&gt; model of Universal Grammar (Chomsky 1986) expands upon the
notion of linguistic universals by splitting them up into two sets:
principles, which are hardwired and immutable, and parameters, which are
hardwired too but can be flipped on or off based on linguistic behavior
observed by the child in her particular language community. Since the
choices are heavily constrained, the learner can infer correct parameter
settings in spite of deficient input. This line of research culminates
in the so-called &lt;em&gt;minimalist program&lt;/em&gt; (Chomsky 1995), where Chomsky
identifies the “core principle of language, [the operation of]
unbounded Merge” (Chomsky 2007, 22). Under the “strong” minimalist
hypothesis, this would be the only principle necessary to account for
human-like languages (Chomsky 2007, 20), which would paradoxically
essentially discard all work (or should I say speculation?) previously
done on the parameters side of the Universal Grammar project. All other
universal characteristics of language could then be explained by newly
introduced “interface” conditions,&lt;sup id="fnref-3"&gt;&lt;a class="footnote-ref" href="#fn-3"&gt;3&lt;/a&gt;&lt;/sup&gt; i.e. constraints on how language
inter-operates with other systems, including thought and physical
language production (Chomsky 2007, 14);&lt;sup id="fnref-4"&gt;&lt;a class="footnote-ref" href="#fn-4"&gt;4&lt;/a&gt;&lt;/sup&gt; all empirically documented
variation between the world’s languages would be chalked up to lexical
differences (Chomsky 2007, 25).&lt;/p&gt;
&lt;p&gt;The poverty of stimulus and language universals arguments for innateness
have been thoroughly debunked, especially in Geoffrey Sampson’s
book-length diatribe &lt;em&gt;The &lt;/em&gt;‘&lt;em&gt;Language Instinct&lt;/em&gt;’&lt;em&gt; Debate&lt;/em&gt;. In short, it
turns out that some of the grammatical constructions which were assumed
to be absent from a language learner’s input yet acquired nonetheless
have since been empirically proven to occur fairly commonly (Sampson
2005, 72–79). Moreover, there is no qualitative difference between a
statement like “the stimulus is too poor to allow language learning
without a genetic basis” and “the stimulus is just rich enough etc.”,
both are unverifiable unless we have already independently proven that
language learning occurs one way or the other, so to adduce either of
the statements as proof for the hypothesis at stake is misguided
(Sampson 2005, 47–48). Finally, the alleged language universals turn out
to be either false when checked against additional languages (Sampson
2005, 138–9) or so general as to be meaningless (Sampson 2005, Chap. 5).&lt;/p&gt;
&lt;p&gt;Irrespective of this, let us suppose for a moment that genetic mutation
and subsequent inheritance do play a role in the emergence of language,
and work out an account of language emergence consistent with this
hypothesis. If language started out through mutation in a single
individual as a purely internal advanced conceptualization faculty, then
once it started to spread, what was the motivation for the
genetically-endowed humans to externalize their thoughts? How did they
know to which of their peers they could speak (which had inherited the
mutation) and which not? And most importantly, how did they know which
parameters of Universal Grammar to flip on and which off, if there was
no prior language based on which to decide? Universal Grammar would have
had to be fairly detailed in order for intersubjective agreement on the
norms for the first ever human language to be reached on the basis of it
alone. Yet as we have seen, Chomsky has been moving away from this
notion – at the limit, the minimalist program posits only one very
general mechanism required for language. The poverty of stimulus
argument is turned against its creator as the argument from poverty of
the machinery supposed to make up for the poverty of said stimulus.&lt;/p&gt;
&lt;p&gt;Alternatively, if we fully subscribe to the minimalist program and the
notion that all the surface variety exhibited by language comes from the
lexicon, then how are individual words created, how do they propagate?
One might be tempted to say “people just invented them”, but consider
for a while that in the current setup, there is absolutely no mechanism
that would explain how a community of speakers reaches agreement on
their lexicon – this theory offers no incentive whatsoever for consensus
to be reached; from its point of view, a solution where each speaker
ends up with their own private lexicon is equally valid because
indistinguishable on the basis of the theory’s conceptual apparatus.
Chomsky’s ideas on phylogenesis appear thoroughly ridiculous when fully
carried out to their logical consequences, and this can all be blamed on
his sterile, idealized and static view of language which dismisses
actual communication as a secondary purpose and therefore a peripheral
issue.&lt;/p&gt;
&lt;p&gt;On a side note, it is hard to say which aspect of Chomsky’s theory of
language came first – whether innateness accommodated the mentalism and
the concomitant quest for formal purity (botched as it may be) of
generative grammar, whether it was the other way round, or whether they
perhaps co-evolved in his mind. The facts are that Chomsky’s initial
publications on generative grammar concentrate on the formal language
theory part (Chomsky 1956; Chomsky 2002), but he added the innateness
argument fairly early on, even tacking a seemingly respectable
philosophical lineage onto it in &lt;em&gt;Cartesian Linguistics&lt;/em&gt; (Chomsky 2009),
which pretends to trace back both innateness and mentalism to Descartes
and the Port-Royal grammarians, binding them as two sides of the same
coin. It is worth noting that in both formal language theory and history
of linguistics / philosophy, Chomsky is more of a dabbler than an
expert: he has provably borrowed most of his ideas in the former field
from others, sometimes mangling them or extending them in unfortunate
ways (Pullum 2011; Sampson 2015), and has thoroughly underresearched (or
wilfully twisted?) his understanding of the latter, which has resulted
in serious misrepresentations of the history of ideas (Miel 1969;
Aarsleff 1970).&lt;/p&gt;
&lt;h1&gt;Evolutionary linguistics&lt;/h1&gt;
&lt;p&gt;There are various sub-fields of linguistics which are in discord with
generative grammar, especially over the notion that performance data
should be used only as evidence for guiding the speculation and detailed
usage and frequency patterns should be disregarded; the primacy of
syntax (as advocated by Chomsky) is also disputed. One of these
sub-fields is obviously corpus linguistics, which takes a decidedly
empiricist stance and starts by assembling a large body of language data
(a corpus) from which patterns of language use are inferred.
Nevertheless, not all of these compete with generative grammar at the
fundamental explanatory level of how language came about
phylogenetically and how it is transmitted by ontogenetic acquisition.&lt;/p&gt;
&lt;p&gt;We have repeatedly encountered Chomsky’s emphasis on how communication,
actual interactions between speakers, are just an afterthought in the
system of language:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;evolutionary biologist Salvador Luria was the most forceful advocate of the
view that communicative needs would not have provided “any great selective
pressure to produce a system such as language,” with its crucial relation to
“development of abstract or productive thinking.” His fellow Nobel laureate
François Jacob (1977) added later that “the role of language as a
communication system between individuals would have come about only
secondarily, as many linguists believe,” (Chomsky 2007, 23)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Part of this vehemence dovetails with the single individual mutation
hypothesis of the origins of language – it helps if the significance of
communication is downplayed in an account where communication is
initially impossible, simply because there is no other language-endowed
being to communicate with. If communication were language’s killer
feature, then the selective pressure for the incriminated gene to
propagate would not kick in.&lt;/p&gt;
&lt;p&gt;The other part can reasonably be attributed to Chomsky’s intent to make
a clean break from a prior popular theory on language acquisition,
epitomized by B. F. Skinner’s 1957 monograph &lt;em&gt;Verbal Behavior&lt;/em&gt;, which
offered a heavily empiricist, behaviorist account of language learning
in terms of a stimulus-response cycle. Characteristically, Chomsky’s
strategy is to trivialize the function of the stimulus, casually
implying both that it might not be needed at all, and if it is, then
details of the role it plays are of little interest:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;it would not be at all surprising to find that normal language learning
requires use of language in real-life situations, in some way. But this, if
true [sic!], would not be sufficient to show that information regarding
situational context (in particular, a pairing of signals with structural
descriptions that is at least in part prior to assumptions about syntactic
structure) plays any role in determining how language is acquired, once the
mechanism is put to work and the task of language learning is undertaken by
the child. (Chomsky 1965, 33)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In retrospect, Skinner’s account may be simplistic in many ways, but the
basic notion that one has to pay attention to stimuli and responses in
the course of particular linguistic interactions is sound. In
particular, a theory of language built on this foundation successfully
copes with all of the impasses we have explored above regarding
Chomsky’s approach. One such framework is that of evolutionary
linguistics.&lt;/p&gt;
&lt;p&gt;Evolutionary linguistics views language as a &lt;em&gt;complex adaptive system&lt;/em&gt;
with &lt;em&gt;emergent properties&lt;/em&gt; (Steels 2015, 8–9). A complex adaptive system
is a system which is not centrally organized, coordinated or designed:
its “macroscopic” characteristics are said to “emerge” as the result of
localized interactions between individual entities (agents) with similar
“microscopic” characteristics (be they physical, behavioral or
motivational). The whole is more than the sum of its parts, and none of
the agents can be properly said to have designed the system, nor can
they deliberately change it in an arbitrary way; but all are
continuously shaping it by taking part in the interactions that
constitute its fabric. Examples of complex adaptive systems include the
dynamics of insect societies (beehives, ant nests) or patterns of
collective motion in large animal groups (flocks of birds or shoals of
fish). These and more are discussed in much greater depth in the first
chapter of Pierre-Yves Oudeyer’s book &lt;em&gt;Self-Organization in the
Evolution of Speech&lt;/em&gt;. Adaptiveness is a property that these systems
acquire by virtue of not being hardwired on the macro level: they are
defined functionally instead of structurally. If the conditions in the
environment change, the system will adapt to keep fulfilling its
function, because the agents are forced to modify their behavior in
order to achieve their individual goals. Of course, they may fail to do
so, in which case the system breaks down and ceases to exist.&lt;/p&gt;
&lt;p&gt;If we revert to the metaphor from the title of the present essay,
according to Chomsky, language is a cathedral erected by a single
unwitting architect, the random genetic mutation that endowed us with
the language faculty. Conversely, Luc Steels and fellow evolutionary
linguists argue that the apparent macroscopic orderliness of language is
the result of a myriad interactions of multiple individual agents, as
suggested by the the bazaar image.&lt;/p&gt;
&lt;p&gt;One form that linguistic research can take under this paradigm is
formulating and running computational models which simulate the behavior
of agent populations and study the microscopic conditions, i.e. the
cognitive and physical abilities, motivations etc. of each agent,
necessary for a system like language to emerge within the population and
stabilize. By direct inspiration from Wittgenstein’s &lt;em&gt;Philosophical
Investigations&lt;/em&gt;, the interactions between agents are termed “language
games” (Steels 2015, 167–8); depending on the topic being investigated,
the agents can play different types of language games with different
rules. It is openly acknowledged that such simulations represent only a
limited approximation of a well-defined subspace of the actual uses of
language. In the research to date, rules are generally definite and set
for the entire experiment, but simulating language games with fuzzy
rules remains a perfectly valid research topic within this framework, in
the Wittgensteinian spirit of allowing rules to be made up and modified
“as we go along” (Wittgenstein 2009, 44e).&lt;/p&gt;
&lt;p&gt;A relatively simple game that agents can play is the so-called &lt;em&gt;Guessing
Game&lt;/em&gt; (see Chap. 2 of Steels 2015 for more details). In this scenario, a
population of agents, embodied in physical robots, tries to establish a
shared lexicon and coupled ontology for a simple world consisting of
geometrical shapes. Each game is an interaction of two agents picked at
random, in the context of a scene consisting of said geometrical shapes.
One agent (the speaker) takes the initiative, selects a topic from the
scene and names it; the other (the hearer) tries to guess which object
the first one had in mind and points to it; the speaker decodes the
pointing gesture and the game succeeds if he interprets it as
referencing his original topic. If so, he acknowledges the match;
otherwise, he points at the intended topic as a repair strategy. At the
outset, neither the lexicon nor the ontology are given, only a set of
sensors and actuators (which allow the agents to interact with the
environment by taking in streams of raw perceptual data or producing
sound and pointing gestures) and very general cognitive principles.
These include an associative memory and feedback mechanisms to propagate
failures and successes in conceptualization and communication to all
components of the system and act on them.&lt;sup id="fnref-5"&gt;&lt;a class="footnote-ref" href="#fn-5"&gt;5&lt;/a&gt;&lt;/sup&gt; New distinctions along the
perceptual dimensions are introduced in a random fashion,&lt;sup id="fnref-6"&gt;&lt;a class="footnote-ref" href="#fn-6"&gt;6&lt;/a&gt;&lt;/sup&gt; but those
that lead to a successful unambiguous selection of a topic and
communicative success are strengthened over the course of many
interactions, while useless ones are dampened by lateral inhibition and
eventually pruned. At the same time, speakers create new words for
concepts that are as of yet missing from their lexicon, and hearers may
adopt them into theirs for their conceptualization of the topic the
speaker points at in case of failure. A similar feedback mechanism then
ensures that highly successful words are preferred and come to dominate
within the speech community. It is important to realize that at no time
do the individual agents share the same ontology or lexicon: newly
introduced distinctions and words are random and unique for each agent,
agents simply gradually learn which of these are useful in achieving
communicative success, which means that they naturally settle on
ontologies and lexicons that are close enough to those of others in the
population.&lt;/p&gt;
&lt;p&gt;This barely scratches the surface of how all these notions must be
orchestrated for a working computer implementation of this model, not to
mention the even more elaborate agent-based language game modeling
experiments that are already being conducted, investigating for instance
the emergence of grammar (see Part III of Steels 2015 for an overview of
recent scholarship). We see that even a seemingly simple task like
establishing a shared conceptualization of reality and agreeing on names
for these concepts is a complex endeavor which relies on a highly
sophisticated (though also highly general) machinery. Another key
observation is that while agent-based models can be fully virtual,
grounding them in physical reality (cf. the use of robots with sensors
and actuators) brings additional challenges that enable researchers to
reach vital insights which would otherwise be impossible. In particular,
grounding introduces fuzziness on the sensory input channels (by virtue
of different points of view for the two robots and analog-to-digital
conversion) which the agents must cope with, or else the mechanisms they
were endowed with cannot be considered as constituting a plausible,
sufficient model of the dynamics of human language. Unlike in generative
grammar, anything that is transient, imperfect, is eminently included in
the purview of linguistic inquiry. Failures are very much part of the
dynamics that steer the evolution of language. How could it adapt to the
speakers’ changing requirements if it did not include appropriate repair
strategies? Indeed, how could it be bootstrapped at all? The reward is a
model that successfully simulates not only language emergence, but also
transmission: if virgin agents are added into an existing population,
they gradually acquire its language (see Fig. 1).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img alt="communicative success plot" src="images/communicative_success.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1.&lt;/strong&gt; Communicative success in a population of agents with a steady
influx of virgin agents and outflux of old ones (overall population size
remains the same). The game starts in phase 1 with 20 virgin agents; phases 2
and 4 show the behavior of the model at an agent renewal rate of 1/1000 games,
whereas phase 3 corresponds to a heavier rate of 1/100. (Figure from Steels
2015, 121.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Crucially, the drive to communicate, to interact, is built into the
agents, they just keep playing games as long as they can. But if it was
not, the simulation would have to be more complex and somehow elicit
this drive by introducing appropriate ecological constraints, e.g. by
requiring co-operation as a survival strategy (Steels 2015, 106).
Otherwise, the agents would have no motivation to strive for
communicative success in their mutual encounters, they would fail to
reach intersubjective alignment of their conceptual spaces and lexicons,
and language would not emerge. In other words, far from being an
afterthought, successful communication with a partner, grounded in an
external context, turns out to be a fundamental requirement to establish
the kind of dynamics which allow languages to appear.&lt;/p&gt;
&lt;p&gt;Paradoxically, since it concerns itself with simulations and
computational models, this branch of evolutionary linguistics is, like
generative grammar, also highly speculative. However, unlike generative
grammar, it is a kind of speculation which considers guidance by
empirical observations a necessity, not a nuisance. Furthermore,
simulations are meant to be tested: if an agent-based model of language
emergence fails to converge on the result stipulated for that particular
experiment, the model is plain wrong and the dynamics it is trying to
put into place (cognitive strategies, feedback propagation etc.) need to
be revised. There is thus a clear-cut criterion for validity. Lastly,
even if a simulation works, an accompanying debate as to whether the
mechanisms involved are actually plausible approximations of reality is
considered an integral part of hypothesis evaluation, with evidence from
strongly empirically grounded disciplines like biology and
neurophysiology a vital element in the process.&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Taking a cue from Wittgenstein’s &lt;em&gt;Philosophical Investigations&lt;/em&gt;, this
essay should not be construed as an attempt to replace one doctrine with
another, but to advocate a “change of attitude” (cf. McGinn 2013, 33)
which allows asking more meaningful questions about language. This being
said, on the evidence presented above, it is hard not to conclude that
Noam Chomsky is fundamentally mistaken about the corrective that is
necessary for language learning to take place. According to Chomsky, the
criterion for evaluating linguistic rules lies within a dedicated
language organ we are genetically endowed with; the innate structures
themselves embody the metric by which conjectures pertaining to
linguistic rules will be judged. By contrast, in the evolutionary
linguistics perspective, genetics provide innate structures which are
capable of random growth, but the feedback (reinforcement and pruning)
which results in steering this growth in a particular direction comes
from interactions with the environment. This theory presupposes much
less specificity in the hardware infrastructure which makes this
possible and so should be preferred both on grounds of simplicity and
flexibility of the model, not to mention that it is biologically
plausible and has been empirically verified to work.&lt;/p&gt;
&lt;p&gt;In the context of science, Chomsky’s rhetorical strategy in and of
itself is dishonest: he preaches formal rigor while practicing sleight
of hand, and casually retreats to increasingly abstract ground on
reaching an impasse. He thus carves out a region in discursive space
which has no corresponding equivalent in a logically consistent
conceptual space, without which a piece of discourse can hardly
constitute a scientific theory. In other words, much like his famous
example sentence “Colorless green ideas sleep furiously”, his discourse
is grammatical but largely nonsensical under the requirements on a
system of thought which aspires to mirror reality in a coherent fashion.&lt;/p&gt;
&lt;p&gt;Requirements on scientific discourse notwithstanding, we as linguists
should keep in mind that language in general is much more than a system
for encoding logical propositions. Even Wittgenstein had to resign
himself to the fact – or perhaps knew all along – that the &lt;em&gt;Tractatus&lt;/em&gt;,
which he framed as the ultimate solution to all metaphysical
controversies, could only fan the flames of philosophical debate. It was
after all addressed to a diverse community of people bound perhaps
exclusively by their penchant for elaborate language games.&lt;/p&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;Aarsleff, Hans. 1970. “The History of Linguistics and Professor
Chomsky.” &lt;em&gt;Language&lt;/em&gt; 46 (3): 570–85.&lt;/p&gt;
&lt;p&gt;Chomsky, Noam. 1956. “Three Models for the Description of Language.”&lt;/p&gt;
&lt;p&gt;———. 1965. &lt;em&gt;Aspects of the Theory of Syntax&lt;/em&gt;. Cambridge, MA: The M.I.T.
Press.&lt;/p&gt;
&lt;p&gt;———. 1986. &lt;em&gt;Knowledge of Language: Its Nature, Origin, and Use&lt;/em&gt;.
Convergence. New York, Westport, London: Praeger.&lt;/p&gt;
&lt;p&gt;———. 1995. &lt;em&gt;The Minimalist Program&lt;/em&gt;. Cambridge, MA: The MIT Press.&lt;/p&gt;
&lt;p&gt;———. 2002. &lt;em&gt;Syntactic Structures&lt;/em&gt;. 2nd ed. Berlin, New York: Mouton de
Gruyter.&lt;/p&gt;
&lt;p&gt;———. 2007. “Of Minds and Language.” &lt;em&gt;Biolinguistics&lt;/em&gt;, no. 1: 9–27.&lt;/p&gt;
&lt;p&gt;———. 2009. &lt;em&gt;Cartesian Linguistics: A Chapter in the History of
Rationalist Thought&lt;/em&gt;. 3rd ed. Cambridge: Cambridge University Press.&lt;/p&gt;
&lt;p&gt;McGinn, Marie. 2006. &lt;em&gt;Elucidating the Tractatus: Wittgenstein’s Early
Philosophy of Logic and Language&lt;/em&gt;. Oxford: Oxford University Press.&lt;/p&gt;
&lt;p&gt;———. 2013. &lt;em&gt;The Routledge Guidebook to Wittgenstein’s Philosophical
Investigations&lt;/em&gt;. The Routledge Guides to Great Books. Routledge.&lt;/p&gt;
&lt;p&gt;Miel, Jan. 1969. “Pascal, Port-Royal, and Cartesian Linguistics.”
&lt;em&gt;Journal of the History of Ideas&lt;/em&gt; 30 (2): 261–71.&lt;/p&gt;
&lt;p&gt;Oudeyer, Pierre-Yves. 2006. &lt;em&gt;Self-Organization in the Evolution of
Speech&lt;/em&gt;. Translated by James R. Hurford. Oxford, New York: OUP.&lt;/p&gt;
&lt;p&gt;Pullum, Geoffrey K. 2011. “On the Mathematical Foundations of &lt;em&gt;Syntactic
Structures&lt;/em&gt;.” &lt;em&gt;Journal of Logic, Language and Information&lt;/em&gt; 20: 277–96.&lt;/p&gt;
&lt;p&gt;Raymond, Eric S. 1999. &lt;em&gt;The Cathedral &amp;amp; the Bazaar: Musings on Linux and
Open Source by an Accidental Revolutionary&lt;/em&gt;. O’Reilly Media.&lt;/p&gt;
&lt;p&gt;Sampson, Geoffrey. 2005. &lt;em&gt;The “Language Instinct” Debate&lt;/em&gt;. 3rd ed.
London, New York: Continuum.&lt;/p&gt;
&lt;p&gt;———. 2015. “Rigid Strings and Flaky Snowflakes.” &lt;em&gt;Language and
Cognition&lt;/em&gt; 10: 1–17.&lt;/p&gt;
&lt;p&gt;Skinner, B. F. 1957. &lt;em&gt;Verbal Behavior&lt;/em&gt;. The Century Psychology Series.
New York: Appleton – Century – Crofts.&lt;/p&gt;
&lt;p&gt;Steels, Luc. 2015. &lt;em&gt;The Talking Heads Experiment: Origins of Words and
Meanings&lt;/em&gt;. Computational Models of Language Evolution 1. Berlin:
Language Science Press.&lt;/p&gt;
&lt;p&gt;Wittgenstein, Ludwig. 2001. &lt;em&gt;Tractatus Logico-Philosophicus&lt;/em&gt;. Routledge
Classics. London, New York: Routledge.&lt;/p&gt;
&lt;p&gt;———. 2009. &lt;em&gt;Philosophical Investigations&lt;/em&gt;. Chichester, United Kingdom:
Blackwell Publishing.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn-1"&gt;
&lt;p&gt;Let us pretend for a moment that language games like “poetry” or
the surrealist pastime of &lt;em&gt;cadavre exquis&lt;/em&gt; do not exist; in these,
the quoted sentence could appear as perfectly valid and meaningful,
though perhaps not in the sense that Chomsky intended. “Meaningful”
in a late-Wittgensteinian perspective could be paraphrased as
“accepted by at least one involved party as a valid turn within the
context of a particular language game”.&amp;#160;&lt;a class="footnote-backref" href="#fnref-1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-2"&gt;
&lt;p&gt;Arguing that this does not matter because we should be concerned
with the &lt;em&gt;ideal&lt;/em&gt; speaker-hearer’s competence just takes us further
down the impasse, because now we have to determine how to delimit
the purported “ideal”.&amp;#160;&lt;a class="footnote-backref" href="#fnref-2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-3"&gt;
&lt;p&gt;This is the beauty of building empirically unmotivated, purely
speculative theories: at any moment, one can freely accommodate a
new element into the existing framework, substituting novelty and
amalgamation for critical evaluation.&amp;#160;&lt;a class="footnote-backref" href="#fnref-3" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-4"&gt;
&lt;p&gt;Cf. also Sampson’s riposte : “If complex properties of some aspect
of human behaviour have to be as they are as a matter of conceptual
necessity, then there is no reason to postulate complex genetically
inherited cognitive machinery determining those behaviour patterns”
(Sampson 2015, 9).&amp;#160;&lt;a class="footnote-backref" href="#fnref-4" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-5"&gt;
&lt;p&gt;This interconnected architecture stands in stark contrast to
Chomsky’s deliberately isolationist approach: “the relation between
semantics and syntax […] can only be studied after the syntactic
structure has been determined on independent grounds” (Chomsky 2002,
17).&amp;#160;&lt;a class="footnote-backref" href="#fnref-5" title="Jump back to footnote 5 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-6"&gt;
&lt;p&gt;Wittgenstein only hints at the problem of conceptualization, but
he is prescient in realizing it is not a given: “The primary
elements [of the objects which constitute the world in this
particular language game] are the coloured squares. ‘But are these
simple?’ – I wouldn’t know what I could more naturally call a
‘simple’ in this language-game. But under other circumstances, I’d
call a monochrome square, consisting perhaps of two rectangles or of
the elements colour and shape, ‘composite’” (Wittgenstein 2009,
27e).&amp;#160;&lt;a class="footnote-backref" href="#fnref-6" title="Jump back to footnote 6 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="linguistics"></category><category term="Wittgenstein"></category><category term="Chomsky"></category></entry><entry><title>How computers handle text: a gentle but thorough introduction to Unicode</title><link href="https://dlukes.github.io/unicode.html" rel="alternate"></link><published>2016-01-27T00:00:00+01:00</published><updated>2016-01-27T00:00:00+01:00</updated><author><name>dlukes</name></author><id>tag:dlukes.github.io,2016-01-27:/unicode.html</id><summary type="html">&lt;p&gt;An introductory text about computer text encodings, primarily aimed at linguists.&lt;/p&gt;</summary><content type="html">&lt;p&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Or, &lt;a href="http://www.joelonsoftware.com/articles/Unicode.html"&gt;the absolute minimum every &lt;del&gt;software developer&lt;/del&gt; linguist absolutely, positively must know about Unicode and character sets (no excuses!)&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This text uses the Python programming language to give some hands-on experience of the concepts discussed. If you're not familiar with programming at all, much less with Python, my advice is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;either: ignore the code, focus on the comments around it, they should be enough to follow the thread of the explanation;&lt;/li&gt;
&lt;li&gt;or, if you've got a little more time: Python is pretty intuitive, so you might want to have a look at the code examples anyway.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In any case, the code might make more sense to you if you tinker with it in an interactive Python session: &lt;a href="https://mybinder.org/v2/gh/dlukes/dlukes.github.io/source?filepath=content%2Fnotebooks%2Funicode.ipynb"&gt;&lt;img src="https://mybinder.org/badge.svg" alt="Binder"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And now, without further ado...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Much like any other piece of data inside a digital computer, text is represented as a series of binary digits (bits), i.e. 0's and 1's. A mapping between sequences of bits and characters is called an encoding. How many different characters your encoding can handle depends on how many bits you allow per character:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;with 1 bit you can have 2^1 = 2 characters (one is mapped to 0, the other to 1)&lt;/li&gt;
&lt;li&gt;with 2 bits you can have 2^2 = 2*2 = 4 characters (mapped to 00, 01, 10 and 11)&lt;/li&gt;
&lt;li&gt;with 3 bits you can have 2^3 = 2*2*2 = 8 characters &lt;/li&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, a short digression on representing numbers, to make sure we're all on the same page: in the context of computers, you often see the same number represented in three different ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;as a &lt;strong&gt;decimal&lt;/strong&gt; number, using 10 digits 0--9 (e.g. 12)&lt;/li&gt;
&lt;li&gt;as a &lt;strong&gt;binary&lt;/strong&gt; number, using only 2 digits, 0 and 1 (e.g. 1100, which equals decimal 12)&lt;/li&gt;
&lt;li&gt;as a &lt;strong&gt;hexadecimal&lt;/strong&gt; number, using 16 digits: 0--9 and a--f (e.g. c, which also equals decimal 12)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As a consequence, the need often arises to convert between these different &lt;strong&gt;numeral systems&lt;/strong&gt;. If you don't want to do so by hand, Python has your back! The &lt;code&gt;bin()&lt;/code&gt; function gives you a &lt;em&gt;string representation&lt;/em&gt; of the binary form of a number:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[1]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;bin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;65&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[1]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;&amp;#39;0b1000001&amp;#39;&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;As you can see, in Python, binary numbers are given a &lt;code&gt;0b&lt;/code&gt; prefix to distinguish them from regular (decimal) numbers. The number itself is what follows after (i.e. 1000001).&lt;/p&gt;
&lt;p&gt;Similarly, hexadecimal numbers are given an &lt;code&gt;0x&lt;/code&gt; prefix:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[2]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;hex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;65&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[2]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;&amp;#39;0x41&amp;#39;&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;To convert in the opposite direction, i.e. &lt;em&gt;to&lt;/em&gt; decimal, just evaluate the binary or hexadecimal representation of a number:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[3]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mb"&gt;0b1000001&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[3]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;65&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[4]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mh"&gt;0x41&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[4]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;65&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Numbers using these different &lt;strong&gt;bases&lt;/strong&gt; (base 2 = binary, base 10 = decimal, base 16 = hexadecimal) are mutually compatible, e.g. for comparison purposes:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[5]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mb"&gt;0b1000001&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mh"&gt;0x41&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;65&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[5]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;True&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Why are hexadecimals useful? They're primarily a more &lt;strong&gt;convenient, condensed&lt;/strong&gt; way of representing sequences of bits: each hexadecimal digit can represent 16 different values, and therefore it can stand in for a sequence of 4 bits (2^4 = 16).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[6]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mh"&gt;0xa&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mb"&gt;0b1010&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[6]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;True&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[7]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mh"&gt;0xb&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mb"&gt;0b1011&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[7]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;True&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[8]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# if we paste together hexadecimal a and b, it&amp;#39;s the&lt;/span&gt;
&lt;span class="c1"&gt;# same as pasting together binary 1010 and 1011&lt;/span&gt;
&lt;span class="mh"&gt;0xab&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mb"&gt;0b10101011&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[8]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;True&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In other words, instead of binary 10101011, we can just write hexadecimal ab and save ourselves some space. Of course, this only works if shorter binary numbers are &lt;strong&gt;padded to a 4-bit width&lt;/strong&gt;:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[9]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mh"&gt;0x2&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mb"&gt;0b10&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[9]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;True&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[10]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mh"&gt;0x3&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mb"&gt;0b11&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[10]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;True&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[11]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# if we paste together hexadecimal 2 and 3, we have to&lt;/span&gt;
&lt;span class="c1"&gt;# paste together binary 0010 and 0011...&lt;/span&gt;
&lt;span class="mh"&gt;0x23&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mb"&gt;0b00100011&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[11]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;True&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[12]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# ... not just 10 and 11&lt;/span&gt;
&lt;span class="mh"&gt;0x23&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mb"&gt;0b1011&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[12]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;False&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;The padding has no effect on the value, much like decimal 42 and 00000042 are effectively the same numbers.&lt;/p&gt;
&lt;p&gt;Now back to text encodings. The oldest encoding still in widespread use is &lt;a href="https://en.wikipedia.org/wiki/ASCII"&gt;&lt;code&gt;ASCII&lt;/code&gt;&lt;/a&gt;, which is a 7-bit encoding. What's the number of &lt;em&gt;different&lt;/em&gt; sequences of seven 1's and 0's?&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[13]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# this is how Python spells 2^7, i.e. 2*2*2*2*2*2*2&lt;/span&gt;
&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[13]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;128&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This means ASCII can represent &lt;a href="http://www.ascii-code.com/"&gt;128 different characters&lt;/a&gt;, which comfortably fits the basic Latin alphabet (both lowercase and uppercase), Arabic numerals, punctuation and some "control characters" which were primarily useful on the old &lt;a href="https://en.wikipedia.org/wiki/Teleprinter"&gt;teletype terminals&lt;/a&gt; for which &lt;code&gt;ASCII&lt;/code&gt; was designed. For instance, the letter "A" corresponds to the number 65 (&lt;code&gt;1000001&lt;/code&gt; in binary, see above).&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;"ASCII" stands for "&lt;strong&gt;American&lt;/strong&gt; Standard Code for Information Interchange" -- which explains why there are no accented characters, for instance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Nowadays, &lt;code&gt;ASCII&lt;/code&gt; is represented using 8 bits (= 1 byte), because that's the unit of computer memory which has become ubiquitous (in terms of both hardware and software assumptions), but still uses only 7 bits' worth of information. That extra bit means that there's &lt;strong&gt;room for another 128 characters in addition to the 128 ASCII ones&lt;/strong&gt;, coming up to a total of 256.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[14]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[14]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;256&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;What happens in the range [128; 256) is not covered by the &lt;code&gt;ASCII&lt;/code&gt; standard. In the 1990s, many encodings were standardized which used this range for their own purposes, usually representing additional accented characters used in a particular region. E.g. Czech (and Slovak, Polish...) alphabets can be represented using the ISO &lt;code&gt;latin-2&lt;/code&gt; encoding, or Microsoft's &lt;code&gt;cp-1250&lt;/code&gt;. Encodings which stick to the same character mappings as &lt;code&gt;ASCII&lt;/code&gt; in the range [0; 128) &lt;strong&gt;and represent them physically in the same way (as 1 byte)&lt;/strong&gt;, while potentially adding more character mappings beyond that, are called &lt;strong&gt;&lt;code&gt;ASCII&lt;/code&gt;-compatible&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ASCII&lt;/code&gt; compatibility is a good thing&amp;trade;, because when you start reading a character stream in a computer, there's &lt;strong&gt;no way to know in advance what encoding it is in&lt;/strong&gt; (unless it's a file you've encoded yourself). So in practice, a heuristic has been established to start reading the stream assuming it is &lt;code&gt;ASCII&lt;/code&gt; by default, and switch to a different encoding if evidence becomes available that motivates it. For instance, HTML files should all start something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cp"&gt;&amp;lt;!DOCTYPE html&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;html&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;head&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;meta&lt;/span&gt; &lt;span class="na"&gt;charset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;utf-8&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;/&amp;gt;&lt;/span&gt;
  ...
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This way, whenever a program wants to read a file like this, it can start off with &lt;code&gt;ASCII&lt;/code&gt;, waiting to see if it reaches the &lt;code&gt;charset&lt;/code&gt; (i.e. encoding) attribute, and once it does, it can switch from &lt;code&gt;ASCII&lt;/code&gt; to that encoding (&lt;code&gt;UTF-8&lt;/code&gt; here) and restart reading the file, now fairly sure that it's using the correct encoding. This trick works only if we can assume that whatever encoding the rest of the file is in, the first few lines can be considered as &lt;code&gt;ASCII&lt;/code&gt; for all practical intents and purposes.&lt;/p&gt;
&lt;p&gt;Without the &lt;code&gt;charset&lt;/code&gt; attribute, the only way to know if the encoding is right would be for you to look at the rendered text and see if it makes sense; if it did not, you'd have to resort to trial and error, manually switching the encodings and looking for the one in which the numbers behind the characters stop coming out as gibberish and are actually translated into intelligible text.&lt;/p&gt;
&lt;p&gt;Let's take a look at printable characters in the &lt;code&gt;Latin-2&lt;/code&gt; &lt;strong&gt;character set&lt;/strong&gt;. The character set consists of mappings between positive &lt;strong&gt;integers&lt;/strong&gt; (whole numbers) and characters; each one of these is called a &lt;strong&gt;codepoint&lt;/strong&gt;. The &lt;code&gt;Latin-2&lt;/code&gt; &lt;strong&gt;encoding&lt;/strong&gt; then defines how to encode each of these integers as a series of bits (1's and 0's) in the computer's memory.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[15]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;latin2_printable_characters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="c1"&gt;# the Latin-2 character set has 256 codepoints, corresponding to&lt;/span&gt;
&lt;span class="c1"&gt;# integers from 0 to 255&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;codepoint&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# the Latin-2 encoding is simple: each codepoint is encoded&lt;/span&gt;
    &lt;span class="c1"&gt;# as the byte corresponding to that integer in binary&lt;/span&gt;
    &lt;span class="n"&gt;byte&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;bytes&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;codepoint&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;character&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;byte&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;latin2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;character&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isprintable&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="n"&gt;latin2_printable_characters&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;codepoint&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;character&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;latin2_printable_characters&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[15]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;[(32, &amp;#39; &amp;#39;),
 (33, &amp;#39;!&amp;#39;),
 (34, &amp;#39;&amp;#34;&amp;#39;),
 (35, &amp;#39;#&amp;#39;),
 (36, &amp;#39;$&amp;#39;),
 (37, &amp;#39;%&amp;#39;),
 (38, &amp;#39;&amp;amp;&amp;#39;),
 (39, &amp;#34;&amp;#39;&amp;#34;),
 (40, &amp;#39;(&amp;#39;),
 (41, &amp;#39;)&amp;#39;),
 (42, &amp;#39;*&amp;#39;),
 (43, &amp;#39;+&amp;#39;),
 (44, &amp;#39;,&amp;#39;),
 (45, &amp;#39;-&amp;#39;),
 (46, &amp;#39;.&amp;#39;),
 (47, &amp;#39;/&amp;#39;),
 (48, &amp;#39;0&amp;#39;),
 (49, &amp;#39;1&amp;#39;),
 (50, &amp;#39;2&amp;#39;),
 (51, &amp;#39;3&amp;#39;),
 (52, &amp;#39;4&amp;#39;),
 (53, &amp;#39;5&amp;#39;),
 (54, &amp;#39;6&amp;#39;),
 (55, &amp;#39;7&amp;#39;),
 (56, &amp;#39;8&amp;#39;),
 (57, &amp;#39;9&amp;#39;),
 (58, &amp;#39;:&amp;#39;),
 (59, &amp;#39;;&amp;#39;),
 (60, &amp;#39;&amp;lt;&amp;#39;),
 (61, &amp;#39;=&amp;#39;),
 (62, &amp;#39;&amp;gt;&amp;#39;),
 (63, &amp;#39;?&amp;#39;),
 (64, &amp;#39;@&amp;#39;),
 (65, &amp;#39;A&amp;#39;),
 (66, &amp;#39;B&amp;#39;),
 (67, &amp;#39;C&amp;#39;),
 (68, &amp;#39;D&amp;#39;),
 (69, &amp;#39;E&amp;#39;),
 (70, &amp;#39;F&amp;#39;),
 (71, &amp;#39;G&amp;#39;),
 (72, &amp;#39;H&amp;#39;),
 (73, &amp;#39;I&amp;#39;),
 (74, &amp;#39;J&amp;#39;),
 (75, &amp;#39;K&amp;#39;),
 (76, &amp;#39;L&amp;#39;),
 (77, &amp;#39;M&amp;#39;),
 (78, &amp;#39;N&amp;#39;),
 (79, &amp;#39;O&amp;#39;),
 (80, &amp;#39;P&amp;#39;),
 (81, &amp;#39;Q&amp;#39;),
 (82, &amp;#39;R&amp;#39;),
 (83, &amp;#39;S&amp;#39;),
 (84, &amp;#39;T&amp;#39;),
 (85, &amp;#39;U&amp;#39;),
 (86, &amp;#39;V&amp;#39;),
 (87, &amp;#39;W&amp;#39;),
 (88, &amp;#39;X&amp;#39;),
 (89, &amp;#39;Y&amp;#39;),
 (90, &amp;#39;Z&amp;#39;),
 (91, &amp;#39;[&amp;#39;),
 (92, &amp;#39;\\&amp;#39;),
 (93, &amp;#39;]&amp;#39;),
 (94, &amp;#39;^&amp;#39;),
 (95, &amp;#39;_&amp;#39;),
 (96, &amp;#39;`&amp;#39;),
 (97, &amp;#39;a&amp;#39;),
 (98, &amp;#39;b&amp;#39;),
 (99, &amp;#39;c&amp;#39;),
 (100, &amp;#39;d&amp;#39;),
 (101, &amp;#39;e&amp;#39;),
 (102, &amp;#39;f&amp;#39;),
 (103, &amp;#39;g&amp;#39;),
 (104, &amp;#39;h&amp;#39;),
 (105, &amp;#39;i&amp;#39;),
 (106, &amp;#39;j&amp;#39;),
 (107, &amp;#39;k&amp;#39;),
 (108, &amp;#39;l&amp;#39;),
 (109, &amp;#39;m&amp;#39;),
 (110, &amp;#39;n&amp;#39;),
 (111, &amp;#39;o&amp;#39;),
 (112, &amp;#39;p&amp;#39;),
 (113, &amp;#39;q&amp;#39;),
 (114, &amp;#39;r&amp;#39;),
 (115, &amp;#39;s&amp;#39;),
 (116, &amp;#39;t&amp;#39;),
 (117, &amp;#39;u&amp;#39;),
 (118, &amp;#39;v&amp;#39;),
 (119, &amp;#39;w&amp;#39;),
 (120, &amp;#39;x&amp;#39;),
 (121, &amp;#39;y&amp;#39;),
 (122, &amp;#39;z&amp;#39;),
 (123, &amp;#39;{&amp;#39;),
 (124, &amp;#39;|&amp;#39;),
 (125, &amp;#39;}&amp;#39;),
 (126, &amp;#39;~&amp;#39;),
 (161, &amp;#39;Ą&amp;#39;),
 (162, &amp;#39;˘&amp;#39;),
 (163, &amp;#39;Ł&amp;#39;),
 (164, &amp;#39;¤&amp;#39;),
 (165, &amp;#39;Ľ&amp;#39;),
 (166, &amp;#39;Ś&amp;#39;),
 (167, &amp;#39;§&amp;#39;),
 (168, &amp;#39;¨&amp;#39;),
 (169, &amp;#39;Š&amp;#39;),
 (170, &amp;#39;Ş&amp;#39;),
 (171, &amp;#39;Ť&amp;#39;),
 (172, &amp;#39;Ź&amp;#39;),
 (174, &amp;#39;Ž&amp;#39;),
 (175, &amp;#39;Ż&amp;#39;),
 (176, &amp;#39;°&amp;#39;),
 (177, &amp;#39;ą&amp;#39;),
 (178, &amp;#39;˛&amp;#39;),
 (179, &amp;#39;ł&amp;#39;),
 (180, &amp;#39;´&amp;#39;),
 (181, &amp;#39;ľ&amp;#39;),
 (182, &amp;#39;ś&amp;#39;),
 (183, &amp;#39;ˇ&amp;#39;),
 (184, &amp;#39;¸&amp;#39;),
 (185, &amp;#39;š&amp;#39;),
 (186, &amp;#39;ş&amp;#39;),
 (187, &amp;#39;ť&amp;#39;),
 (188, &amp;#39;ź&amp;#39;),
 (189, &amp;#39;˝&amp;#39;),
 (190, &amp;#39;ž&amp;#39;),
 (191, &amp;#39;ż&amp;#39;),
 (192, &amp;#39;Ŕ&amp;#39;),
 (193, &amp;#39;Á&amp;#39;),
 (194, &amp;#39;Â&amp;#39;),
 (195, &amp;#39;Ă&amp;#39;),
 (196, &amp;#39;Ä&amp;#39;),
 (197, &amp;#39;Ĺ&amp;#39;),
 (198, &amp;#39;Ć&amp;#39;),
 (199, &amp;#39;Ç&amp;#39;),
 (200, &amp;#39;Č&amp;#39;),
 (201, &amp;#39;É&amp;#39;),
 (202, &amp;#39;Ę&amp;#39;),
 (203, &amp;#39;Ë&amp;#39;),
 (204, &amp;#39;Ě&amp;#39;),
 (205, &amp;#39;Í&amp;#39;),
 (206, &amp;#39;Î&amp;#39;),
 (207, &amp;#39;Ď&amp;#39;),
 (208, &amp;#39;Đ&amp;#39;),
 (209, &amp;#39;Ń&amp;#39;),
 (210, &amp;#39;Ň&amp;#39;),
 (211, &amp;#39;Ó&amp;#39;),
 (212, &amp;#39;Ô&amp;#39;),
 (213, &amp;#39;Ő&amp;#39;),
 (214, &amp;#39;Ö&amp;#39;),
 (215, &amp;#39;×&amp;#39;),
 (216, &amp;#39;Ř&amp;#39;),
 (217, &amp;#39;Ů&amp;#39;),
 (218, &amp;#39;Ú&amp;#39;),
 (219, &amp;#39;Ű&amp;#39;),
 (220, &amp;#39;Ü&amp;#39;),
 (221, &amp;#39;Ý&amp;#39;),
 (222, &amp;#39;Ţ&amp;#39;),
 (223, &amp;#39;ß&amp;#39;),
 (224, &amp;#39;ŕ&amp;#39;),
 (225, &amp;#39;á&amp;#39;),
 (226, &amp;#39;â&amp;#39;),
 (227, &amp;#39;ă&amp;#39;),
 (228, &amp;#39;ä&amp;#39;),
 (229, &amp;#39;ĺ&amp;#39;),
 (230, &amp;#39;ć&amp;#39;),
 (231, &amp;#39;ç&amp;#39;),
 (232, &amp;#39;č&amp;#39;),
 (233, &amp;#39;é&amp;#39;),
 (234, &amp;#39;ę&amp;#39;),
 (235, &amp;#39;ë&amp;#39;),
 (236, &amp;#39;ě&amp;#39;),
 (237, &amp;#39;í&amp;#39;),
 (238, &amp;#39;î&amp;#39;),
 (239, &amp;#39;ď&amp;#39;),
 (240, &amp;#39;đ&amp;#39;),
 (241, &amp;#39;ń&amp;#39;),
 (242, &amp;#39;ň&amp;#39;),
 (243, &amp;#39;ó&amp;#39;),
 (244, &amp;#39;ô&amp;#39;),
 (245, &amp;#39;ő&amp;#39;),
 (246, &amp;#39;ö&amp;#39;),
 (247, &amp;#39;÷&amp;#39;),
 (248, &amp;#39;ř&amp;#39;),
 (249, &amp;#39;ů&amp;#39;),
 (250, &amp;#39;ú&amp;#39;),
 (251, &amp;#39;ű&amp;#39;),
 (252, &amp;#39;ü&amp;#39;),
 (253, &amp;#39;ý&amp;#39;),
 (254, &amp;#39;ţ&amp;#39;),
 (255, &amp;#39;˙&amp;#39;)]&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Using the 8th bit (and thus the codepoint range [128; 256)) solves the problem of handling languages with character sets different than that of American English, but introduces a lot of complexity -- whenever you come across a text file with an unknown encoding, it might be in one of literally dozens of encodings. Additional drawbacks include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;how to handle multilingual text with characters from many different alphabets, which are not part of the same 8-bit encoding?&lt;/li&gt;
&lt;li&gt;how to handle writing systems which have way more than 256 "characters", e.g. Chinese, Japanese and Korean (CJK) ideograms?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For these purposes, a standard character set known as &lt;a href="https://en.wikipedia.org/wiki/Unicode"&gt;&lt;strong&gt;Unicode&lt;/strong&gt;&lt;/a&gt; was developed which strives for universal coverage of (ultimately) all characters ever used in the history of writing, even adding new ones like &lt;a href="https://unicode.org/emoji/charts/full-emoji-list.html"&gt;emojis&lt;/a&gt;. Unicode is much bigger than the character sets we've seen so far -- its most frequently used subset, the &lt;a href="https://en.wikipedia.org/wiki/Plane_%28Unicode%29#Basic_Multilingual_Plane"&gt;Basic Multilingual Plane&lt;/a&gt;, has 2^16 codepoints, but overall the number of codepoints is past 1M and there's room to accommodate many more.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[16]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[16]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;65536&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Now, the most straightforward representation for 2^16 codepoints is what? Well, it's simply using 16 bits per character, i.e. 2 bytes. That encoding exists, it's called &lt;code&gt;UTF-16&lt;/code&gt; ("UTF" stands for "Unicode Transformation Format"), but consider the drawbacks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we've lost &lt;code&gt;ASCII&lt;/code&gt; compatibility by the simple fact of using 2 bytes per character instead of 1 (encoding "a" as &lt;code&gt;01100001&lt;/code&gt; or &lt;code&gt;00000000|01100001&lt;/code&gt;, with the &lt;code&gt;|&lt;/code&gt; indicating an imaginary boundary between bytes, is not the same thing)&lt;/li&gt;
&lt;li&gt;encoding a string in a language which is mostly written down using basic letters of the Latin alphabet now takes up twice as much space (which is probably not a good idea, given the general dominance of English in electronic communication)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Looks like we'll have to think outside the box. The box in question here is called &lt;strong&gt;fixed-width encodings&lt;/strong&gt; -- all of the encoding schemes we've encountered so far were fixed-width, meaning that each character was represented by either 7, 8 or 16 bits. In other word, you could jump around the string in multiples of 7, 8 or 16 and always land at the beginning of a character. (Not exactly true for &lt;code&gt;UTF-16&lt;/code&gt;, because it is something more than just a "16-bit &lt;code&gt;ASCII&lt;/code&gt;": it has ways of handling characters beyond 2^16 using so-called &lt;a href="https://en.wikipedia.org/wiki/UTF-16#U.2B10000_to_U.2B10FFFF"&gt;surrogate sequences&lt;/a&gt; -- but you get the gist.)&lt;/p&gt;
&lt;p&gt;The smart idea that some bright people have come up with was to use a &lt;strong&gt;variable-width encoding&lt;/strong&gt;. The most ubiquitous one currently is &lt;strong&gt;&lt;code&gt;UTF-8&lt;/code&gt;&lt;/strong&gt;, which we've already met in the HTML example above. &lt;code&gt;UTF-8&lt;/code&gt; &lt;em&gt;is&lt;/em&gt; &lt;code&gt;ASCII&lt;/code&gt;-compatible, i.e. the 1's and 0's used to encode text containing only &lt;code&gt;ASCII&lt;/code&gt; characters are the same regardless of whether you use &lt;code&gt;ASCII&lt;/code&gt; or &lt;code&gt;UTF-8&lt;/code&gt;: it's a sequence of 8-bit bytes. But &lt;code&gt;UTF-8&lt;/code&gt; can also handle many more additional characters, as defined by the Unicode standard, by using progressively longer and longer sequences of bits.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[17]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;print_utf8_bytes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;char&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Prints binary representation of character as encoded by UTF-8.&lt;/span&gt;
&lt;span class="sd"&gt;    &lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;# encode the string as UTF-8 and iterate over the bytes;&lt;/span&gt;
    &lt;span class="c1"&gt;# iterating over a sequence of bytes yields integers in the&lt;/span&gt;
    &lt;span class="c1"&gt;# range [0; 256); the formatting directive &amp;quot;{:08b}&amp;quot; does two&lt;/span&gt;
    &lt;span class="c1"&gt;# things:&lt;/span&gt;
    &lt;span class="c1"&gt;#   - &amp;quot;b&amp;quot; prints the integer in its binary representation&lt;/span&gt;
    &lt;span class="c1"&gt;#   - &amp;quot;08&amp;quot; left-pads the binary representation with 0&amp;#39;s to a total&lt;/span&gt;
    &lt;span class="c1"&gt;#     width of 8, which is the width of a byte&lt;/span&gt;
    &lt;span class="n"&gt;binary_bytes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;{byte:08b}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;byte&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;char&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;utf8&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;{char!r}&lt;/span&gt;&lt;span class="s2"&gt; encoded in UTF-8 is: &lt;/span&gt;&lt;span class="si"&gt;{binary_bytes}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;print_utf8_bytes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;A&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# the representations...&lt;/span&gt;
&lt;span class="n"&gt;print_utf8_bytes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;č&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# ... keep...&lt;/span&gt;
&lt;span class="n"&gt;print_utf8_bytes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;字&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# ... getting longer.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;&amp;#39;A&amp;#39; encoded in UTF-8 is: [&amp;#39;01000001&amp;#39;]
&amp;#39;č&amp;#39; encoded in UTF-8 is: [&amp;#39;11000100&amp;#39;, &amp;#39;10001101&amp;#39;]
&amp;#39;字&amp;#39; encoded in UTF-8 is: [&amp;#39;11100101&amp;#39;, &amp;#39;10101101&amp;#39;, &amp;#39;10010111&amp;#39;]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;How does that even work? The obvious problem here is that with a fixed-width encoding, you just chop up the string at regular intervals (7, 8, 16 bits) and you know that each interval represents one character. So &lt;strong&gt;how do you know where to chop up a variable width-encoded string, if each character can take up a different number of bits?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Essentially, the trick is to &lt;strong&gt;use some of the bits&lt;/strong&gt; in the representation of a codepoint &lt;strong&gt;to store information&lt;/strong&gt; not about which character it is (whether it's an "A" or a "字"), but &lt;strong&gt;how many bits it occupies&lt;/strong&gt;. In other words, if you want to skip ahead 10 characters in a string encoded with a variable width-encoding, you can't just skip 10 * 7 or 8 or 16 bits; you have to read all the intervening characters to figure out how much space they take up. Take the following example:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[18]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;char&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Básník 李白&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;print_utf8_bytes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;char&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;&amp;#39;B&amp;#39; encoded in UTF-8 is: [&amp;#39;01000010&amp;#39;]
&amp;#39;á&amp;#39; encoded in UTF-8 is: [&amp;#39;11000011&amp;#39;, &amp;#39;10100001&amp;#39;]
&amp;#39;s&amp;#39; encoded in UTF-8 is: [&amp;#39;01110011&amp;#39;]
&amp;#39;n&amp;#39; encoded in UTF-8 is: [&amp;#39;01101110&amp;#39;]
&amp;#39;í&amp;#39; encoded in UTF-8 is: [&amp;#39;11000011&amp;#39;, &amp;#39;10101101&amp;#39;]
&amp;#39;k&amp;#39; encoded in UTF-8 is: [&amp;#39;01101011&amp;#39;]
&amp;#39; &amp;#39; encoded in UTF-8 is: [&amp;#39;00100000&amp;#39;]
&amp;#39;李&amp;#39; encoded in UTF-8 is: [&amp;#39;11100110&amp;#39;, &amp;#39;10011101&amp;#39;, &amp;#39;10001110&amp;#39;]
&amp;#39;白&amp;#39; encoded in UTF-8 is: [&amp;#39;11100111&amp;#39;, &amp;#39;10011001&amp;#39;, &amp;#39;10111101&amp;#39;]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Notice the initial bits in each byte of a character follow a pattern depending on how many bytes in total that character has:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if it's a 1-byte character, that byte starts with 0&lt;/li&gt;
&lt;li&gt;if it's a 2-byte character, the first byte starts with 11 and the following one with 10&lt;/li&gt;
&lt;li&gt;if it's a 3-byte character, the first byte starts with 111 and the following ones with 10&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This makes it possible to find out which bytes belong to which characters, and also to spot invalid strings, as the &lt;strong&gt;leading&lt;/strong&gt; byte in a &lt;strong&gt;multi-byte sequence&lt;/strong&gt; always "announces" how many &lt;strong&gt;continuation&lt;/strong&gt; bytes (= starting with 10) should follow.&lt;/p&gt;
&lt;p&gt;So much for a quick introduction to &lt;code&gt;UTF-8&lt;/code&gt; (= the encoding), but there's much more to Unicode (= the character set). While &lt;code&gt;UTF-8&lt;/code&gt; defines only how integer numbers corresponding to codepoints are to be represented as 1's and 0's in a computer's memory, Unicode specifies how those numbers are to be interpreted as characters, what their properties and mutual relationships are, what conversions (i.e. mappings between (sequences of) codepoints) they can undergo, etc.&lt;/p&gt;
&lt;p&gt;Consider for instance the various ways diacritics are handled: "č" can be represented either as a single codepoint (&lt;a href="http://www.fileformat.info/info/unicode/char/010D/index.htm"&gt;&lt;code&gt;LATIN SMALL LETTER C WITH CARON&lt;/code&gt;&lt;/a&gt; -- all Unicode codepoints have cute names like this) or a sequence of two codepoints, the character "c" and a combining diacritic mark (&lt;code&gt;COMBINING CARON&lt;/code&gt;). You can search for the codepoints corresponding to Unicode characters e.g. &lt;a href="http://www.fileformat.info/info/unicode/char/search.htm"&gt;here&lt;/a&gt; and play with them in Python using the &lt;code&gt;chr(0xXXXX)&lt;/code&gt; built-in function or with the special string escape sequence &lt;code&gt;\uXXXX&lt;/code&gt; (where &lt;code&gt;XXXX&lt;/code&gt; is the hexadecimal representation of the codepoint) -- both are ways to get the character corresponding to the given codepoint:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[19]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# &amp;quot;č&amp;quot; as LATIN SMALL LETTER C WITH CARON, codepoint 010d&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;chr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mh"&gt;0x010d&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\u010d&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;č
č
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[20]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# &amp;quot;č&amp;quot; as a sequence of LATIN SMALL LETTER C, codepoint 0063, and&lt;/span&gt;
&lt;span class="c1"&gt;# COMBINING CARON, codepoint 030c&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;chr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mh"&gt;0x0063&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;chr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mh"&gt;0x030c&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\u0063\u030c&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;č
č
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[21]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# of course, chr() also works with decimal numbers&lt;/span&gt;
&lt;span class="nb"&gt;chr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;269&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[21]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;&amp;#39;č&amp;#39;&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This means you have to be careful when working with languages that use accents, because &lt;strong&gt;to a computer, the two possible representations are of course different strings&lt;/strong&gt;, even though to you, they're conceptually the same:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[22]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\u010d&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;s2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\u0063\u030c&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
&lt;span class="c1"&gt;# s1 and s2 look the same to the naked eye...&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;č č
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[23]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# ... but they&amp;#39;re not&lt;/span&gt;
&lt;span class="n"&gt;s1&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;s2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[23]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;False&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Watch out, &lt;strong&gt;they even have different lengths&lt;/strong&gt;! This might come to bite you if you're trying to compute the length of a word in letters.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[24]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;s1 is&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;character(s) long.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;s2 is&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;character(s) long.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;s1 is 1 character(s) long.
s2 is 2 character(s) long.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;For this reason, even though we've been informally calling these Unicode entities "characters", it is more accurate and less confusing to use the technical term "codepoints".&lt;/p&gt;
&lt;p&gt;Generally, most text out there will use the first, single-codepoint approach whenever possible, and pre-packaged linguistic corpora will try to be consistent about this (unless they come from the web, which always warrants being suspicious and defensive about your material). If you're worried about inconsistencies in your data, you can perform a &lt;a href="https://en.wikipedia.org/wiki/Unicode_equivalence#Normalization"&gt;normalization&lt;/a&gt;:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[25]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;unicodedata&lt;/span&gt; &lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="n"&gt;normalize&lt;/span&gt;

&lt;span class="c1"&gt;# NFC stands for Normal Form C; this normalization applies a canonical&lt;/span&gt;
&lt;span class="c1"&gt;# decomposition (into a multi-codepoint representation) followed by a&lt;/span&gt;
&lt;span class="c1"&gt;# canonical composition (into a single-codepoint representation)&lt;/span&gt;
&lt;span class="n"&gt;s1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;normalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;NFC&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;s2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;normalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;NFC&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;s1&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;s2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[25]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;True&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Let's wrap things up by saying that Python itself uses Unicode internally, but the encoding it defaults to when opening an external file depends on the &lt;em&gt;locale&lt;/em&gt; of the system (broadly speaking, the set of region, language and character-encoding related settings of the operating system). On most modern Linux and macOS systems, this will probably be a &lt;code&gt;UTF-8&lt;/code&gt; locale and Python will therefore assume &lt;code&gt;UTF-8&lt;/code&gt; as the encoding by default. Unfortunately, Windows is different. To be on the safe side, whenever opening files in Python, you can specify the encoding explicitly:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[26]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;unicode.ipynb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;utf-8&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;pass&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In fact, it's &lt;strong&gt;always a good idea to specify the encoding explicitly, using &lt;code&gt;UTF-8&lt;/code&gt; as a default&lt;/strong&gt; if you don't know, for at least two reasons -- it makes your code more:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;portable&lt;/strong&gt; -- it will work the same across different operating systems which assume different default encodings;&lt;/li&gt;
&lt;li&gt;and &lt;strong&gt;resistant to data corruption&lt;/strong&gt; -- &lt;code&gt;UTF-8&lt;/code&gt; is more restrictive than fixed-width encodings, in the sense that not all sequences of bytes are valid &lt;code&gt;UTF-8&lt;/code&gt;. E.g. if one byte starts with 11, then the following one &lt;em&gt;must&lt;/em&gt; start with 10 (see above). If it starts with anything else, it's an error. By contrast, in a fixed-width encoding, &lt;em&gt;any&lt;/em&gt; sequence of bytes is valid. Decoding will always succeed, but if you use the wrong fixed-width encoding, the result will be garbage, which you might not notice. Therefore, it makes sense to default to &lt;code&gt;UTF-8&lt;/code&gt;: if it works, then there's a good chance that the file actually &lt;em&gt;was&lt;/em&gt; encoded in &lt;code&gt;UTF-8&lt;/code&gt; and you've read the data in correctly; if it fails, you get an explicit error which prompts you to investigate further.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another good idea, when dealing with Unicode text from an unknown and unreliable source, is to look at the set of codepoints contained in it and eliminate or replace those that look suspicious. Here's a function to help with that:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[27]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;unicodedata&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;ud&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;inspect_codepoints&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Create a frequency distribution of the codepoints in a string.&lt;/span&gt;
&lt;span class="sd"&gt;    &lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;char_frequencies&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_records&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;freq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;char&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;U+{ord(char):04x}&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;ud&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;char&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;ud&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;category&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;char&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;char&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;freq&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;char_frequencies&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;most_common&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;freq&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;char&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;codepoint&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;category&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Depending on your font configuration, it may be very hard to spot the two intruders in the sentence below. The frequency table shows the string contains regular &lt;code&gt;LATIN SMALL LETTER T&lt;/code&gt; and &lt;code&gt;LATIN SMALL LETTER G&lt;/code&gt;, but also their specialized but visually similar variants &lt;code&gt;MATHEMATICAL SANS-SERIF SMALL T&lt;/code&gt; and &lt;code&gt;LATIN SMALL LETTER SCRIPT G&lt;/code&gt;. You might want to replace such codepoints before doing further text processing...&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[28]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;inspect_codepoints&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Intruders here, good 𝗍hinɡ I checked.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[28]:&lt;/div&gt;



&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;freq&lt;/th&gt;
      &lt;th&gt;char&lt;/th&gt;
      &lt;th&gt;codepoint&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;category&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;e&lt;/td&gt;
      &lt;td&gt;U+0065&lt;/td&gt;
      &lt;td&gt;LATIN SMALL LETTER E&lt;/td&gt;
      &lt;td&gt;Ll&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;U+0020&lt;/td&gt;
      &lt;td&gt;SPACE&lt;/td&gt;
      &lt;td&gt;Zs&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;r&lt;/td&gt;
      &lt;td&gt;U+0072&lt;/td&gt;
      &lt;td&gt;LATIN SMALL LETTER R&lt;/td&gt;
      &lt;td&gt;Ll&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;d&lt;/td&gt;
      &lt;td&gt;U+0064&lt;/td&gt;
      &lt;td&gt;LATIN SMALL LETTER D&lt;/td&gt;
      &lt;td&gt;Ll&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;h&lt;/td&gt;
      &lt;td&gt;U+0068&lt;/td&gt;
      &lt;td&gt;LATIN SMALL LETTER H&lt;/td&gt;
      &lt;td&gt;Ll&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;I&lt;/td&gt;
      &lt;td&gt;U+0049&lt;/td&gt;
      &lt;td&gt;LATIN CAPITAL LETTER I&lt;/td&gt;
      &lt;td&gt;Lu&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;n&lt;/td&gt;
      &lt;td&gt;U+006e&lt;/td&gt;
      &lt;td&gt;LATIN SMALL LETTER N&lt;/td&gt;
      &lt;td&gt;Ll&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;o&lt;/td&gt;
      &lt;td&gt;U+006f&lt;/td&gt;
      &lt;td&gt;LATIN SMALL LETTER O&lt;/td&gt;
      &lt;td&gt;Ll&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;c&lt;/td&gt;
      &lt;td&gt;U+0063&lt;/td&gt;
      &lt;td&gt;LATIN SMALL LETTER C&lt;/td&gt;
      &lt;td&gt;Ll&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;t&lt;/td&gt;
      &lt;td&gt;U+0074&lt;/td&gt;
      &lt;td&gt;LATIN SMALL LETTER T&lt;/td&gt;
      &lt;td&gt;Ll&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;u&lt;/td&gt;
      &lt;td&gt;U+0075&lt;/td&gt;
      &lt;td&gt;LATIN SMALL LETTER U&lt;/td&gt;
      &lt;td&gt;Ll&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;s&lt;/td&gt;
      &lt;td&gt;U+0073&lt;/td&gt;
      &lt;td&gt;LATIN SMALL LETTER S&lt;/td&gt;
      &lt;td&gt;Ll&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;,&lt;/td&gt;
      &lt;td&gt;U+002c&lt;/td&gt;
      &lt;td&gt;COMMA&lt;/td&gt;
      &lt;td&gt;Po&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;g&lt;/td&gt;
      &lt;td&gt;U+0067&lt;/td&gt;
      &lt;td&gt;LATIN SMALL LETTER G&lt;/td&gt;
      &lt;td&gt;Ll&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;𝗍&lt;/td&gt;
      &lt;td&gt;U+1d5cd&lt;/td&gt;
      &lt;td&gt;MATHEMATICAL SANS-SERIF SMALL T&lt;/td&gt;
      &lt;td&gt;Ll&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;15&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;i&lt;/td&gt;
      &lt;td&gt;U+0069&lt;/td&gt;
      &lt;td&gt;LATIN SMALL LETTER I&lt;/td&gt;
      &lt;td&gt;Ll&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;16&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;ɡ&lt;/td&gt;
      &lt;td&gt;U+0261&lt;/td&gt;
      &lt;td&gt;LATIN SMALL LETTER SCRIPT G&lt;/td&gt;
      &lt;td&gt;Ll&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;17&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;k&lt;/td&gt;
      &lt;td&gt;U+006b&lt;/td&gt;
      &lt;td&gt;LATIN SMALL LETTER K&lt;/td&gt;
      &lt;td&gt;Ll&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;18&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;.&lt;/td&gt;
      &lt;td&gt;U+002e&lt;/td&gt;
      &lt;td&gt;FULL STOP&lt;/td&gt;
      &lt;td&gt;Po&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;... because of course, for a computer, the word "thing" written with two different variants of "g" is really just two different words, which is probably not what you want:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[29]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;thing&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;thinɡ&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[29]:&lt;/div&gt;




&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;False&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Finally, to put things into perspective, here's a diagram what happens when processing text with Python ("Unicode" in the central box stands for Python's internal representation of Unicode, which is &lt;strong&gt;not&lt;/strong&gt; &lt;code&gt;UTF-8&lt;/code&gt; nor &lt;code&gt;UTF-16&lt;/code&gt;):&lt;/p&gt;
&lt;p&gt;&lt;img alt="Text IO in Python" src="http://www.nltk.org/images/unicode.png" style="max-width: 100%;"&gt;&lt;/p&gt;
&lt;p&gt;(Image shamelessly hotlinked from / courtesy of the &lt;a href="http://www.nltk.org/book/"&gt;NLTK Book&lt;/a&gt;. Go check it out, it's an awesome intro to Python programming for linguists!)&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;A terminological postscript: we've been using some terms a bit informally, but now that we have a practical intuition for what they mean, it's good to get the definitions straight in one's head. So, a &lt;strong&gt;character set&lt;/strong&gt; is a mapping between &lt;strong&gt;codepoints&lt;/strong&gt; (integers) and &lt;strong&gt;characters&lt;/strong&gt;. We may for instance say that in our character set, the integer 99 corresponds to the character "c".&lt;/p&gt;
&lt;p&gt;On the other hand, an &lt;strong&gt;encoding&lt;/strong&gt; is a mapping between a &lt;strong&gt;codepoint&lt;/strong&gt; (an integer) and a &lt;strong&gt;physical sequence of 1's and 0's that represent it in memory&lt;/strong&gt;. With fixed-width encodings, this mapping is generally straightforward -- the 1's and 0's directly represent the given integer, only in binary and padded with zeros to fit the desired width. With variable-width encodings, which have to explicitly encode information about how many bits are spanned by each codepoint, this straightforward correspondence breaks down.&lt;/p&gt;
&lt;p&gt;A comparison might be helpful here: as encodings, &lt;code&gt;UTF-8&lt;/code&gt; and &lt;code&gt;UTF-16&lt;/code&gt; both use &lt;strong&gt;the same character set&lt;/strong&gt; -- the same integers corresponding to the same characters. But since they're &lt;strong&gt;different encodings&lt;/strong&gt;, when the time comes to turn these integers into sequences of bits to store in a computer's memory, each of them generates a different one.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For more on Unicode, a great read already hinted at above is Joel Spolsky's &lt;a href="http://www.joelonsoftware.com/articles/Unicode.html"&gt;The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)&lt;/a&gt;. Another great piece of material is the &lt;a href="https://youtu.be/MijmeoH9LT4"&gt;Characters, Symbols and the Unicode Miracle&lt;/a&gt; video by the &lt;a href="https://www.youtube.com/channel/UC9-y-6csu5WGm29I7JiwpnA"&gt;Computerphile&lt;/a&gt; channel on YouTube. To make the discussion digestible for newcomers, I sometimes slightly distorted facts about how things are "really really" done. And some inaccuracies may be genuine mistakes. In any case, please let me know in the comments! I'm grateful for feedback and looking to improve this material; I'll fix the mistakes and consider ditching some of the simplifications if they prove untenable :)&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
 

&lt;/p&gt;</content><category term="unicode"></category><category term="encoding"></category><category term="charset"></category><category term="programming"></category><category term="linguistics"></category><category term="python"></category></entry><entry><title>Úprava rozhraní konkordanceru KonText -- vylepšená verze</title><link href="https://dlukes.github.io/kontext-interface-tweak-update.html" rel="alternate"></link><published>2015-05-14T00:00:00+02:00</published><updated>2015-05-14T00:00:00+02:00</updated><author><name>dlukes</name></author><id>tag:dlukes.github.io,2015-05-14:/kontext-interface-tweak-update.html</id><summary type="html">&lt;p&gt;Vylepšená verze skriptu, kterým si uživatel Českého národního korpusu může upravit rozhraní konkordanceru KonText.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Před nějakou dobou jsem zde vyvěsil
&lt;a href="https://dlukes.github.io/kontext-interface-tweak.html"&gt;skript&lt;/a&gt;, jehož pomocí lze lehce
"přeskládat" a upravit rozhraní korpusového konkordanceru
&lt;a href="https://kontext.korpus.cz"&gt;KonText&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;menu je umístěné po straně místo nahoře a permanentně rozbalené&lt;/li&gt;
&lt;li&gt;nad vyhledanou konkordancí je umístěn rychlý hledací box, v němž lze
  předchozí dotaz pohodlně upravit&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Víc o motivaci těchto úprav se dočtete
&lt;a href="kontext-interface-tweak.html#background"&gt;v původním článku&lt;/a&gt;. Stále
platí, že ČNK nemá v plánu tyto změny začlenit přímo do oficiální verze
KonTextu, zejména proto, že rychlý hledací box sice v jistých situacích může
být užitečný, nicméně oproti standardnímu formuláři &lt;em&gt;Nový dotaz&lt;/em&gt; výrazně
omezuje možnosti pro zadání dotazu.&lt;/p&gt;
&lt;p&gt;Vylepšená verze, která je k dispozici níže, odstraňuje některé předchozí
nedostatky skriptu: rychlý hledací box nad konkordancí je větší, ukazuje &lt;strong&gt;vždy
CQL podobu posledního zadaného dotazu&lt;/strong&gt;&lt;sup id="fnref-1"&gt;&lt;a class="footnote-ref" href="#fn-1"&gt;1&lt;/a&gt;&lt;/sup&gt;, a především zůstává zobrazený i
během listování konkordancí (tj. není k dispozici jen na její první
stránce). Dotaz lze nyní navíc pro větší přehlednost rozdělit do více řádků,
takže opětovné vyhledávání se nově spouští stiskem kombinace kláves
&lt;strong&gt;Ctrl+Enter&lt;/strong&gt; (místo jen Enteru).&lt;/p&gt;
&lt;p&gt;Výsledné upravené rozhraní KonText vypadá stále podobně:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Upravené rozhraní KonText." src="images/kontext_interface_tweak_update.png" style="max-width: 100%;"&gt;&lt;/p&gt;
&lt;h1&gt;Postup instalace skriptu&lt;/h1&gt;
&lt;p&gt;Nová verze skriptu je k dispozici zde:&lt;/p&gt;
&lt;script src="https://gist.github.com/dlukes/a99dca231db63c9d5bb7.js"&gt;&lt;/script&gt;

&lt;p&gt;Kroky k jeho zprovoznění zůstávají stejné:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Nainstalovat si do svého prohlížeče plugin
    &lt;a href="https://chrome.google.com/webstore/detail/tampermonkey/dhdgffkkebhmkfjojejmpbldmpobfkfo?hl=en"&gt;Tampermonkey&lt;/a&gt;,
    pokud používáte Chrome, nebo
    &lt;a href="https://addons.mozilla.org/en-us/firefox/addon/greasemonkey/"&gt;Greasemonkey&lt;/a&gt;,
    pokud používáte Firefox. (Pokud používáte Internet Explorer, budete muset
    dočasně přesedlat na Chrome nebo Firefox.) Testovaný je skript zatím jen na
    Chromu.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Založit v daném pluginu nový skript (pro Chrome je tutorial
    &lt;a href="http://hibbard.eu/tampermonkey-tutorial/"&gt;zde&lt;/a&gt;, pro Firefox
    &lt;a href="http://hayageek.com/greasemonkey-tutorial/"&gt;zde&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Smazat kostru nového skriptu a nahradit ji skriptem, který si zkopírujete výše.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Skript uložit.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Používat KonText jako normálně -- skript už by podle adresy měl sám poznat,
    že se má spustit. Pokud se tak nestane, nejspíš to znamená, že je
    prohlížečový plugin (Tampermonkey nebo Greasemonkey) deaktivovaný a je
    potřeba jej znovu aktivovat.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn-1"&gt;
&lt;p&gt;V předchozí verzi se po aplikaci libovolného filtru změnil obsah
hledacího boxu na parametry filtrování.&amp;#160;&lt;a class="footnote-backref" href="#fnref-1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="KonText"></category><category term="korpus"></category><category term="konkordance"></category><category term="NoSke"></category><category term="Bonito"></category></entry><entry><title>Úprava rozhraní konkordanceru KonText</title><link href="https://dlukes.github.io/kontext-interface-tweak.html" rel="alternate"></link><published>2015-02-17T00:00:00+01:00</published><updated>2015-02-17T00:00:00+01:00</updated><author><name>dlukes</name></author><id>tag:dlukes.github.io,2015-02-17:/kontext-interface-tweak.html</id><summary type="html">&lt;p&gt;Skript, kterým si uživatel Českého národního korpusu může upravit rozhraní konkordanceru KonText.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;!POZOR!&lt;/h1&gt;
&lt;p&gt;K dispozici je nyní
&lt;a href="https://dlukes.github.io/kontext-interface-tweak-update.html"&gt;vylepšená verze níže popsaného skriptu&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Hledání v korpusech ČNK&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://korpus.cz"&gt;Český národní korpus&lt;/a&gt; je sbírka jazykových korpusů částečně
vytvářených &lt;a href="http://ucnk.ff.cuni.cz"&gt;Ústavem Českého národního korpusu&lt;/a&gt; a
částečně jinými institucemi. Všechny jsou hostované na jednom serveru a
dostupné skrz různá vyhledávací rozhraní
(tzv. &lt;a href="http://wiki.korpus.cz/doku.php/pojmy:korpusovy_manazer"&gt;konkordancery&lt;/a&gt;),
např. &lt;a href="https://www.korpus.cz/corpora"&gt;NoSke&lt;/a&gt;,
&lt;a href="http://ucnk.ff.cuni.cz/bonito/index.php"&gt;Bonito&lt;/a&gt; či nejnověji
&lt;a href="https://kontext.korpus.cz"&gt;KonText&lt;/a&gt;. Koncem března 2015 ovšem bude podpora
starších rozhraní ukončena a nadále půjde k datům v ČNK přistupovat primárně
pouze přes KonText.&lt;/p&gt;
&lt;p&gt;(Pokud vám odstavec výše nedává příliš smysl, s jazykovými korpusy se setkáváte
poprvé, ale chcete se dozvědět víc, raději si místo tohoto postu přečtěte,
&lt;a href="http://wiki.korpus.cz/doku.php/pojmy:korpus"&gt;k čemu je takový korpus dobrý&lt;/a&gt;, a
&lt;a href="https://kontext.korpus.cz"&gt;zkuste si v něm něco pro zajímavost vyhledat&lt;/a&gt;. Pokud
se vám při vzpomínce na Bonito či NoSke naopak zaskvěla slza v oku, čtěte dál!)&lt;/p&gt;
&lt;h1&gt;&lt;a id="background"&gt;&lt;/a&gt;KonText vs. Bonito / NoSke&lt;/h1&gt;
&lt;p&gt;KonText má oproti starším rozhraním řadu výhod -- bohatší funkcionalitu, mnohé
pomůcky, které vám pomohou se zadáním složitějších dotazů (sestavení
morfologického tagu či podmínky &lt;code&gt;within&lt;/code&gt;), a v neposlední řadě mnohem lépe
vypadá, což kupříkladu mně při práci působí jako balzám na duši. Nicméně
dlouholetí uživatelé ČNK byli jednoduše zvyklí na některé aspekty Bonita a
NoSke, které jim teď v KonTextu chybí.&lt;/p&gt;
&lt;p&gt;Onehdy při rozhovoru s jedním z nich vyplavaly na povrch jako hodně důležité
dvě stížnosti:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Vrchní menu v KonTextu je zákeřné, schovává se, člověk nemá přehled nad
   dostupnými funkcemi. Oproti tomu NoSke má menu po straně a je permanentně
   rozvinuté, takže uživatel má všechny možnosti interakce s konkordancí
   soustavně jako na dlani.&lt;/li&gt;
&lt;li&gt;Po zadání dotazu člověk často na základě konkordance zjistí, že jej
   potřebuje ještě trochu upravit / zjemnit. KonText si sice předchozí dotazy
   pamatuje, je ale potřeba se k nim doklikat; šikovnější by bylo, kdyby tato
   možnost byla dostupná přímo ze stránky konkordance v podobě nějakého
   zjednodušeného hledacího boxu. (NoSke tohle vlastně taky neumí, v Bonitu je
   to jednodušší.)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;V obou případech jde o smysluplné požadavky, jenže KonText je poměrně velká a
složitá aplikace, takže i pokud se ČNK rozhodne do ní tyto podněty v nějaké
podobě zapracovat (např. jako možnost přepnutí zobrazení menu), bude nějakou
chvíli trvat, než se implementace navrhne, vytvoří, řádně otestuje a konečně
dostane k uživatelům. Nicméně aby bylo možné alespoň vyzkoušet, jak by zmíněné
změny vypadaly v praxi, dal jsem dohromady krátký skript, který již v
prohlížeči nahraný KonText trochu "přestaví" a upraví. Výsledek vypadá
následovně:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Upravené rozhraní KonText." src="images/kontext_interface_tweak.png" style="max-width: 100%;"&gt;&lt;/p&gt;
&lt;p&gt;Rovnou předesílám: ten skript je nevzhledný bastl přilepený na KonText
zvnějšku; proto taky bylo možné jej dát dohromady poměrně rychle, protože si
neklade nárok na spolehlivost, která se vyžaduje od oficiální verze
KonTextu. Je to spíš prototyp, jehož účelem je otestovat výše popsané změny v
praxi a získat představu o tom, zda a do jaké míry jsou přínosné. (Vlastní
zkušenost: po chvíli používání mi přijde přídatný hledací box nad konkordancí
hodně šikovný a užitečný.)&lt;/p&gt;
&lt;p&gt;Teď k jádru pudla: &lt;strong&gt;pokud máte zájem, můžete si KonText takto k obrazu svému&lt;/strong&gt;
(resp. k obrázku o odstavec výš) &lt;strong&gt;upravit také&lt;/strong&gt; a vyzkoušet, jak vám takové
nastavení vyhovuje. Když se vám jedna z úprav bude líbit (nebo vás u toho
napadne jiná, kterou by si KonText zasloužil), můžete pak zadat
&lt;a href="https://podpora.korpus.cz/projects/kontext/issues/new"&gt;požadavek na nový feature&lt;/a&gt;.
Návod, jak si KonText upravit, následuje níže.&lt;/p&gt;
&lt;h1&gt;Postup instalace skriptu&lt;/h1&gt;
&lt;p&gt;Skript samotný je k dispozici zde:&lt;/p&gt;
&lt;script src="https://gist.github.com/dlukes/0764590b7a8464cbd000.js"&gt;&lt;/script&gt;

&lt;p&gt;K jeho zprovoznění jsou potřeba následující kroky:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Nainstalovat si do svého prohlížeče plugin
    &lt;a href="https://chrome.google.com/webstore/detail/tampermonkey/dhdgffkkebhmkfjojejmpbldmpobfkfo?hl=en"&gt;Tampermonkey&lt;/a&gt;,
    pokud používáte Chrome, nebo
    &lt;a href="https://addons.mozilla.org/en-us/firefox/addon/greasemonkey/"&gt;Greasemonkey&lt;/a&gt;,
    pokud používáte Firefox. (Pokud používáte Internet Explorer, budete muset
    dočasně přesedlat na Chrome nebo Firefox.) Testovaný je skript zatím jen na
    Chromu.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Založit v daném pluginu nový skript (pro Chrome je tutorial
    &lt;a href="http://hibbard.eu/tampermonkey-tutorial/"&gt;zde&lt;/a&gt;, pro Firefox
    &lt;a href="http://hayageek.com/greasemonkey-tutorial/"&gt;zde&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Smazat kostru nového skriptu a nahradit ji skriptem, který si zkopírujete výše.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Skript uložit.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Používat KonText jako normálně -- skript už by podle adresy měl sám poznat,
    že se má spustit. Pokud se tak nestane, nejspíš to znamená, že je
    prohlížečový plugin (Tampermonkey nebo Greasemonkey) deaktivovaný a je
    potřeba jej znovu aktivovat.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Omezení&lt;/h1&gt;
&lt;p&gt;Skript má pravděpodobně hromadu drobných much, na které se mi zatím nepodařilo
přijít -- budu se je snažit průběžně opravovat, když na ně padnu, nebo
&lt;a href="pages/about.html"&gt;když mi o nich dáte vědět&lt;/a&gt;. Krom toho má i některé mouchy, o
nichž už vím, ale bohužel toho s nimi nejde moc dělat.&lt;/p&gt;
&lt;p&gt;Asi nejnápadnější je, že přidaný hledací box funguje jen na těch stránkách, kde
je původní dotaz i součástí adresy URL (což nejsou všechny -- třeba když
začnete &lt;strong&gt;listovat konkordancí&lt;/strong&gt; na druhou stránku a dál, &lt;strong&gt;dotaz je z adresy
vyjmut&lt;/strong&gt; a &lt;strong&gt;pomocný hledací box tedy zmizí&lt;/strong&gt;). Ale vzhledem k tomu, že jeho
hlavní účel má být možnost lehce upravit dotaz po prvním rychlém nahlédnutí do
konkordance, snad to nebude takový problém. Pokud někdy bude podobný box řádně
přidán přímo do KonTextu, takovými nedostatky samozřejmě trpět nebude.&lt;/p&gt;
&lt;p&gt;A ještě k &lt;strong&gt;používání přidaného hledacího boxu&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Typ dotazu, který je do něj potřeba zadat, je stejný jako ten, který jste
   při prvotním vyhledání konkordance zadali na stránce
   &lt;a href="https://kontext.korpus.cz/first_form"&gt;Nový dotaz&lt;/a&gt;. Pokud tento prvotní
   dotaz byl &lt;em&gt;Základní&lt;/em&gt; dotaz, můžete pomocí rychlého boxu zadat jiný
   &lt;em&gt;Základní&lt;/em&gt; dotaz; pokud to byl &lt;em&gt;CQL&lt;/em&gt; dotaz, můžete ho upravit zas jen na
   další &lt;em&gt;CQL&lt;/em&gt; dotaz. Důvodem je, že &lt;strong&gt;smyslem&lt;/strong&gt; tohoto pomocného boxu &lt;strong&gt;není
   nahradit plnohodnotný formulář&lt;/strong&gt; pro zadání dotazu, jen poskytnout rychlou
   možnost, jak již &lt;strong&gt;zadaný dotaz upravit&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Pomocný hledací box se objeví i poté, co na konkordanci provedete
   filtrování. V takové situaci se dá použít k tomu, abyste &lt;strong&gt;pozměnili zadání
   aktuálního filtru&lt;/strong&gt;, tj. filtrování se provede znovu na původní konkordanci,
   ne na této již filtrované. Pokud chcete opakovaně filtrovat tu samou
   konkordanci a postupně podle daných kritérií vyřazovat / přidávat řádky, je
   potřeba místo hledacího boxu opakovaně použít menu &lt;em&gt;Filtr&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Komu si stěžovat, když to nebude fungovat&lt;/h1&gt;
&lt;p&gt;Skript je volně šiřitelný pod licencí
&lt;a href="http://www.gnu.org/copyleft/gpl.html"&gt;GNU GPL v3&lt;/a&gt;, takže se na něj neváže
žádná záruka. Když se vám ale nebude dařit jej zprovoznit, rád se pokusím
pomoct! Stačí se ozvat na adresu uvedenou &lt;a href="pages/about.html"&gt;zde&lt;/a&gt;.&lt;/p&gt;</content><category term="KonText"></category><category term="korpus"></category><category term="konkordance"></category><category term="NoSke"></category><category term="Bonito"></category></entry></feed>